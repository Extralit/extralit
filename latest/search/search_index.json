{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Extralit is an advanced literature review tool designed for efficient data extraction from scientific literature. It leverages LLM technology and RAG techniques to assist researchers in extracting structured data according to predefined Schemas.</p> <p>To get started:</p> <ul> <li> <p>Get started in 5 minutes!</p> <p>Deploy Extralit for free on the Hugging Face Hub or with <code>Docker</code>. Install the Python SDK with <code>pip</code> and create your first project.</p> <p> Quickstart</p> </li> <li> <p>How-to guides</p> <p>Get familiar with the basic workflows of Extralit. Learn how to manage <code>Users</code>, <code>Workspaces</code>, <code>Datasets</code>, and <code>Records</code> to set up your data annotation projects.</p> <p> Learn more</p> </li> </ul> <p>Or, play with the Extralit UI by signing in with your Hugging Face account on the public HuggingFace Space deployment.</p>"},{"location":"#why-use-extralit","title":"Why use Extralit?","text":"<p>Extralit is designed to helps researchers and data scientists tackle the challenges of processing large volumes of academic papers, ensuring high-quality data extraction for scientific analysis and meta-studies. By combining LLMs and advanced ML models with intuitive workflows, Extralit is a powerful tool designed to transform unstructured scientific papers into structured, analyzable data. </p> <p>Accelerate scientific data extraction</p> <p>Manual data extraction from scientific papers is time-consuming and error-prone. Extralit leverages LLMs to automate and assist in the extraction process, significantly reducing the time and effort required to compile structured datasets from literature.</p> <p>Ensure consistency and accuracy in extracted data</p> <p>Scientific research demands high-quality, complete, and consistent data, but every field of study has unique data requirements. Extralit offers customizable extraction schemas and workflows to validate and review data, allowing you to define exactly the size and format of data extracted and how it should be structured, adapting to the specific needs of your research domain.</p> <p>Advanced scientific text and table parsing</p> <p>Scientific papers come in a variety of formats and layouts, often with complex tables and figures. Extralit excels at handling diverse scientific paper formats, employing advanced parsing techniques to ensure you get complete and accurate data, regardless of the complexity of the source material.</p> <p>Collaborate effectively on large-scale literature reviews</p> <p>Literature review and meta-analysis projects often require team effort. Extralit builds upon Argilla's platform to facilitate collaborative extraction, allowing multiple researchers to work together efficiently, share insights, and maintain a consistent approach across large volumes of literature.</p>"},{"location":"#relationship-to-argilla","title":"Relationship to Argilla","text":"<p>Extralit builds upon Argilla's foundation, adding specialized features for scientific data extraction.</p>"},{"location":"admin_guide/","title":"How-to guides","text":"<p> This page is currently under construction. Please check back later for updates.</p> <p>These guides provide step-by-step instructions for common scenarios, including detailed explanations and code samples. They are divided into two categories: basic and advanced. The basic guides will help you get started with the core concepts of Argilla, while the advanced guides will help you explore more advanced features.</p>"},{"location":"admin_guide/#basic","title":"Basic","text":"<ul> <li> <p>Manage users and credentials</p> <p>Learn what they are and how to manage (create, read and delete) <code>Users</code> in Argilla.</p> <p> How-to guide</p> </li> <li> <p>Manage workspaces</p> <p>Learn what they are and how to manage (create, read and delete) <code>Workspaces</code> in Argilla.</p> <p> How-to guide</p> </li> <li> <p>Create, update, and delete datasets</p> <p>Learn what they are and how to manage (create, read and delete) <code>Datasets</code> and customize them using the <code>Settings</code> for <code>Fields</code>, <code>Questions</code>,  <code>Metadata</code> and <code>Vectors</code>.</p> <p> How-to guide</p> </li> <li> <p>Add, update, and delete records</p> <p>Learn what they are and how to add, update and delete the values for a <code>Record</code>, which are made up of <code>Metadata</code>, <code>Vectors</code>, <code>Suggestions</code> and <code>Responses</code>.</p> <p> How-to guide</p> </li> <li> <p>Distribute the annotation</p> <p>Learn how to use Argilla's automatic task distribution to annotate as a team efficiently.</p> <p> How-to guide</p> </li> <li> <p>Annotate a dataset</p> <p>Learn how to use the Argilla UI to navigate datasets and submit responses.</p> <p> How-to guide</p> </li> <li> <p>Query and filter a dataset</p> <p>Learn how to query and filter a <code>Dataset</code>.</p> <p> How-to guide</p> </li> <li> <p>Import and export datasets and records</p> <p>Learn how to export your dataset or its records to Python, your local disk, or the Hugging face Hub.</p> <p> How-to guide</p> </li> </ul>"},{"location":"admin_guide/#advanced","title":"Advanced","text":"<ul> <li> <p>Use Markdown to format rich content</p> <p>Learn how to use Markdown and HTML in TextFields to format chat conversations and allow for basic multi-modal support for images, audio, video and PDFs.</p> <p> How-to guide</p> </li> <li> <p>Migrate to Argilla V2</p> <p>Learn how to migrate your legacy datasets from Argilla 1.x to 2.x.</p> <p> How-to guide</p> </li> </ul>"},{"location":"admin_guide/annotate/","title":"Annotate your dataset","text":"<p>To experience the UI features firsthand, you can take a look at the Demo \u2197.</p> <p>Argilla UI offers many functions to help you manage your annotation workflow, aiming to provide the most flexible approach to fit the wide variety of use cases handled by the community.</p>"},{"location":"admin_guide/annotate/#annotation-interface-overview","title":"Annotation interface overview","text":""},{"location":"admin_guide/annotate/#flexible-layout","title":"Flexible layout","text":"<p>The UI is responsive with two columns for larger devices and one column for smaller devices. This enables you to annotate data using your mobile phone for simple datasets (i.e., not very long text and 1-2 questions) or resize your screen to get a more compact UI.</p> HeaderLeft paneRight paneLeft bottom panelRight bottom panel <p>At the right side of the navigation breadcrumb, you can customize the dataset settings and edit your profile.</p> <p>This area displays the control panel on the top. The control panel is used for performing keyword-based search, applying filters, and sorting the results.</p> <p>Below the control panel, the record card(s) are displayed one by one (Focus view) or in a vertical list (Bulk view).</p> <p>This is where you annotate your dataset. Simply fill it out as a form, then choose to <code>Submit</code>, <code>Save as Draft</code>, or <code>Discard</code>.</p> <p>This expandable area displays the annotation guidelines. The annotation guidelines can be edited by owner and admin roles in the dataset settings.</p> <p>This expandable area displays your annotation progress.</p>"},{"location":"admin_guide/annotate/#shortcuts","title":"Shortcuts","text":"<p>The Argilla UI includes a range of shortcuts. For the main actions (submit, discard, save as draft and selecting labels) the keys are showed in the corresponding button.</p> <p>To learn how to move from one question to another or between records using the keyboard, take a look at the table below.</p> <p>Shortcuts provide a smoother annotation experience, especially with datasets using a single question (Label, MultiLabel, Rating, or Ranking).</p> Available shortcuts Action Keys Activate form \u21e5 Tab Move between questions \u2193 Down arrow\u00a0or\u00a0\u2191 Up arrow Select and unselect label 1,\u00a02,\u00a03 Move between labels or ranking options \u21e5 Tab\u00a0or\u00a0\u21e7 Shift\u00a0\u21e5 Tab Select rating and rank 1,\u00a02,\u00a03 Fit span to character selection Hold\u00a0\u21e7 Shift Activate text area \u21e7 Shift\u00a0\u21b5 Enter Exit text area Esc Discard \u232b Backspace Save draft (Mac os) \u2318 Cmd\u00a0S Save draft (Other) Ctrl\u00a0S Submit \u21b5 Enter Move between pages \u2192 Right arrow\u00a0or\u00a0\u2190 Left arrow"},{"location":"admin_guide/annotate/#view-by-status","title":"View by status","text":"<p>The view selector is set by default on Pending.</p> <p>If you are starting an annotation effort, all the records are initially kept in the Pending view. Once you start annotating, the records will move to the other queues: Draft, Submitted, Discarded.</p> <ul> <li>Pending: The records without a response.</li> <li>Draft: The records with partial responses. They can be submitted or discarded later. You can\u2019t move them back to the pending queue.</li> <li>Discarded: The records may or may not have responses. They can be edited but you can\u2019t move them back to the pending queue.</li> <li>Submitted: The records have been fully annotated and have already been submitted. You can remove them from this queue and send them to the draft or discarded queues, but never back to the pending queue.</li> </ul> <p>Note</p> <p>If you are working as part of a team, the number of records in your Pending queue may change as other members of the team submit responses and those records get completed.</p> <p>Tip</p> <p>If you are working as part of a team, the records in the draft queue that have been completed by other team members will show a check mark to indicate that there is no need to provide a response.</p>"},{"location":"admin_guide/annotate/#suggestions","title":"Suggestions","text":"<p>If your dataset includes model predictions, you will see them represented by a sparkle icon <code>\u2728</code> in the label or value button. We call them \u201cSuggestions\u201d and they appear in the form as pre-filled responses. If confidence scores have been included by the dataset admin, they will be shown alongside with the label. Additionally, admins can choose to always show suggested labels at the beginning of the list. This can be configured from the dataset settings.</p> <p>If you agree with the suggestions, you just need to click on the <code>Submit</code> button, and they will be considered as your response. If the suggestion is incorrect, you can modify it and submit your final response.</p>"},{"location":"admin_guide/annotate/#focus-view","title":"Focus view","text":"<p>This is the default view to annotate your dataset linearly, displaying one record after another.</p> <p>Tip</p> <p>You should use this view if you have a large number of required questions or need a strong focus on the record content to be labelled. This is also the recommended view for annotating a dataset sample to avoid potential biases introduced by using filters, search, sorting and bulk labelling.</p> <p>Once you submit your first response, the next record will appear automatically. To see again your submitted response, just click on <code>Prev</code>.</p> <p>Navigating through the records</p> <p>To navigate through the records, you can use the\u00a0<code>Prev</code>, shown as\u00a0<code>&lt;</code>, and\u00a0<code>Next</code>,\u00a0<code>&gt;</code> buttons on top of the record card.</p> <p>Each time the page is fully refreshed, the records with modified statuses (Pending to Discarded, Pending to Save as Draft, Pending to Submitted) are sent to the corresponding queue. The control panel displays the status selector, which is set to Pending by default.</p>"},{"location":"admin_guide/annotate/#bulk-view","title":"Bulk view","text":"<p>The bulk view is designed to speed up the annotation and get a quick overview of the whole dataset.</p> <p>The bulk view displays the records in a vertical list. Once this view is active, some functions from the control panel will activate to optimize the view. You can define the number of records to display by page between <code>10</code>, <code>25</code>, <code>50</code>, <code>100</code> and whether records are shown with a fixed (<code>Collapse records</code>) or their natural height (<code>Expand records</code>).</p> <p>Tip</p> <p>You should use this to quickly explore a dataset. This view is also recommended if you have a good understanding of the domain and want to apply your knowledge based on things like similarity and keyword search, filters, and suggestion score thresholds. For a datasets with a large number of required questions or very long fields, the focus view would be more suitable.</p> <p>With multiple questions, think about using the bulk view to annotate massively one question. Then, you can complete the annotation per record from the draft queue.</p> <p>Note</p> <p>Please note that suggestions are not shown in bulk view (except for Spans) and that you will need to save as a draft when you are not providing responses to all required questions.</p>"},{"location":"admin_guide/annotate/#annotation-progress","title":"Annotation progress","text":"<p>You can track the progress of an annotation task in the progress bar shown in the dataset list and in the progress panel inside the dataset. This bar shows the number of records that have been completed (i.e., those that have the minimum number of submitted responses) and those left to be completed.</p> <p>You can also track your own progress in real time expanding the right-bottom panel inside the dataset page. There you can see the number of records for which you have <code>Pending</code>,\u00a0<code>Draft</code>,\u00a0<code>Submitted</code>\u00a0and\u00a0<code>Discarded</code> responses.</p>"},{"location":"admin_guide/annotate/#use-search-filters-and-sort","title":"Use search, filters, and sort","text":"<p>The UI offers various features designed for data exploration and understanding. Combining these features with bulk labelling can save you and your team hours of time.</p> <p>Tip</p> <p>You should use this when you are familiar with your data and have large volumes to annotate based on verified beliefs and experience.</p>"},{"location":"admin_guide/annotate/#search","title":"Search","text":"<p>From the control panel at the top of the left pane, you can search by keyword across the entire dataset. If you have more than one field in your records, you may specify if the search is to be performed \u201cAll\u201d fields or on a specific one. Matched results are highlighted in color.</p>"},{"location":"admin_guide/annotate/#order-by-record-semantic-similarity","title":"Order by record semantic similarity","text":"<p>You can retrieve records based on their similarity to another record if vectors have been added to the dataset.</p> <p>Note</p> <p>Check these guides to know how to add vectors to your\u00a0dataset and\u00a0records.</p> <p>To use the search by semantic similarity function, click on <code>Find similar</code> within the record you wish to use as a reference. If multiple vectors are available, select the desired vector. You can also choose whether to retrieve the most or least similar records.</p> <p>The retrieved records are then ordered by similarity, with the similarity score displayed on each record card.</p> <p>While the semantic search is active, you can update the selected vector or adjust the order of similarity, and specify the number of desired results.</p> <p>To cancel the search, click on the cross icon next to the reference record.</p>"},{"location":"admin_guide/annotate/#filter-and-sort-by-metadata-responses-and-suggestions","title":"Filter and sort by metadata, responses, and suggestions","text":""},{"location":"admin_guide/annotate/#filter","title":"Filter","text":"<p>If the dataset contains metadata, responses and suggestions, click on\u00a0Filter in the control panel to display the available filters. You can select multiple filters and combine them.</p> <p>Note</p> <p>Record info including metadata is visible from the ellipsis menu in the record card.</p> <p>From the <code>Metadata</code> dropdown, type and select the property. You can set a range for integer and float properties, and select specific values for term metadata.</p> <p>Note</p> <p>Note that if a metadata property was set to <code>visible_for_annotators=False</code> this metadata property will only appear in the metadata filter for users with the <code>admin</code> or <code>owner</code> role.</p> <p>From the <code>Responses</code> dropdown, type and select the question. You can set a range for rating questions and select specific values for label, multi-label, and span questions.</p> <p>Note</p> <p>The text and ranking questions are not available for filtering.</p> <p>From the Suggestions dropdown, filter the suggestions by\u00a0<code>Suggestion values</code>,\u00a0<code>Score</code>\u00a0, or\u00a0<code>Agent</code>.\u00a0</p>"},{"location":"admin_guide/annotate/#sort","title":"Sort","text":"<p>You can sort your records according to one or several attributes.</p> <p>The insertion time and last update are general to all records.</p> <p>The suggestion scores, response, and suggestion values for rating questions and metadata properties are available only when they were provided.</p>"},{"location":"admin_guide/dataset/","title":"Dataset management","text":"<p>This guide provides an overview of datasets, explaining the basics of how to set them up and manage them in Argilla.</p> <p>A dataset is a collection of records that you can configure for labelers to provide feedback using the UI. Depending on the specific requirements of your task, you may need various types of feedback. You can customize the dataset to include different kinds of questions, so the first step will be to define the aim of your project and the kind of data and feedback you will need. With this information, you can start configuring a dataset by defining fields, questions, metadata, vectors, and guidelines through settings.</p> Question: Who can manage datasets? <p>Only users with the <code>owner</code> role can manage (create, retrieve, update and delete) all the datasets.</p> <p>The users with the <code>admin</code> role can manage (create, retrieve, update and delete) the datasets in the workspaces they have access to.</p> <p>Main Classes</p> <code>rg.Dataset</code><code>rg.Settings</code> <pre><code>rg.Dataset(\n    name=\"name\",\n    workspace=\"workspace\",\n    settings=settings,\n    client=client\n)\n</code></pre> <p>Check the Dataset - Python Reference to see the attributes, arguments, and methods of the <code>Dataset</code> class in detail.</p> <pre><code>rg.Settings(\n    fields=[rg.TextField(name=\"text\")],\n    questions=[\n        rg.LabelQuestion(\n            name=\"label\",\n            labels=[\"label_1\", \"label_2\", \"label_3\"]\n        )\n    ],\n    metadata=[rg.TermsMetadataProperty(name=\"metadata\")],\n    vectors=[rg.VectorField(name=\"vector\", dimensions=10)],\n    guidelines=\"guidelines\",\n    allow_extra_metadata=True,\n    distribution=rg.TaskDistribution(min_submitted=2),\n)\n</code></pre> <p>Check the Settings - Python Reference to see the attributes, arguments, and methods of the <code>Settings</code> class in detail.</p>"},{"location":"admin_guide/dataset/#create-a-dataset","title":"Create a dataset","text":"<p>To create a dataset, you can define it in the <code>Dataset</code> class and then call the <code>create</code> method that will send the dataset to the server so that it can be visualized in the UI. If the dataset does not appear in the UI, you may need to click the refresh button to update the view. For further configuration of the dataset, you can refer to the settings section.</p> <p>Info</p> <p>If you have deployed Argilla with Hugging Face Spaces and HF Sign in, you can use <code>argilla</code> as a workspace name. Otherwise, you might need to create a workspace following this guide.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nsettings = rg.Settings(\n    guidelines=\"These are some guidelines.\",\n    fields=[\n        rg.TextField(\n            name=\"text\",\n        ),\n    ],\n    questions=[\n        rg.LabelQuestion(\n            name=\"label\",\n            labels=[\"label_1\", \"label_2\", \"label_3\"]\n        ),\n    ],\n)\n\ndataset = rg.Dataset(\n    name=\"my_dataset\",\n    workspace=\"my_workspace\",\n    settings=settings,\n)\n\ndataset.create()\n</code></pre> <p>The created dataset will be empty, to add records go to this how-to guide.</p> <p>Accessing attributes</p> <p>Access the attributes of a dataset by calling them directly on the <code>dataset</code> object. For example, <code>dataset.id</code>, <code>dataset.name</code> or <code>dataset.settings</code>. You can similarly access the fields, questions, metadata, vectors and guidelines. For instance, <code>dataset.fields</code> or <code>dataset.questions</code>.</p>"},{"location":"admin_guide/dataset/#create-multiple-datasets-with-the-same-settings","title":"Create multiple datasets with the same settings","text":"<p>To create multiple datasets with the same settings, define the settings once and pass it to each dataset.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nsettings = rg.Settings(\n    guidelines=\"These are some guidelines.\",\n    fields=[rg.TextField(name=\"text\", use_markdown=True)],\n    questions=[\n        rg.LabelQuestion(name=\"label\", labels=[\"label_1\", \"label_2\", \"label_3\"])\n    ],\n    distribution=rg.TaskDistribution(min_submitted=3),\n)\n\ndataset1 = rg.Dataset(name=\"my_dataset_1\", settings=settings)\ndataset2 = rg.Dataset(name=\"my_dataset_2\", settings=settings)\n\n# Create the datasets on the server\ndataset1.create()\ndataset2.create()\n</code></pre>"},{"location":"admin_guide/dataset/#create-a-dataset-from-an-existing-dataset","title":"Create a dataset from an existing dataset","text":"<p>To create a new dataset from an existing dataset, get the settings from the existing dataset and pass them to the new dataset.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nexisting_dataset = client.datasets(\"my_dataset\")\n\nnew_dataset = rg.Dataset(name=\"my_dataset_copy\", settings=existing_dataset.settings)\n\nnew_dataset.create()\n</code></pre> <p>Info</p> <p>You can also copy the records from the original dataset to the new one:</p> <pre><code>records = list(existing_dataset.records)\nnew_dataset.records.log(records)\n</code></pre>"},{"location":"admin_guide/dataset/#define-dataset-settings","title":"Define dataset settings","text":""},{"location":"admin_guide/dataset/#fields","title":"Fields","text":"<p>The fields in a dataset consist of one or more data items requiring annotation. Currently, Argilla only supports plain text and markdown through the <code>TextField</code>, though we plan to introduce additional field types in future updates.</p> <p>A field is defined in the <code>TextField</code> class that has the following arguments:</p> <ul> <li><code>name</code>: The name of the field.</li> <li><code>title</code> (optional): The name of the field, as it will be displayed in the UI. Defaults to the <code>name</code> value.</li> <li><code>required</code> (optional): Whether the field is required or not. Defaults to <code>True</code>. At least one field must be required.</li> <li><code>use_markdown</code> (optional): Specify whether you want markdown rendered in the UI. Defaults to <code>False</code>. If you set it to True, you will be able to use all the Markdown features for text formatting, including LaTex formulas and embedding multimedia content and PDFs.</li> </ul> <p>Note</p> <p>The order of the fields in the UI follows the order in which these are added to the fields attribute in the Python SDK.</p> <p><pre><code>rg.TextField(\n    name=\"text\",\n    title=\"Text\",\n    required=True,\n    use_markdown=False\n)\n</code></pre> </p>"},{"location":"admin_guide/dataset/#questions","title":"Questions","text":"<p>To collect feedback for your dataset, you need to formulate questions that annotators will be asked to answer. Currently, Argilla supports the following types of questions: <code>LabelQuestion</code>, <code>MultiLabelQuestion</code>, <code>RankingQuestion</code>, <code>RatingQuestion</code>, <code>SpanQuestion</code>, and <code>TextQuestion</code>.</p> LabelMulti-labelRankingRatingSpanText <p>A <code>LabelQuestion</code> asks annotators to choose a unique label from a list of options. This type is useful for text classification tasks. In the UI, they will have a rounded shape. It has the following configuration:</p> <ul> <li><code>name</code>: The name of the question.</li> <li><code>title</code> (optional): The name of the question, as it will be displayed in the UI. Defaults to the <code>name</code> value.</li> <li><code>description</code> (optional): The text to be displayed in the question tooltip in the UI. You can use it to give more context or information to annotators.</li> <li><code>required</code> (optional): Whether the question is required or not. Defaults to <code>True</code>. At least one question must be required.</li> <li><code>labels</code>: A list of strings with the options for these questions. If you'd like the text of the labels to be different in the UI and internally, you can pass a dictionary instead where the key is the internal name and the value will be the text displayed in the UI.</li> </ul> <p><pre><code>rg.LabelQuestion(\n    name=\"label\",\n    title=\"Is the response relevant for the given prompt?\",\n    description=\"Select the one that applies.\",\n    required=True,\n    labels={\"YES\": \"Yes\", \"NO\": \"No\"}, # or [\"YES\", \"NO\"]\n)\n</code></pre> </p> <p>A <code>MultiLabelQuestion</code> asks annotators to choose all applicable labels from a list of options. This type is useful for multi-label text classification tasks. In the UI, they will have a squared shape. It has the following configuration:</p> <ul> <li><code>name</code>: The name of the question.</li> <li><code>title</code> (optional): The name of the question, as it will be displayed in the UI. Defaults to the <code>name</code> value.</li> <li><code>description</code> (optional): The text to be displayed in the question tooltip in the UI. You can use it to give more context or information to annotators.</li> <li><code>required</code> (optional): Whether the question is required or not. Defaults to <code>True</code>. At least one question must be required.</li> <li><code>labels</code>: A list of strings with the options for these questions. If you'd like the text of the labels to be different in the UI and internally, you can pass a dictionary instead where the key is the internal name and the value will be the text displayed in the UI.</li> <li><code>visible_labels</code> (optional): The number of labels that will be visible at first sight in the UI. By default, the UI will show 20 labels and collapse the rest. Set your preferred number to change this limit or set <code>visible_labels=None</code> to show all options.</li> </ul> <p><pre><code>rg.MultiLabelQuestion(\n    name=\"multi_label\",\n    title=\"Does the response include any of the following?\",\n    description=\"Select all that apply.\",\n    required=True,\n    labels={\n        \"hate\": \"Hate Speech\",\n        \"sexual\": \"Sexual content\",\n        \"violent\": \"Violent content\",\n        \"pii\": \"Personal information\",\n        \"untruthful\": \"Untruthful info\",\n        \"not_english\": \"Not English\",\n        \"inappropriate\": \"Inappropriate content\"\n    }, # or [\"hate\", \"sexual\", \"violent\", \"pii\", \"untruthful\", \"not_english\", \"inappropriate\"]\n    visible_labels=4\n)\n</code></pre> </p> <p>A <code>RankingQuestion</code> asks annotators to order a list of options. It is useful to gather information on the preference or relevance of a set of options. Ties are allowed and all options will need to be ranked. It has the following configuration:</p> <ul> <li><code>name</code>: The name of the question.</li> <li><code>title</code> (optional): The name of the question, as it will be displayed in the UI. Defaults to the <code>name</code> value.</li> <li><code>description</code> (optional): The text to be displayed in the question tooltip in the UI. You can use it to give more context or information to annotators.</li> <li><code>required</code> (optional): Whether the question is required or not. Defaults to <code>True</code>. At least one question must be required.</li> <li><code>values</code>: A list of strings with the options they will need to rank. If you'd like the text of the options to be different in the UI and internally, you can pass a dictionary instead where the key is the internal name and the value is the text to display in the UI.</li> </ul> <pre><code>rg.RankingQuestion(\n    name=\"ranking\",\n    title=\"Order replies based on your preference\",\n    description=\"1 = best, 3 = worst. Ties are allowed.\",\n    required=True,\n    values={\n        \"reply-1\": \"Reply 1\",\n        \"reply-2\": \"Reply 2\",\n        \"reply-3\": \"Reply 3\"\n    } # or [\"reply-1\", \"reply-2\", \"reply-3\"]\n)\n</code></pre> <p></p> <p>A <code>RatingQuestion</code> asks annotators to select one option from a list of integer values. This type is useful for collecting numerical scores. It has the following configuration:</p> <ul> <li><code>name</code>: The name of the question.</li> <li><code>title</code> (optional): The name of the question, as it will be displayed in the UI. Defaults to the <code>name</code> value.</li> <li><code>description</code> (optional): The text to be displayed in the question tooltip in the UI. You can use it to give more context or information to annotators.</li> <li><code>required</code> (optional): Whether the question is required or not. Defaults to <code>True</code>. At least one question must be required.</li> <li><code>values</code>: A list of unique integers representing the scores that annotators can select from should be defined within the range [1, 10].</li> </ul> <pre><code>rg.RatingQuestion(\n    name=\"rating\",\n    title=\"How satisfied are you with the response?\",\n    description=\"1 = very unsatisfied, 10 = very satisfied\",\n    required=True,\n    values=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n)\n</code></pre> <p></p> <p>A <code>SpanQuestion</code> asks annotators to select a portion of the text of a specific field and apply a label to it. This type of question is useful for named entity recognition or information extraction tasks. It has the following configuration:</p> <ul> <li><code>name</code>: The name of the question.</li> <li><code>title</code> (optional): The name of the question, as it will be displayed in the UI. Defaults to the <code>name</code> value, but capitalized.</li> <li><code>description</code> (optional): The text to be displayed in the question tooltip in the UI. You can use it to give more context or information to annotators.</li> <li><code>required</code> (optional): Whether the question is required or not. Defaults to <code>True</code>. At least one question must be required.</li> <li><code>labels</code>: A list of strings with the options for these questions. If you'd like the text of the labels to be different in the UI and internally, you can pass a dictionary instead where the key is the internal name and the value will be the text to display in the UI.</li> <li><code>field</code>: This question is always attached to a specific field. You should pass a string with the name of the field where the labels of the <code>SpanQuestion</code> should be used.</li> <li><code>allow_overlapping</code>: This value specifies whether overlapped spans are allowed or not. Defaults to <code>False</code>.</li> <li><code>visible_labels</code> (optional): The number of labels that will be visible at first sight in the UI. By default, the UI will show 20 labels and collapse the rest. Set your preferred number to change this limit or set <code>visible_labels=None</code> to show all options.</li> </ul> <pre><code>rg.SpanQuestion(\n    name=\"span\",\n    title=\"Select the entities in the text\",\n    description=\"Select the entities in the text\",\n    required=True,\n    labels={\n        \"PERSON\": \"Person\",\n        \"ORG\": \"Organization\",\n        \"LOC\": \"Location\",\n        \"MISC\": \"Miscellaneous\"\n    },\n    field=\"text\",\n    allow_overlapping=False,\n    visible_labels=None\n)\n</code></pre> <p></p> <p>A <code>TextQuestion</code> offers to annotators a free-text area where they can enter any text. This type is useful for collecting natural language data, such as corrections or explanations. It has the following configuration:</p> <ul> <li><code>name</code>: The name of the question.</li> <li><code>title</code> (optional): The name of the question, as it will be displayed in the UI. Defaults to the <code>name</code> value, but capitalized.</li> <li><code>description</code> (optional): The text to be displayed in the question tooltip in the UI. You can use it to give more context or information to annotators.</li> <li><code>required</code> (optional): Whether the question is required or not. Defaults to <code>True</code>. At least one question must be required.</li> <li><code>use_markdown</code> (optional): Define whether the field should render markdown text. Defaults to <code>False</code>. If you set it to <code>True</code>, you will be able to use all the Markdown features for text formatting, as well as embed multimedia content and PDFs.</li> </ul> <pre><code>rg.TextQuestion(\n    name=\"text\",\n    title=\"Please provide feedback on the response\",\n    description=\"Please provide feedback on the response\",\n    required=True,\n    use_markdown=True\n)\n</code></pre> <p></p>"},{"location":"admin_guide/dataset/#metadata","title":"Metadata","text":"<p>Metadata properties allow you to configure the use of metadata information for the filtering and sorting features available in the UI and Python SDK. There exist three types of metadata you can add: <code>TermsMetadataProperty</code>, <code>IntegerMetadataProperty</code> and <code>FloatMetadataProperty</code>.</p> TermsIntegerFloat <p>A <code>TermsMetadataProperty</code> allows to add a list of strings as metadata options. It has the following configuration:</p> <ul> <li><code>name</code>: The name of the metadata property.</li> <li><code>title</code> (optional): The name of the metadata property, as it will be displayed in the UI. Defaults to the <code>name</code> value, but capitalized.</li> <li><code>options</code> (optional): You can pass a list of valid values for this metadata property, in case you want to run any validation.</li> </ul> <p><pre><code>rg.TermsMetadataProperty(\n    name=\"terms\",\n    title=\"Annotation groups\",\n    options=[\"group-a\", \"group-b\", \"group-c\"]\n)\n</code></pre> </p> <p>An <code>IntegerMetadataProperty</code> allows to add integer values as metadata. It has the following configuration:</p> <ul> <li><code>name</code>: The name of the metadata property.</li> <li><code>title</code> (optional): The name of the metadata property, as it will be displayed in the UI. Defaults to the <code>name</code> value, but capitalized.</li> <li><code>min</code> (optional): You can pass a minimum valid value. If none is provided, the minimum value will be computed from the values provided in the records.</li> <li><code>max</code> (optional): You can pass a maximum valid value. If none is provided, the maximum value will be computed from the values provided in the records.</li> </ul> <p><pre><code>rg.IntegerMetadataProperty(\n    name=\"integer\",\n    title=\"length-input\",\n    min=42,\n    max=1984,\n)\n</code></pre> </p> <p>A <code>FloatMetadataProperty</code> allows to add float values as metadata. It has the following configuration:</p> <ul> <li><code>name</code>: The name of the metadata property.</li> <li><code>title</code> (optional): The name of the metadata property, as it will be displayed in the UI. Defaults to the <code>name</code> value, but capitalized.</li> <li><code>min</code> (optional): You can pass a minimum valid value. If none is provided, the minimum value will be computed from the values provided in the records.</li> <li><code>max</code> (optional): You can pass a maximum valid value. If none is provided, the maximum value will be computed from the values provided in the records.</li> </ul> <p><pre><code>rg.FloatMetadataProperty(\n    name=\"float\",\n    title=\"Reading ease\",\n    min=-92.29914,\n    max=119.6975,\n)\n</code></pre> </p> <p>Note</p> <p>You can also set the <code>allow_extra_metadata</code> argument in the dataset to <code>True</code> to specify whether the dataset will allow metadata fields in the records other than those specified under metadata. Note that these will not be accessible from the UI for any user, only retrievable using the Python SDK.</p>"},{"location":"admin_guide/dataset/#vectors","title":"Vectors","text":"<p>To use the similarity search in the UI and the Python SDK, you will need to configure vectors using the <code>VectorField</code> class. It has the following configuration:</p> <ul> <li><code>name</code>: The name of the vector.</li> <li><code>title</code> (optional): A name for the vector to display in the UI for better readability.</li> <li><code>dimensions</code>: The dimensions of the vectors used in this setting.</li> </ul> <p><pre><code>rg.VectorField(\n    name=\"my_vector\",\n    title=\"My Vector\",\n    dimensions=768\n),\n</code></pre> </p>"},{"location":"admin_guide/dataset/#guidelines","title":"Guidelines","text":"<p>Once you have decided on the data to show and the questions to ask, it's important to provide clear guidelines to the annotators. These guidelines help them understand the task and answer the questions consistently. You can provide guidelines in two ways:</p> <ul> <li> <p>In the dataset guidelines: this is added as an argument when you create your dataset in the Python SDK. It will appear in the dataset settings in the UI. <pre><code>guidelines = \"In this dataset, you will find a collection of records that show a category, an instruction, a context and a response to that instruction. [...]\"\n</code></pre> </p> </li> <li> <p>As question descriptions: these are added as an argument when you create questions in the Python SDK. This text will appear in a tooltip next to the question in the UI. </p> </li> </ul> <p>It is good practice to use at least the dataset guidelines if not both methods. Question descriptions should be short and provide context to a specific question. They can be a summary of the guidelines to that question, but often that is not sufficient to align the whole annotation team. In the guidelines, you can include a description of the project, details on how to answer each question with examples, instructions on when to discard a record, etc.</p> <p>Tip</p> <p>If you want further guidance on good practices for guidelines during the project development, check our blog post.</p>"},{"location":"admin_guide/dataset/#distribution","title":"Distribution","text":"<p>When working as a team, you may want to distribute the annotation task to ensure efficiency and quality. You can use the\u00a0<code>TaskDistribution</code> settings to configure the number of minimum submitted responses expected for each record. Argilla will use this setting to automatically handle records in your team members' pending queues.</p> <pre><code>rg.TaskDistribution(\n    min_submitted = 2\n)\n</code></pre> <p>To learn more about how to distribute the task among team members in the Distribute the annotation guide.</p>"},{"location":"admin_guide/dataset/#list-datasets","title":"List datasets","text":"<p>You can list all the datasets available in a workspace using the <code>datasets</code> attribute of the <code>Workspace</code> class. You can also use <code>len(workspace.datasets)</code> to get the number of datasets in a workspace.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nworkspace = client.workspaces(\"my_workspace\")\n\ndatasets = workspace.datasets\n\nfor dataset in datasets:\n    print(dataset)\n</code></pre> <p>When you list datasets, dataset settings are not preloaded, since this can introduce extra requests to the server. If you want to work with settings when listing datasets, you need to load them: <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nfor dataset in client.datasets:\n    dataset.settings.get() # this will get the dataset settings from the server\n    print(dataset.settings)\n</code></pre></p> <p>Notebooks</p> <p>When using a notebook, executing <code>client.datasets</code> will display a table with the <code>name</code>of the existing datasets, the <code>id</code>, <code>workspace_id</code> to which they belong, and the last update as <code>updated_at</code>. .</p>"},{"location":"admin_guide/dataset/#retrieve-a-dataset","title":"Retrieve a dataset","text":"<p>You can retrieve a dataset by calling the <code>datasets</code> method on the <code>Argilla</code> class and passing the <code>name</code> or <code>id</code> of the dataset as an argument. If the dataset does not exist, a warning message will be raised and <code>None</code> will be returned.</p> By nameBy id <p>By default, this method attempts to retrieve the dataset from the first workspace. If the dataset is in a different workspace, you must specify either the workspace or workspace name as an argument.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\n# Retrieve the dataset from the first workspace\nretrieved_dataset = client.datasets(name=\"my_dataset\")\n\n# Retrieve the dataset from the specified workspace\nretrieved_dataset = client.datasets(name=\"my_dataset\", workspace=\"my_workspace\")\n</code></pre> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\ndataset = client.datasets(id=\"&lt;uuid-or-uuid-string&gt;\")\n</code></pre>"},{"location":"admin_guide/dataset/#check-dataset-existence","title":"Check dataset existence","text":"<p>You can check if a dataset exists. The <code>client.datasets</code> method will return <code>None</code> if the dataset was not found.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\ndataset = client.datasets(name=\"my_dataset\")\n\nif dataset is not None:\n    pass\n</code></pre>"},{"location":"admin_guide/dataset/#update-a-dataset","title":"Update a dataset","text":"<p>Once a dataset is published, there are limited things you can update. Here is a summary of the attributes you can change for each setting:</p> FieldsQuestionsMetadataVectorsGuidelinesDistribution Attributes From SDK From UI Name \u274c \u274c Title \u2705 \u2705 Required \u274c \u274c Use markdown \u2705 \u2705 Attributes From SDK From UI Name \u274c \u274c Title \u274c \u2705 Description \u274c \u2705 Required \u274c \u274c Labels \u274c \u274c Values \u274c \u274c Label order \u274c \u2705 Suggestions first \u274c \u2705 Visible labels \u274c \u2705 Field \u274c \u274c Allow overlapping \u274c \u274c Use markdown \u274c \u2705 Attributes From SDK From UI Name \u274c \u274c Title \u2705 \u2705 Options \u274c \u274c Minimum value \u274c \u274c Maximum value \u274c \u274c Visible for annotators \u2705 \u2705 Allow extra metadata \u2705 \u2705 Attributes From SDK From UI Name \u274c \u274c Title \u2705 \u2705 Dimensions \u274c \u274c From SDK From UI \u2705 \u2705 Attributes From SDK From UI Minimum submitted \u2705* \u2705* <p>* Can be changed as long as the dataset doesn't have any responses.</p> <p>To modify these attributes, you can simply set the new value of the attributes you wish to change and call the <code>update</code> method on the <code>Dataset</code> object.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\ndataset = client.datasets(\"my_dataset\")\n\ndataset.settings.fields[\"text\"].use_markdown = True\ndataset.settings.metadata[\"my_metadata\"].visible_for_annotators = False\n\ndataset.update()\n</code></pre> <p>You can also add and delete metadata properties and vector fields using the <code>add</code> and <code>delete</code> methods.</p> AddDelete <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\ndataset = client.datasets(\"my_dataset\")\n\ndataset.settings.vectors.add(rg.VectorField(name=\"my_new_vector\", dimensions=123))\ndataset.settings.metadata.add(\n    rg.TermsMetadataProperty(\n        name=\"my_new_metadata\",\n        options=[\"option_1\", \"option_2\", \"option_3\"],\n    ),\n)\ndataset.update()\n</code></pre> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\ndataset = client.datasets(\"my_dataset\")\n\ndataset.settings.vectors[\"my_old_vector\"].delete()\ndataset.settings.metadata[\"my_old_metadata\"].delete()\n\ndataset.update()\n</code></pre>"},{"location":"admin_guide/dataset/#delete-a-dataset","title":"Delete a dataset","text":"<p>You can delete an existing dataset by calling the <code>delete</code> method on the <code>Dataset</code> class.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\ndataset_to_delete = client.datasets(name=\"my_dataset\")\n\ndataset_deleted = dataset_to_delete.delete()\n</code></pre>"},{"location":"admin_guide/distribution/","title":"Distribute the annotation task among the team","text":"<p>This guide explains how you can use Argilla\u2019s automatic task distribution to efficiently divide the task of annotating a dataset among multiple team members.</p> <p>Owners and admins can define the minimum number of submitted responses expected for each record. Argilla will use this setting to handle automatically the records that will be shown in the pending queues of all users with access to the dataset.</p> <p>When a record has met the minimum number of submissions, the status of the record will change to <code>completed</code>, and the record will be removed from the <code>Pending</code> queue of all team members so they can focus on providing responses where they are most needed. The dataset\u2019s annotation task will be fully completed once all records have the <code>completed</code> status.</p> <p></p> <p>Note</p> <p>The status of a record can be either <code>completed</code>, when it has the required number of responses with <code>submitted</code> status, or <code>pending</code>, when it doesn\u2019t meet this requirement.</p> <p>Each record can have multiple responses, and each of those can have the status <code>submitted</code>, <code>discarded</code>, or <code>draft.</code></p> <p>Main Class</p> <pre><code>rg.TaskDistribution(\n    min_submitted = 2\n)\n</code></pre> <p>Check the Task Distribution - Python Reference to see the attributes, arguments, and methods of the <code>TaskDistribution</code> class in detail.</p>"},{"location":"admin_guide/distribution/#configure-task-distribution-settings","title":"Configure task distribution settings","text":"<p>By default, Argilla will set the required minimum submitted responses to 1. This means that whenever a record has at least 1 response with the status <code>submitted</code> the status of the record will be <code>completed</code> and removed from the <code>Pending</code> queue of other team members.</p> <p>Tip</p> <p>Leave the default value of minimum submissions (1) if you are working on your own or when you don't require more than one submitted response per record.</p> <p>If you wish to set a different number, you can do so through the <code>distribution</code> setting in your dataset settings:</p> <pre><code>settings = rg.Settings(\n    guidelines=\"These are some guidelines.\",\n    fields=[\n        rg.TextField(\n            name=\"text\",\n        ),\n    ],\n    questions=[\n        rg.LabelQuestion(\n            name=\"label\",\n            labels=[\"label_1\", \"label_2\", \"label_3\"]\n        ),\n    ],\n    distribution=rg.TaskDistribution(min_submitted=3)\n)\n</code></pre> <p>Learn more about configuring dataset settings in the Dataset management guide.</p> <p>Tip</p> <p>Increase the number of minimum subsmissions if you\u2019d like to ensure you get more than one submitted response per record. Make sure that this number is never higher than the number of members in your team. Note that the lower this number is, the faster the task will be completed.</p> <p>Note</p> <p>Note that some records may have more responses than expected if multiple team members submit responses on the same record simultaneously.</p>"},{"location":"admin_guide/distribution/#change-task-distribution-settings","title":"Change task distribution settings","text":"<p>If you wish to change the minimum submitted responses required in a dataset, you can do so as long as the annotation hasn\u2019t started, i.e., the dataset has no responses for any records.</p> <p>Admins and owners can change this value from the dataset settings page in the UI or from the SDK:</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\ndataset = client.datasets(\"my_dataset\")\n\ndataset.settings.distribution.min_submitted = 4\n\ndataset.update()\n</code></pre>"},{"location":"admin_guide/docker_deployment/","title":"Extralit System Installation Guide","text":"<p>This guide will walk you through the process of setting up the Extralit system end-to-end.</p>"},{"location":"admin_guide/docker_deployment/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>Python 3.9 or later</li> <li>Docker (for containerized deployment)</li> <li>Kubernetes cluster (for scalable deployment)</li> <li>Access to a PostgreSQL database</li> <li>Access to an Elasticsearch instance</li> <li>MinIO or S3-compatible object storage</li> </ul>"},{"location":"admin_guide/import_export/","title":"Importing and exporting datasets and records","text":"<p>This guide provides an overview of how to import and export your dataset or its records to Python, your local disk, or the Hugging Face Hub.</p> <p>In Argilla, you can import/export two main components of a dataset:</p> <ul> <li>The dataset's complete configuration is defined in <code>rg.Settings</code>. This is useful if you want to share your feedback task or restore it later in Argilla.</li> <li>The records stored in the dataset, including <code>Metadata</code>, <code>Vectors</code>, <code>Suggestions</code>, and <code>Responses</code>. This is useful if you want to use your dataset's records outside of Argilla.</li> </ul> <p>Check the Dataset - Python Reference to see the attributes, arguments, and methods of the export <code>Dataset</code> class in detail.</p> <p>Main Classes</p> <code>rg.Dataset.to_hub</code><code>rg.Dataset.from_hub</code><code>rg.Dataset.to_disk</code><code>rg.Dataset.from_disk</code><code>rg.Dataset.records.to_datasets()</code><code>rg.Dataset.records.to_dict()</code><code>rg.Dataset.records.to_list()</code> <pre><code>rg.Dataset.to_hub(\n    repo_id=\"&lt;my_org&gt;/&lt;my_dataset&gt;\",\n    with_records=True,\n    generate_card=True\n)\n</code></pre> <pre><code>rg.Dataset.from_hub(\n    repo_id=\"&lt;my_org&gt;/&lt;my_dataset&gt;\",\n    name=\"my_dataset\",\n    workspace=\"my_workspace\",\n    client=rg.Client(),\n    with_records=True\n)\n</code></pre> <pre><code>rg.Dataset.to_disk(\n    path=\"&lt;path-empty-directory&gt;\",\n    with_records=True\n)\n</code></pre> <pre><code>rg.Dataset.from_disk(\n    path=\"&lt;path-dataset-directory&gt;\",\n    name=\"my_dataset\",\n    workspace=\"my_workspace\",\n    client=rg.Client(),\n    with_records=True\n)\n</code></pre> <pre><code>rg.Dataset.records.to_datasets()\n</code></pre> <pre><code>rg.Dataset.records.to_dict()\n</code></pre> <pre><code>rg.Dataset.records.to_list()\n</code></pre> <p>Check the Dataset - Python Reference to see the attributes, arguments, and methods of the export <code>Dataset</code> class in detail.</p> <p>Check the Record - Python Reference to see the attributes, arguments, and methods of the <code>Record</code> class in detail.</p>"},{"location":"admin_guide/import_export/#importing-and-exporting-datasets","title":"Importing and exporting datasets","text":"<p>First, we will go through exporting a complete dataset from Argilla. This includes the dataset's settings and records. All of these methods use the <code>rg.Dataset.from_*</code> and <code>rg.Dataset.to_*</code> methods.</p>"},{"location":"admin_guide/import_export/#hugging-face-hub","title":"Hugging Face Hub","text":""},{"location":"admin_guide/import_export/#export-to-hub","title":"Export to Hub","text":"<p>You can push a dataset from Argilla to the Hugging Face Hub. This is useful if you want to share your dataset with the community or version control it. You can push the dataset to the Hugging Face Hub using the <code>rg.Dataset.to_hub</code> method.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\ndataset = client.datasets(name=\"my_dataset\")\n\ndataset.to_hub(repo_id=\"&lt;my_org&gt;/&lt;my_dataset&gt;\")\n</code></pre> <p>With or without records</p> <p>The example above will push the dataset's <code>Settings</code> and records to the hub. If you only want to push the dataset's configuration, you can set the <code>with_records</code> parameter to <code>False</code>. This is useful if you're just interested in a specific dataset template or you want to make changes in the dataset settings and/or records.</p> <pre><code>dataset.to_hub(repo_id=\"&lt;my_org&gt;/&lt;my_dataset&gt;\", with_records=False)\n</code></pre>"},{"location":"admin_guide/import_export/#import-from-hub","title":"Import from Hub","text":"<p>You can pull a dataset from the Hugging Face Hub to Argilla. This is useful if you want to restore a dataset and its configuration. You can pull the dataset from the Hugging Face Hub using the <code>rg.Dataset.from_hub</code> method.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\ndataset = rg.Dataset.from_hub(repo_id=\"&lt;my_org&gt;/&lt;my_dataset&gt;\")\n</code></pre> <p>The <code>rg.Dataset.from_hub</code> method loads the configuration and records from the dataset repo. If you only want to load records, you can pass a <code>datasets.Dataset</code> object to the <code>rg.Dataset.log</code> method. This enables you to configure your own dataset and reuse existing Hub datasets. See the guide on records for more information.</p> <p>With or without records</p> <p>The example above will pull the dataset's <code>Settings</code> and records from the hub. If you only want to pull the dataset's configuration, you can set the <code>with_records</code> parameter to <code>False</code>. This is useful if you're just interested in a specific dataset template or you want to make changes in the dataset settings and/or records.</p> <pre><code>dataset = rg.Dataset.from_hub(repo_id=\"&lt;my_org&gt;/&lt;my_dataset&gt;\", with_records=False)\n</code></pre> <p>With the dataset's configuration, you could then make changes to the dataset. For example, you could adapt the dataset's settings for a different task:</p> <pre><code>dataset.settings.questions = [rg.TextQuestion(name=\"answer\")]\ndataset.update()\n</code></pre> <p>You could then log the dataset's records using the <code>load_dataset</code> method of the <code>datasets</code> package and pass the dataset to the <code>rg.Dataset.log</code> method.</p> <pre><code>hf_dataset = load_dataset(\"&lt;my_org&gt;/&lt;my_dataset&gt;\")\ndataset.records.log(hf_dataset)\n</code></pre>"},{"location":"admin_guide/import_export/#local-disk","title":"Local Disk","text":""},{"location":"admin_guide/import_export/#export-to-disk","title":"Export to Disk","text":"<p>You can save a dataset from Argilla to your local disk. This is useful if you want to back up your dataset. You can use the <code>rg.Dataset.to_disk</code> method. We recommend you to use an empty directory.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\ndataset = client.datasets(name=\"my_dataset\")\n\ndataset.to_disk(path=\"&lt;path-empty-directory&gt;\")\n</code></pre> <p>This will save the dataset's configuration and records to the specified path. If you only want to save the dataset's configuration, you can set the <code>with_records</code> parameter to <code>False</code>.</p> <pre><code>dataset.to_disk(path=\"&lt;path-empty-directory&gt;\", with_records=False)\n</code></pre>"},{"location":"admin_guide/import_export/#import-from-disk","title":"Import from Disk","text":"<p>You can load a dataset from your local disk to Argilla. This is useful if you want to restore a dataset's configuration. You can use the <code>rg.Dataset.from_disk</code> method.</p> <pre><code>import argilla as rg\n\ndataset = rg.Dataset.from_disk(path=\"&lt;path-dataset-directory&gt;\")\n</code></pre> <p>Directing the dataset to a name and workspace</p> <p>You can also specify the name and workspace of the dataset when loading it from the disk.</p> <pre><code>dataset = rg.Dataset.from_disk(path=\"&lt;path-dataset-directory&gt;\", name=\"my_dataset\", workspace=\"my_workspace\")\n</code></pre>"},{"location":"admin_guide/import_export/#importing-and-exporting-records","title":"Importing and exporting records","text":"<p>The records alone can be exported from a dataset in Argilla.  This is useful if you want to process the records in Python, export them to a different platform, or use them in model training. All of these methods use the <code>rg.Dataset.records</code> attribute.</p>"},{"location":"admin_guide/import_export/#export-records","title":"Export records","text":"<p>The records can be exported as a dictionary, a list of dictionaries, or a <code>Dataset</code> of the <code>datasets</code> package.</p> To a python dictionaryTo a python listTo the <code>datasets</code> package <p>Records can be exported from <code>Dataset.records</code> as a dictionary. The <code>to_dict</code> method can be used to export records as a dictionary. You can specify the orientation of the dictionary output. You can also decide if to flatten or not the dictionary.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\ndataset = client.datasets(name=\"my_dataset\")\n\n# Export records as a dictionary\nexported_records = dataset.records.to_dict()\n# {'fields': [{'text': 'Hello'},{'text': 'World'}], suggestions': [{'label': {'value': 'positive'}}, {'label': {'value': 'negative'}}]\n\n# Export records as a dictionary with orient=index\nexported_records = dataset.records.to_dict(orient=\"index\")\n# {\"uuid\": {'fields': {'text': 'Hello'}, 'suggestions': {'label': {'value': 'positive'}}}, {\"uuid\": {'fields': {'text': 'World'}, 'suggestions': {'label': {'value': 'negative'}}},\n\n# Export records as a dictionary with flatten=True\nexported_records = dataset.records.to_dict(flatten=True)\n# {\"text\": [\"Hello\", \"World\"], \"label.suggestion\": [\"greeting\", \"greeting\"]}\n</code></pre> <p>Records can be exported from <code>Dataset.records</code> as a list of dictionaries. The <code>to_list</code> method can be used to export records as a list of dictionaries. You can decide if to flatten it or not.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nworkspace = client.workspaces(\"my_workspace\")\n\ndataset = client.datasets(name=\"my_dataset\", workspace=workspace)\n\n# Export records as a list of dictionaries\nexported_records = dataset.records.to_list()\n# [{'fields': {'text': 'Hello'}, 'suggestion': {'label': {value: 'greeting'}}}, {'fields': {'text': 'World'}, 'suggestion': {'label': {value: 'greeting'}}}]\n\n# Export records as a list of dictionaries with flatten=True\nexported_records = dataset.records.to_list(flatten=True)\n# [{\"text\": \"Hello\", \"label\": \"greeting\"}, {\"text\": \"World\", \"label\": \"greeting\"}]\n</code></pre> <p>Records can be exported from <code>Dataset.records</code> to the <code>datasets</code> package. The <code>to_dataset</code> method can be used to export records to the <code>datasets</code> package. You can specify the name of the dataset and the split to export the records.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\ndataset = client.datasets(name=\"my_dataset\")\n\n# Export records as a dictionary\nexported_dataset = dataset.records.to_datasets()\n</code></pre>"},{"location":"admin_guide/import_export/#import-records","title":"Import records","text":"<p>To import records to a dataset, use the <code>rg.Datasets.records.log</code> method. There is a guide on how to do this in How-to guides - Record, or you can check the Record - Python Reference.</p>"},{"location":"admin_guide/k8s_deployment/","title":"Kubernetes Development Setup","text":""},{"location":"admin_guide/k8s_deployment/#overview","title":"Overview","text":"<p>The Extralit system consists of multiple microservices:</p> <ul> <li>argilla-server: Web server for data annotation, dataset management, and extraction services</li> <li>extralit-server: API server for data extraction, PDF parsing, and schema generation</li> <li>Postgres database: Main database for extracted data and user accounts</li> <li>Elasticsearch: Search engine for data records</li> <li>Minio/S3: File storage for schemas, PDFs, and intermediate outputs</li> <li>Weaviate: Vector database for text and table sections</li> <li>Langfuse: LLM instrumentation service for query tracing and logging</li> </ul>"},{"location":"admin_guide/k8s_deployment/#setup-and-configuration","title":"Setup and Configuration","text":"<ol> <li> <p>Install required tools:</p> <ul> <li>kubectl</li> <li>Tilt</li> <li>kind (for local clusters)</li> </ul> </li> <li> <p>Create a local Kubernetes cluster:     <pre><code>kind create cluster --name {cluster_name}\n</code></pre></p> </li> <li> <p>Clone the repository:     <pre><code>git clone https://github.com/extralit/extralit.git\ncd extralit\n</code></pre></p> </li> <li> <p>Set up the Kubernetes cluster:      Local <code>kind</code> Cluster Setup <p>Install additional tools: - ctlptl</p> <p>Create cluster and local registry: <pre><code>ctlptl create registry ctlptl-registry --port=5005\nctlptl create cluster {cluster_name} --registry=ctlptl-registry\n</code></pre></p> <p>Apply storage configurations: <pre><code>ctlptl apply -f k8s/kind/kind-config.yaml\nkubectl --context kind-kind taint node kind-control-plane node-role.kubernetes.io/control-plane:NoSchedule-\n</code></pre></p> <p>Note: Tilt will apply <code>k8s/kind/tilt-local-dev-kind-storage-policy.yaml</code> for local storage class.</p> <li> <p>Configure cluster context:     <pre><code>kubectl config set-cluster {cluster_name}\nkubectl config use-context {cluster_context}\n</code></pre></p> </li>"},{"location":"admin_guide/k8s_deployment/#deploying-services","title":"Deploying Services","text":"<p>Set environment variables: - <code>ENV</code>: <code>dev</code> for development, <code>prod</code> for production - <code>NAMESPACE</code>: Kubernetes namespace for services - <code>DOCKER_REPO</code>: Docker repository URL - <code>CLUSTER_CONTEXT</code>: Kubernetes cluster context</p> <ol> <li> <p>Create namespace:     <pre><code>kubectl create ns {NAMESPACE}\n</code></pre></p> </li> <li> <p>Set up secrets:     <pre><code>kubectl apply -f extralit-secrets.yaml -n {NAMESPACE}\nkubectl apply -f langfuse-secrets.yaml -n {NAMESPACE}\nkubectl apply -f weaviate-api-keys.yaml -n {NAMESPACE}\n</code></pre></p> </li> <li> <p>Deploy services:</p> <p>We use Tilt to manage the development environment and deploys services to Kubernetes by providing a single command to build, deploy, and monitor services.</p> <pre><code>ENV={ENV} DOCKER_REPO={DOCKER_REPO} tilt up --namespace {NAMESPACE} --context {CLUSTER_CONTEXT}\n</code></pre> <p>If you set <code>DOCKER_REPO</code>, Tilt will automatically handles building, tagging, pushing, and pulling Docker images.</p> <p> Troubleshooting PV issues on local clusters <p>Deploy services iteratively: <pre><code>ENV=dev DOCKER_REPO=localhost:5005 tilt up --namespace {NAMESPACE} --context {CLUSTER_CONTEXT} elasticsearch\nENV=dev DOCKER_REPO=localhost:5005 tilt up --namespace {NAMESPACE} --context {CLUSTER_CONTEXT} main-db\nENV=dev DOCKER_REPO=localhost:5005 tilt up --namespace {NAMESPACE} --context {CLUSTER_CONTEXT} minio\nENV=dev DOCKER_REPO=localhost:5005 tilt up --namespace {NAMESPACE} --context {CLUSTER_CONTEXT} weaviate\nENV=dev DOCKER_REPO=localhost:5005 tilt up --namespace {NAMESPACE} --context {CLUSTER_CONTEXT}\n</code></pre> </p> <li> <p>Monitor deployment in Tilt UI: <code>http://localhost:10350</code></p> </li>"},{"location":"admin_guide/k8s_deployment/#user-account-setup","title":"User Account Setup","text":"<p>Run the <code>start_argilla_server.sh</code> script for initial setup. Manage users with <code>argilla_server</code> CLI:</p> <pre><code>ARGILLA_DATABASE_URL=postgresql+asyncpg://postgres:$POSTGRES_PASSWORD@$POSTGRES_HOST/postgres \\\nARGILLA_LOCAL_AUTH_USERS_DB_FILE=path/to/users.yaml \\\nargilla_server database users migrate\n</code></pre>"},{"location":"admin_guide/k8s_deployment/#frontend-development","title":"Frontend Development","text":"<ol> <li> <p>Set up and run frontend:    <pre><code>cd argilla/argilla-frontend\nnpm install\nAPI_BASE_URL=http://path.to.server npm run dev\n</code></pre></p> </li> <li> <p>Update frontend:    <pre><code>sh scripts/build_frontend.sh\n</code></pre></p> </li> <li> <p>Rebuild <code>argilla-server-deployment</code> in Tilt UI</p> </li> </ol>"},{"location":"admin_guide/k8s_deployment/#backend-development","title":"Backend Development","text":"<p>Changes to <code>src/argilla_server/</code> or <code>src/extralit/</code> are automatically updated while running Tilt with <code>ENV=dev</code>. Manually rebuild if needed.</p>"},{"location":"admin_guide/k8s_deployment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin_guide/k8s_deployment/#persistent-volume-storage-clases","title":"Persistent Volume &amp; storage clases","text":"<p>The services are configured to use persistent volumes (PVs) to store data, such as the Postgres database, Elasticsearch data, Minio files, and Weaviate vector embeddings. The PVs are created with a storage class, which currently is hard-coded to <code>k8s-storage-policy</code>. The storage class is defined in the K8s yaml files for each service, such as <code>main-db</code>, <code>elasticsearch</code>, <code>minio</code>, and <code>weaviate</code>.</p> <p>However these issues can arise: - The PVs are not available when the services are deployed, causing the services to fail to start. This especially happens on the <code>kind</code> cluster, the PV were not created automatically before the service is deployed, so the <code>k8s/kind/tilt-local-dev-kind-storage-policy.yaml</code> file has to manually create the PV. - Some Persistent Volume Claim (PVC) may bound to the incorrect PV, depending on the order they were created, causing issues where the expected storage path doesn't match requirements of the . You may check the <code>uncategorized</code> resource under <code>unlabeled</code> in the Tilt web interface to see if the PVs were created, or restart this resource to recreate the PVs, but it doesn't always fix it. Sometimes <code>rm -rf /tmp/kind-volumes/</code> and restarting the <code>kind</code> cluster is needed for redeployment of elasticsearch and postgres. - Many of the Persistent Volume definitions were initially setup with a modest size, 4Gi to 9Gi. This will need to be resized to support higher data storage requirements.</p>"},{"location":"admin_guide/k8s_deployment/#deployment","title":"Deployment","text":"<p>Check the Tilt web interface for services that are not green in deployment status. Services that often fail to deploy when the cluster restarts or the pods are restarted are: - <code>elasticsearch</code>: This helm chart often fails to redeploy due to the <code>elasticsearch-master-0</code> worker pod not being able reach green status due to issues with the data-shards in the persistent volume when the deployment restart. This can be fixed by deleting the <code>elasticsearch-master-0</code> pod, the <code>elasticsearch-master-elasticsearch-master-0</code> PVC and PV and allowing it to recreate a new database. - <code>main-db</code> Postgres: This service can fail to redeploy due to the <code>main-db-0</code> pod not being able to mount the original persistent volume when the helm chart is redeployed, because it generated a new random password that was different from the original password. Fix it by changing the <code>posgres-password</code> to original password in the <code>main-db</code> K8s secret.</p>"},{"location":"admin_guide/k8s_deployment/#data-persistence","title":"Data persistence","text":"<ul> <li><code>elasticsearch</code>: Same issue described above causes the data index to be lost when the <code>elasticsearch-master-0</code> pod is recreated. The data index can be restored with persistent data in the <code>main-db</code> Postgres database by reindexing the data with the <code>argilla_server</code> CLI tool, see check_search_engine.sh.</li> <li><code>minio</code>: As a standalone pod in the K8s cluster for file blob storage, the Minio service is not automatically backed up. The data in the Minio bucket can be lost if the pod is deleted or the cluster fails in anyway. The data can be restored by re-uploading the data to the Minio bucket.</li> </ul> <p>For support, join the Extralit Slack channel.</p>"},{"location":"admin_guide/migrate_from_legacy_datasets/","title":"Migrate your legacy datasets to Argilla V2","text":"<p>This guide will help you migrate task specific datasets to Argilla V2. These do not include the <code>FeedbackDataset</code> which is just an interim naming convention for the latest extensible dataset. Task specific datasets are datasets that are used for a specific task, such as text classification, token classification, etc. If you would like to learn about the backstory of SDK this migration, please refer to the SDK migration blog post.</p> <p>Note</p> <p>Legacy Datasets include: <code>DatasetForTextClassification</code>, <code>DatasetForTokenClassification</code>, and <code>DatasetForText2Text</code>.</p> <p><code>FeedbackDataset</code>'s do not need to be migrated as they are already in the Argilla V2 format.</p> <p>To follow this guide, you will need to have the following prerequisites:</p> <ul> <li>An argilla 1.* server instance running with legacy datasets.</li> <li>An argilla &gt;=1.29 server instance running. If you don't have one, you can create one by following the Argilla installation guide.</li> <li>The <code>argilla</code> sdk package installed in your environment.</li> </ul> <p>If your current legacy datasets are on a server with Argilla release after 1.29, you could chose to recreate your legacy datasets as new datasets on the same server. You could then upgrade the server to Argilla 2.0 and carry on working their. Your legacy datasets will not be visible on the new server, but they will remain in storage layers if you need to access them.</p>"},{"location":"admin_guide/migrate_from_legacy_datasets/#steps","title":"Steps","text":"<p>The guide will take you through three steps:</p> <ol> <li>Retrieve the legacy dataset from the Argilla V1 server using the new <code>argilla</code> package.</li> <li>Define the new dataset in the Argilla V2 format.</li> <li>Upload the dataset records to the new Argilla V2 dataset format and attributes.</li> </ol>"},{"location":"admin_guide/migrate_from_legacy_datasets/#step-1-retrieve-the-legacy-dataset","title":"Step 1: Retrieve the legacy dataset","text":"<p>Connect to the Argilla V1 server via the new <code>argilla</code> package. First, you should install an extra dependency: <pre><code>pip install \"argilla[legacy]\"\n</code></pre></p> <p>Now, you can use the <code>v1</code> module to connect to the Argilla V1 server.</p> <pre><code>import argilla.v1 as rg_v1\n\n# Initialize the API with an Argilla server less than 2.0\napi_url = \"&lt;your-url&gt;\"\napi_key = \"&lt;your-api-key&gt;\"\nrg_v1.init(api_url, api_key)\n</code></pre> <p>Next, load the dataset settings and records from the Argilla V1 server:</p> <pre><code>dataset_name = \"news-programmatic-labeling\"\nworkspace = \"demo\"\n\nsettings_v1 = rg_v1.load_dataset_settings(dataset_name, workspace)\nrecords_v1 = rg_v1.load(dataset_name, workspace)\nhf_dataset = records_v1.to_datasets()\n</code></pre> <p>Your legacy dataset is now loaded into the <code>hf_dataset</code> object.</p>"},{"location":"admin_guide/migrate_from_legacy_datasets/#step-2-define-the-new-dataset","title":"Step 2: Define the new dataset","text":"<p>Define the new dataset in the Argilla V2 format. The new dataset format is defined in the <code>argilla</code> package. You can create a new dataset with the <code>Settings</code> and <code>Dataset</code> classes:</p> <p>First, instantiate the <code>Argilla</code> class to connect to the Argilla V2 server:</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla()\n</code></pre> <p>Next, define the new dataset settings:</p> For single-label classificationFor multi-label classificationFor token classificationFor text generation <pre><code>settings = rg.Settings(\n    fields=[\n        rg.TextField(name=\"text\"), # (1)\n    ],\n    questions=[\n        rg.LabelQuestion(name=\"label\", labels=settings_v1.label_schema),\n    ],\n    metadata=[\n        rg.TermsMetadataProperty(name=\"split\"), # (2)\n    ],\n    vectors=[\n        rg.VectorField(name='mini-lm-sentence-transformers', dimensions=384), # (3)\n    ],\n)\n</code></pre> <ol> <li> <p>The default field in <code>DatasetForTextClassification</code> is <code>text</code>, but make sure you provide all fields included in <code>record.inputs</code>.</p> </li> <li> <p>Make sure you provide all relevant metadata fields available in the dataset.</p> </li> <li> <p>Make sure you provide all relevant vectors available in the dataset.</p> </li> </ol> <pre><code>settings = rg.Settings(\n    fields=[\n        rg.TextField(name=\"text\"), # (1)\n    ],\n    questions=[\n        rg.MultiLabelQuestion(name=\"labels\", labels=settings_v1.label_schema),\n    ],\n    metadata=[\n        rg.TermsMetadataProperty(name=\"split\"), # (2)\n    ],\n    vectors=[\n        rg.VectorField(name='mini-lm-sentence-transformers', dimensions=384), # (3)\n    ],\n)\n</code></pre> <ol> <li> <p>The default field in <code>DatasetForTextClassification</code> is <code>text</code>, but we should provide all fields included in <code>record.inputs</code>.</p> </li> <li> <p>Make sure you provide all relevant metadata fields available in the dataset.</p> </li> <li> <p>Make sure you provide all relevant vectors available in the dataset.</p> </li> </ol> <pre><code>settings = rg.Settings(\n    fields=[\n        rg.TextField(name=\"text\"),\n    ],\n    questions=[\n        rg.SpanQuestion(name=\"spans\", labels=settings_v1.label_schema),\n    ],\n    metadata=[\n        rg.TermsMetadataProperty(name=\"split\"), # (1)\n    ],\n    vectors=[\n        rg.VectorField(name='mini-lm-sentence-transformers', dimensions=384), # (2)\n    ],\n)\n</code></pre> <ol> <li> <p>Make sure you provide all relevant metadata fields available in the dataset.</p> </li> <li> <p>Make sure you provide all relevant vectors available in the dataset.</p> </li> </ol> <pre><code>settings = rg.Settings(\n    fields=[\n        rg.TextField(name=\"text\"),\n    ],\n    questions=[\n        rg.TextQuestion(name=\"text_generation\"),\n    ],\n    metadata=[\n        rg.TermsMetadataProperty(name=\"split\"), # (1)\n    ],\n    vectors=[\n        rg.VectorField(name='mini-lm-sentence-transformers', dimensions=384), # (2)\n    ],\n)\n</code></pre> <ol> <li> <p>We should provide all relevant metadata fields available in the dataset.</p> </li> <li> <p>We should provide all relevant vectors available in the dataset.</p> </li> </ol> <p>Finally, create the new dataset on the Argilla V2 server:</p> <pre><code>dataset = rg.Dataset(name=dataset_name, settings=settings)\ndataset.create()\n</code></pre> <p>Note</p> <p>If a dataset with the same name already exists, the <code>create</code> method will raise an exception. You can check if the dataset exists and delete it before creating a new one.</p> <pre><code>dataset = client.datasets(name=dataset_name)\n\nif dataset is not None:\n    dataset.delete()\n</code></pre>"},{"location":"admin_guide/migrate_from_legacy_datasets/#step-3-upload-the-dataset-records","title":"Step 3: Upload the dataset records","text":"<p>To upload the records to the new server, we will need to convert the records from the Argilla V1 format to the Argilla V2 format. The new <code>argilla</code> sdk package uses a generic <code>Record</code> class, but legacy datasets have specific record classes. We will need to convert the records to the generic <code>Record</code> class.</p> <p>Here are a set of example functions to convert the records for single-label and multi-label classification. You can modify these functions to suit your dataset.</p> For single-label classificationFor multi-label classificationFor token classificationFor text generation <pre><code>def map_to_record_for_single_label(data: dict, users_by_name: dict, current_user: rg.User) -&gt; rg.Record:\n    \"\"\" This function maps a text classification record dictionary to the new Argilla record.\"\"\"\n    suggestions = []\n    responses = []\n\n    if prediction := data.get(\"prediction\"):\n        label, score = prediction[0].values()\n        agent = data[\"prediction_agent\"]\n        suggestions.append(\n            rg.Suggestion(\n                question_name=\"label\", # (1)\n                value=label,\n                score=score,\n                agent=agent\n            )\n        )\n\n    if annotation := data.get(\"annotation\"):\n        user_id = users_by_name.get(data[\"annotation_agent\"], current_user).id\n        responses.append(\n            rg.Response(\n                question_name=\"label\", # (2)\n                value=annotation,\n                user_id=user_id\n            )\n        )\n\n    return rg.Record(\n        id=data[\"id\"],\n        fields=data[\"inputs\"],\n        # The inputs field should be a dictionary with the same keys as the `fields` in the settings\n        metadata=data[\"metadata\"],\n        # The metadata field should be a dictionary with the same keys as the `metadata` in the settings\n        vectors=data.get(\"vectors\") or {},\n        suggestions=suggestions,\n        responses=responses,\n    )\n</code></pre> <ol> <li> <p>Make sure the <code>question_name</code> matches the name of the question in question settings.</p> </li> <li> <p>Make sure the <code>question_name</code> matches the name of the question in question settings.</p> </li> </ol> <pre><code>def map_to_record_for_multi_label(data: dict, users_by_name: dict, current_user: rg.User) -&gt; rg.Record:\n    \"\"\" This function maps a text classification record dictionary to the new Argilla record.\"\"\"\n    suggestions = []\n    responses = []\n\n    if prediction := data.get(\"prediction\"):\n        labels, scores = zip(*[(pred[\"label\"], pred[\"score\"]) for pred in prediction])\n        agent = data[\"prediction_agent\"]\n        suggestions.append(\n            rg.Suggestion(\n                question_name=\"labels\", # (1)\n                value=labels,\n                score=scores,\n                agent=agent\n            )\n        )\n\n    if annotation := data.get(\"annotation\"):\n        user_id = users_by_name.get(data[\"annotation_agent\"], current_user).id\n        responses.append(\n            rg.Response(\n                question_name=\"labels\", # (2)\n                value=annotation,\n                user_id=user_id\n            )\n        )\n\n    return rg.Record(\n        id=data[\"id\"],\n        fields=data[\"inputs\"],\n        # The inputs field should be a dictionary with the same keys as the `fields` in the settings\n        metadata=data[\"metadata\"],\n        # The metadata field should be a dictionary with the same keys as the `metadata` in the settings\n        vectors=data.get(\"vectors\") or {},\n        suggestions=suggestions,\n        responses=responses,\n    )\n</code></pre> <ol> <li> <p>Make sure the <code>question_name</code> matches the name of the question in question settings.</p> </li> <li> <p>Make sure the <code>question_name</code> matches the name of the question in question settings.</p> </li> </ol> <pre><code>def map_to_record_for_span(data: dict, users_by_name: dict, current_user: rg.User) -&gt; rg.Record:\n    \"\"\" This function maps a token classification record dictionary to the new Argilla record.\"\"\"\n    suggestions = []\n    responses = []\n\n    if prediction := data.get(\"prediction\"):\n        scores = [span[\"score\"] for span in prediction]\n        agent = data[\"prediction_agent\"]\n        suggestions.append(\n            rg.Suggestion(\n                question_name=\"spans\", # (1)\n                value=prediction,\n                score=scores,\n                agent=agent\n            )\n        )\n\n    if annotation := data.get(\"annotation\"):\n        user_id = users_by_name.get(data[\"annotation_agent\"], current_user).id\n        responses.append(\n            rg.Response(\n                question_name=\"spans\", # (2)\n                value=annotation,\n                user_id=user_id\n            )\n        )\n\n    return rg.Record(\n        id=data[\"id\"],\n        fields={\"text\": data[\"text\"]},\n        # The inputs field should be a dictionary with the same keys as the `fields` in the settings\n        metadata=data[\"metadata\"],\n        # The metadata field should be a dictionary with the same keys as the `metadata` in the settings\n        vectors=data.get(\"vectors\") or {},\n        # The vectors field should be a dictionary with the same keys as the `vectors` in the settings\n        suggestions=suggestions,\n        responses=responses,\n    )\n</code></pre> <ol> <li> <p>Make sure the <code>question_name</code> matches the name of the question in question settings.</p> </li> <li> <p>Make sure the <code>question_name</code> matches the name of the question in question settings.</p> </li> </ol> <pre><code>def map_to_record_for_text_generation(data: dict, users_by_name: dict, current_user: rg.User) -&gt; rg.Record:\n    \"\"\" This function maps a text2text record dictionary to the new Argilla record.\"\"\"\n    suggestions = []\n    responses = []\n\n    if prediction := data.get(\"prediction\"):\n        first = prediction[0]\n        agent = data[\"prediction_agent\"]\n        suggestions.append(\n            rg.Suggestion(\n                question_name=\"text_generation\", # (1)\n                value=first[\"text\"],\n                score=first[\"score\"],\n                agent=agent\n            )\n        )\n\n    if annotation := data.get(\"annotation\"):\n        # From data[annotation]\n        user_id = users_by_name.get(data[\"annotation_agent\"], current_user).id\n        responses.append(\n            rg.Response(\n                question_name=\"text_generation\", # (2)\n                value=annotation,\n                user_id=user_id\n            )\n        )\n\n    return rg.Record(\n        id=data[\"id\"],\n        fields={\"text\": data[\"text\"]},\n        # The inputs field should be a dictionary with the same keys as the `fields` in the settings\n        metadata=data[\"metadata\"],\n        # The metadata field should be a dictionary with the same keys as the `metadata` in the settings\n        vectors=data.get(\"vectors\") or {},\n        # The vectors field should be a dictionary with the same keys as the `vectors` in the settings\n        suggestions=suggestions,\n        responses=responses,\n    )\n</code></pre> <ol> <li> <p>Make sure the <code>question_name</code> matches the name of the question in question settings.</p> </li> <li> <p>Make sure the <code>question_name</code> matches the name of the question in question settings.</p> </li> </ol> <p>The functions above depend on the <code>users_by_name</code> dictionary and the <code>current_user</code> object to assign responses to users, we need to load the existing users. You can retrieve the users from the Argilla V2 server and the current user as follows:</p> <pre><code>users_by_name = {user.username: user for user in client.users}\ncurrent_user = client.me\n</code></pre> <p>Finally, upload the records to the new dataset using the <code>log</code> method and map functions.</p> <pre><code>records = []\n\nfor data in hf_records:\n    records.append(map_to_record_for_single_label(data, users_by_name, current_user))\n\n# Upload the records to the new dataset\ndataset.records.log(records)\n</code></pre> <p>You have now successfully migrated your legacy dataset to Argilla V2. For more guides on how to use the Argilla SDK, please refer to the How to guides.</p>"},{"location":"admin_guide/query/","title":"Query and filter records","text":"<p>This guide provides an overview of how to query and filter a dataset in Argilla.</p> <p>You can search for records in your dataset by querying or filtering. The query focuses on the content of the text field, while the filter is used to filter the records based on conditions. You can use them independently or combine multiple filters to create complex search queries. You can also export records from a dataset either as a single dictionary or a list of dictionaries.</p> <p>Main Classes</p> <code>rg.query</code><code>rg.Filter</code> <pre><code>rg.Query(\n    query=\"query\",\n    filter=filter\n)\n</code></pre> <p>Check the Query - Python Reference to see the attributes, arguments, and methods of the <code>Query</code> class in detail.</p> <pre><code>rg.Filter(\n    [\n        (\"field\", \"==\", \"value\"),\n    ]\n)\n</code></pre> <p>Check the Filter - Python Reference to see the attributes, arguments, and methods of the <code>Filter</code> class in detail.</p>"},{"location":"admin_guide/query/#query-with-search-terms","title":"Query with search terms","text":"<p>To search for records with terms, you can use the <code>Dataset.records</code> attribute with a query string. The search terms are used to search for records that contain the terms in the text field. You can search a single term or various terms, in the latter, all of them should appear in the record to be retrieved.</p> Single search termMultiple search term <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\ndataset = client.datasets(name=\"my_dataset\", workspace=\"my_workspace\")\n\nquery = rg.Query(query=\"my_term\")\n\nqueried_records = dataset.records(query=query).to_list(flatten=True)\n</code></pre> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\ndataset = client.datasets(name=\"my_dataset\", workspace=\"my_workspace\")\n\nquery = rg.Query(query=\"my_term1 my_term2\")\n\nqueried_records = dataset.records(query=query).to_list(flatten=True)\n</code></pre>"},{"location":"admin_guide/query/#filter-by-conditions","title":"Filter by conditions","text":"<p>You can use the <code>Filter</code> class to define the conditions and pass them to the <code>Dataset.records</code> attribute to fetch records based on the conditions. Conditions include \"==\", \"&gt;=\", \"&lt;=\", or \"in\". Conditions can be combined with dot notation to filter records based on metadata, suggestions, or responses. You can use a single condition or multiple conditions to filter records.</p> operator description <code>==</code> The <code>field</code> value is equal to the <code>value</code> <code>&gt;=</code> The <code>field</code> value is greater than or equal to the <code>value</code> <code>&lt;=</code> The <code>field</code> value is less than or equal to the <code>value</code> <code>in</code> TThe <code>field</code> value is included in a list of values Single conditionMultiple conditions <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\ndataset = client.datasets(name=\"my_dataset\", workspace=\"my_workspace\")\n\nfilter_label = rg.Filter((\"label\", \"==\", \"positive\"))\n\nfiltered_records = dataset.records(query=rg.Query(filter=filter_label)).to_list(\n    flatten=True\n)\n</code></pre> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\ndataset = client.datasets(name=\"my_dataset\", workspace=\"my_workspace\")\n\nfilters = rg.Filter(\n    [\n        (\"label.suggestion\", \"==\", \"positive\"),\n        (\"metadata.count\", \"&gt;=\", 10),\n        (\"metadata.count\", \"&lt;=\", 20),\n        (\"label\", \"in\", [\"positive\", \"negative\"])\n    ]\n)\n\nfiltered_records = dataset.records(\n    query=rg.Query(filter=filters), with_suggestions=True\n).to_list(flatten=True)\n</code></pre>"},{"location":"admin_guide/query/#filter-by-status","title":"Filter by status","text":"<p>You can filter records based on record or response status. Record status can be <code>pending</code> or <code>completed</code>, and response status can be <code>pending</code>, <code>draft</code>, <code>submitted</code>, or <code>discarded</code>.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\ndataset = client.datasets(name=\"my_dataset\", workspace=\"my_workspace\")\n\nstatus_filter = rg.Query(\n    filter=rg.Filter(\n        [\n            (\"status\", \"==\", \"completed\"),\n            (\"response.status\", \"==\", \"discarded\")\n        ]\n    )\n)\n\nfiltered_records = dataset.records(status_filter).to_list(flatten=True)\n</code></pre>"},{"location":"admin_guide/query/#query-and-filter-a-dataset","title":"Query and filter a dataset","text":"<p>As mentioned, you can use a query with a search term and a filter or various filters to create complex search queries.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\ndataset = client.datasets(name=\"my_dataset\", workspace=\"my_workspace\")\n\nquery_filter = rg.Query(\n    query=\"my_term\",\n    filter=rg.Filter(\n        [\n            (\"label.suggestion\", \"==\", \"positive\"),\n            (\"metadata.count\", \"&gt;=\", 10),\n        ]\n    )\n)\n\nqueried_filtered_records = dataset.records(\n    query=query_filter,\n    with_metadata=True,\n    with_suggestions=True\n).to_list(flatten=True)\n</code></pre>"},{"location":"admin_guide/record/","title":"Add, update, and delete records","text":"<p>This guide provides an overview of records, explaining the basics of how to define and manage them in Argilla.</p> <p>A record in Argilla is a data item that requires annotation, consisting of one or more fields. These are the pieces of information displayed to the user in the UI to facilitate the completion of the annotation task. Each record also includes questions that annotators are required to answer, with the option of adding suggestions and responses to assist them. Guidelines are also provided to help annotators effectively complete their tasks.</p> <p>A record is part of a dataset, so you will need to create a dataset before adding records. Check this guide to learn how to create a dataset.</p> <p>Main Class</p> <pre><code>rg.Record(\n    external_id=\"1234\",\n    fields={\n        \"question\": \"Do you need oxygen to breathe?\",\n        \"answer\": \"Yes\"\n    },\n    metadata={\n        \"category\": \"A\"\n    },\n    vectors={\n        \"my_vector\": [0.1, 0.2, 0.3],\n    },\n    suggestions=[\n        rg.Suggestion(\"my_label\", \"positive\", score=0.9, agent=\"model_name\")\n    ],\n    responses=[\n        rg.Response(\"label\", \"positive\", user_id=user_id)\n    ],\n)\n</code></pre> <p>Check the Record - Python Reference to see the attributes, arguments, and methods of the <code>Record</code> class in detail.</p>"},{"location":"admin_guide/record/#add-records","title":"Add records","text":"<p>You can add records to a dataset in two different ways: either by using a dictionary or by directly initializing a <code>Record</code> object. You should ensure that fields, metadata and vectors match those configured in the dataset settings. In both cases, are added via the <code>Dataset.records.log</code> method. As soon as you add the records, these will be available in the Argilla UI. If they do not appear in the UI, you may need to click the refresh button to update the view.</p> <p>Tip</p> <p>Take some time to inspect the data before adding it to the dataset in case this triggers changes in the <code>questions</code> or <code>fields</code>.</p> <p>Note</p> <p>If you are planning to use public data, the Datasets page of the Hugging Face Hub is a good place to start. Remember to always check the license to make sure you can legally use it for your specific use case.</p> As <code>Record</code> objectsFrom a generic data structureFrom a Hugging Face dataset <p>You can add records to a dataset by initializing a <code>Record</code> object directly. This is ideal if you need to apply logic to the data before defining the record. If the data is already structured, you should consider adding it directly as a dictionary or Hugging Face dataset.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\ndataset = client.datasets(name=\"my_dataset\")\n\nrecords = [\n    rg.Record(\n        fields={\n            \"question\": \"Do you need oxygen to breathe?\",\n            \"answer\": \"Yes\"\n        },\n    ),\n    rg.Record(\n        fields={\n            \"question\": \"What is the boiling point of water?\",\n            \"answer\": \"100 degrees Celsius\"\n        },\n    ), # (1)\n]\n\ndataset.records.log(records)\n</code></pre> <ol> <li>This is an illustration of a definition. In a real-world scenario, you would iterate over a data structure and create <code>Record</code> objects for each iteration.</li> </ol> <p>You can add the data directly as a dictionary like structure, where the keys correspond to the names of fields, questions, metadata or vectors in the dataset and the values are the data to be added.</p> <p>If your data structure does not correspond to your Argilla dataset names, you can use a <code>mapping</code> to indicate which keys in the source data correspond to the dataset fields.</p> <p>We illustrate this python dictionaries that represent your data, but we would not advise you to define dictionaries. Instead, use the <code>Record</code> object to instantiate records.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\ndataset = client.datasets(name=\"my_dataset\")\n\n# Add records to the dataset with the fields 'question' and 'answer'\ndata = [\n    {\n        \"question\": \"Do you need oxygen to breathe?\",\n        \"answer\": \"Yes\",\n    },\n    {\n        \"question\": \"What is the boiling point of water?\",\n        \"answer\": \"100 degrees Celsius\",\n    }, # (1)\n]\ndataset.records.log(data)\n\n# Add records to the dataset with a mapping of the fields 'question' and 'answer'\ndata = [\n    {\n        \"query\": \"Do you need oxygen to breathe?\",\n        \"response\": \"Yes\",\n    },\n    {\n        \"query\": \"What is the boiling point of water?\",\n        \"response\": \"100 degrees Celsius\",\n    },\n]\ndataset.records.log(data, mapping={\"query\": \"question\", \"response\": \"answer\"}) # (2)\n</code></pre> <ol> <li>The data structure's keys must match the fields or questions in the Argilla dataset. In this case, there are fields named <code>question</code> and <code>answer</code>.</li> <li>The data structure has keys <code>query</code> and <code>response</code>, and the Argilla dataset has fields <code>question</code> and <code>answer</code>. You can use the <code>mapping</code> parameter to map the keys in the data structure to the fields in the Argilla dataset.</li> </ol> <p>You can also add records to a dataset using a Hugging Face dataset. This is useful when you want to use a dataset from the Hugging Face Hub and add it to your Argilla dataset.</p> <p>You can add the dataset where the column names correspond to the names of fields, metadata or vectors in the Argilla dataset.</p> <pre><code>import argilla as rg\nfrom datasets import load_dataset\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\ndataset = client.datasets(name=\"my_dataset\") # (1)\n\nhf_dataset = load_dataset(\"imdb\", split=\"train[:100]\") # (2)\n\ndataset.records.log(records=hf_dataset)\n</code></pre> <ol> <li> <p>In this case, we are using the <code>my_dataset</code> dataset from the Argilla workspace. The dataset has a <code>text</code> field and a <code>label</code> question.</p> </li> <li> <p>In this example, the Hugging Face dataset matches the Argilla dataset schema. If that is not the case, you could use the <code>.map</code> of the <code>datasets</code> library to prepare the data before adding it to the Argilla dataset.</p> </li> </ol> <p>If the Hugging Face dataset's schema does not correspond to your Argilla dataset field names, you can use a <code>mapping</code> to specify the relationship. You should indicate as key the column name of the Hugging Face dataset and, as value, the field name of the Argilla dataset.</p> <pre><code>dataset.records.log(\n    records=hf_dataset, mapping={\"text\": \"review\", \"label\": \"sentiment\"}\n) # (1)\n</code></pre> <ol> <li>In this case, the <code>text</code> key in the Hugging Face dataset would correspond to the <code>review</code> field in the Argilla dataset, and the <code>label</code> key in the Hugging Face dataset would correspond to the <code>sentiment</code> field in the Argilla dataset.</li> </ol>"},{"location":"admin_guide/record/#metadata","title":"Metadata","text":"<p>Record metadata can include any information about the record that is not part of the fields in the form of a dictionary. To use metadata for filtering and sorting records, make sure that the key of the dictionary corresponds with the metadata property <code>name</code>. When the key doesn't correspond, this will be considered extra metadata that will get stored with the record (as long as <code>allow_extra_metadata</code> is set to <code>True</code> for the dataset), but will not be usable for filtering and sorting.</p> <p>Note</p> <p>Remember that to use metadata within a dataset, you must define a metadata property in the dataset settings.</p> <p>Check the Metadata - Python Reference to see the attributes, arguments, and methods for using metadata in detail.</p> As <code>Record</code> objectsFrom a generic data structure <p>You can add metadata to a record in an initialized <code>Record</code> object.</p> <pre><code># Add records to the dataset with the metadata 'category'\nrecords = [\n    rg.Record(\n        fields={\n            \"question\": \"Do you need oxygen to breathe?\",\n            \"answer\": \"Yes\"\n        },\n        metadata={\"my_metadata\": \"option_1\"},\n    ),\n    rg.Record(\n        fields={\n            \"question\": \"What is the boiling point of water?\",\n            \"answer\": \"100 degrees Celsius\"\n        },\n        metadata={\"my_metadata\": \"option_1\"},\n    ),\n]\ndataset.records.log(records)\n</code></pre> <p>You can add metadata to a record directly as a dictionary structure, where the keys correspond to the names of metadata properties in the dataset and the values are the metadata to be added. Remember that you can also use the <code>mapping</code> parameter to specify the data structure.</p> <pre><code># Add records to the dataset with the metadata 'category'\ndata = [\n    {\n        \"question\": \"Do you need oxygen to breathe?\",\n        \"answer\": \"Yes\",\n        \"my_metadata\": \"option_1\",\n    },\n    {\n        \"question\": \"What is the boiling point of water?\",\n        \"answer\": \"100 degrees Celsius\",\n        \"my_metadata\": \"option_1\",\n    },\n]\ndataset.records.log(data)\n</code></pre>"},{"location":"admin_guide/record/#vectors","title":"Vectors","text":"<p>You can associate vectors, like text embeddings, to your records. They can be used for semantic search in the UI and the Python SDK. Make sure that the length of the list corresponds to the dimensions set in the vector settings.</p> <p>Note</p> <p>Remember that to use vectors within a dataset, you must define them in the dataset settings.</p> <p>Check the Vector - Python Reference to see the attributes, arguments, and methods of the <code>Vector</code> class in detail.</p> As <code>Record</code> objectsFrom a generic data structure <p>You can also add vectors to a record in an initialized <code>Record</code> object.</p> <pre><code># Add records to the dataset with the vector 'my_vector' and dimension=3\nrecords = [\n    rg.Record(\n        fields={\n            \"question\": \"Do you need oxygen to breathe?\",\n            \"answer\": \"Yes\"\n        },\n        vectors={\n            \"my_vector\": [0.1, 0.2, 0.3]\n        },\n    ),\n    rg.Record(\n        fields={\n            \"question\": \"What is the boiling point of water?\",\n            \"answer\": \"100 degrees Celsius\"\n        },\n        vectors={\n            \"my_vector\": [0.2, 0.5, 0.3]\n        },\n    ),\n]\ndataset.records.log(records)\n</code></pre> <p>You can add vectors from a dictionary-like structure, where the keys correspond to the <code>name</code>s of the vector settings that were configured for your dataset and the value is a list of floats. Remember that you can also use the <code>mapping</code> parameter to specify the data structure.</p> <pre><code># Add records to the dataset with the vector 'my_vector' and dimension=3\ndata = [\n    {\n        \"question\": \"Do you need oxygen to breathe?\",\n        \"answer\": \"Yes\",\n        \"my_vector\": [0.1, 0.2, 0.3],\n    },\n    {\n        \"question\": \"What is the boiling point of water?\",\n        \"answer\": \"100 degrees Celsius\",\n        \"my_vector\": [0.2, 0.5, 0.3],\n    },\n]\ndataset.records.log(data)\n</code></pre>"},{"location":"admin_guide/record/#suggestions","title":"Suggestions","text":"<p>Suggestions refer to suggested responses (e.g. model predictions) that you can add to your records to make the annotation process faster. These can be added during the creation of the record or at a later stage. Only one suggestion can be provided for each question, and suggestion values must be compliant with the pre-defined questions e.g. if we have a <code>RatingQuestion</code> between 1 and 5, the suggestion should have a valid value within that range.</p> <p>Check the Suggestions - Python Reference to see the attributes, arguments, and methods of the <code>Suggestion</code> class in detail.</p> <p>Tip</p> <p>Check the Suggestions - Python Reference for different formats per <code>Question</code> type.</p> As <code>Record</code> objectsFrom a generic data structure <p>You can also add suggestions to a record in an initialized <code>Record</code> object.</p> <pre><code># Add records to the dataset with the label 'my_label'\nrecords = [\n    rg.Record(\n        fields={\n            \"question\": \"Do you need oxygen to breathe?\",\n            \"answer\": \"Yes\"\n        },\n        suggestions=[\n            rg.Suggestion(\n                \"my_label\",\n                \"positive\",\n                score=0.9,\n                agent=\"model_name\"\n            )\n        ],\n    ),\n    rg.Record(\n        fields={\n            \"question\": \"What is the boiling point of water?\",\n            \"answer\": \"100 degrees Celsius\"\n        },\n        suggestions=[\n            rg.Suggestion(\n                \"my_label\",\n                \"negative\",\n                score=0.9,\n                agent=\"model_name\"\n            )\n        ],\n    ),\n]\ndataset.records.log(records)\n</code></pre> <p>You can add suggestions as a dictionary, where the keys correspond to the <code>name</code>s of the labels that were configured for your dataset. Remember that you can also use the <code>mapping</code> parameter to specify the data structure.</p> <pre><code># Add records to the dataset with the label question 'my_label'\ndata =  [\n    {\n        \"question\": \"Do you need oxygen to breathe?\",\n        \"answer\": \"Yes\",\n        \"label\": \"positive\",\n        \"score\": 0.9,\n        \"agent\": \"model_name\",\n    },\n    {\n        \"question\": \"What is the boiling point of water?\",\n        \"answer\": \"100 degrees Celsius\",\n        \"label\": \"negative\",\n        \"score\": 0.9,\n        \"agent\": \"model_name\",\n    },\n]\ndataset.records.log(\n    data=data,\n    mapping={\n        \"label\": \"my_label\",\n        \"score\": \"my_label.suggestion.score\",\n        \"agent\": \"my_label.suggestion.agent\",\n    },\n)\n</code></pre>"},{"location":"admin_guide/record/#responses","title":"Responses","text":"<p>If your dataset includes some annotations, you can add those to the records as you create them. Make sure that the responses adhere to the same format as Argilla's output and meet the schema requirements for the specific type of question being answered. Make sure to include the <code>user_id</code> in case you're planning to add more than one response for the same question, if not responses will apply to all the annotators.</p> <p>Check the Responses - Python Reference to see the attributes, arguments, and methods of the <code>Response</code> class in detail.</p> <p>Note</p> <p>Keep in mind that records with responses will be displayed as \"Draft\" in the UI.</p> <p>Tip</p> <p>Check the Responses - Python Reference for different formats per <code>Question</code> type.</p> As <code>Record</code> objectsFrom a generic data structure <p>You can also add suggestions to a record in an initialized <code>Record</code> object.</p> <pre><code># Add records to the dataset with the label 'my_label'\nrecords = [\n    rg.Record(\n        fields={\n            \"question\": \"Do you need oxygen to breathe?\",\n            \"answer\": \"Yes\"\n        },\n        responses=[\n            rg.Response(\"my_label\", \"positive\", user_id=user.id)\n        ]\n    ),\n    rg.Record(\n        fields={\n            \"question\": \"What is the boiling point of water?\",\n            \"answer\": \"100 degrees Celsius\"\n        },\n        responses=[\n            rg.Response(\"my_label\", \"negative\", user_id=user.id)\n        ]\n    ),\n]\ndataset.records.log(records)\n</code></pre> <p>You can add suggestions as a dictionary, where the keys correspond to the <code>name</code>s of the labels that were configured for your dataset. Remember that you can also use the <code>mapping</code> parameter to specify the data structure. If you want to specify the user that added the response, you can use the <code>user_id</code> parameter.</p> <pre><code># Add records to the dataset with the label 'my_label'\ndata = [\n    {\n        \"question\": \"Do you need oxygen to breathe?\",\n        \"answer\": \"Yes\",\n        \"label\": \"positive\",\n    },\n    {\n        \"question\": \"What is the boiling point of water?\",\n        \"answer\": \"100 degrees Celsius\",\n        \"label\": \"negative\",\n    },\n]\ndataset.records.log(data, user_id=user.id, mapping={\"label\": \"my_label.response\"})\n</code></pre>"},{"location":"admin_guide/record/#list-records","title":"List records","text":"<p>To list records in a dataset, you can use the <code>records</code> method on the <code>Dataset</code> object. This method returns a list of <code>Record</code> objects that can be iterated over to access the record properties.</p> <pre><code>for record in dataset.records(\n    with_suggestions=True,\n    with_responses=True,\n    with_vectors=True\n):\n\n    # Access the record properties\n    print(record.metadata)\n    print(record.vectors)\n    print(record.suggestions)\n    print(record.responses)\n\n    # Access the responses of the record\n    for response in record.responses:\n        print(response.value)\n</code></pre>"},{"location":"admin_guide/record/#update-records","title":"Update records","text":"<p>You can update records in a dataset by calling the <code>log</code> method on the <code>Dataset</code> object. To update a record, you need to provide the record <code>id</code> and the new data to be updated.</p> <pre><code>data = dataset.records.to_list(flatten=True)\n\nupdated_data = [\n    {\n        \"text\": sample[\"text\"],\n        \"label\": \"positive\",\n        \"id\": sample[\"id\"],\n    }\n    for sample in data\n]\ndataset.records.log(records=updated_data)\n</code></pre> Update the metadataUpdate vectorsUpdate suggestionsUpdate responses <p>The <code>metadata</code> of the <code>Record</code> object is a python dictionary. To update it, you can iterate over the records and update the metadata by key. After that, you should update the records in the dataset.</p> <p>Tip</p> <p>Check the Metadata - Python Reference for different formats per <code>MetadataProperty</code> type.</p> <pre><code>updated_records = []\n\nfor record in dataset.records():\n\n    record.metadata[\"my_metadata\"] = \"new_value\"\n    record.metadata[\"my_new_metadata\"] = \"new_value\"\n\n    updated_records.append(record)\n\ndataset.records.log(records=updated_records)\n</code></pre> <p>If a new vector field is added to the dataset settings or some value for the existing record vectors must be updated, you can iterate over the records and update the vectors by key. After that, you should update the records in the dataset.</p> <pre><code>updated_records = []\n\nfor record in dataset.records(with_vectors=True):\n\n    record.vectors[\"my_vector\"] = [ 0, 1, 2, 3, 4, 5 ]\n    record.vectors[\"my_new_vector\"] = [ 0, 1, 2, 3, 4, 5 ]\n\n    updated_records.append(record)\n\ndataset.records.log(records=updated_records)\n</code></pre> <p>If some value for the existing record suggestions must be updated, you can iterate over the records and update the suggestions by key. You can also add a suggestion using the <code>add</code> method. After that, you should update the records in the dataset.</p> <p>Tip</p> <p>Check the Suggestions - Python Reference for different formats per <code>Question</code> type.</p> <pre><code>updated_records = []\n\nfor record in dataset.records(with_suggestions=True):\n\n    # We can update existing suggestions\n    record.suggestions[\"label\"].value = \"new_value\"\n    record.suggestions[\"label\"].score = 0.9\n    record.suggestions[\"label\"].agent = \"model_name\"\n\n    # We can also add new suggestions with the `add` method:\n    if not record.suggestions[\"label\"]:\n        record.suggestions.add(\n            rg.Suggestion(\"value\", \"label\", score=0.9, agent=\"model_name\")\n        )\n\n    updated_records.append(record)\n\ndataset.records.log(records=updated_records)\n</code></pre> <p>If some value for the existing record responses must be updated, you can iterate over the records and update the responses by key. You can also add a response using the <code>add</code> method. After that, you should update the records in the dataset.</p> <p>Tip</p> <p>Check the Responses - Python Reference for different formats per <code>Question</code> type.</p> <pre><code>updated_records = []\n\nfor record in dataset.records(with_responses=True):\n\n    for response in record.responses[\"label\"]:\n\n        if response:\n                response.value = \"new_value\"\n                response.user_id = \"existing_user_id\"\n\n        else:\n            record.responses.add(rg.Response(\"label\", \"YES\", user_id=user.id))\n\n    updated_records.append(record)\n\ndataset.records.log(records=updated_records)\n</code></pre>"},{"location":"admin_guide/record/#delete-records","title":"Delete records","text":"<p>You can delete records in a dataset calling the <code>delete</code> method on the <code>Dataset</code> object. To delete records, you need to retrieve them from the server and get a list with those that you want to delete.</p> <pre><code>records_to_delete = list(dataset.records)[:5]\ndataset.records.delete(records=records_to_delete)\n</code></pre> <p>Delete records based on a query</p> <p>It can be very useful to avoid eliminating records with responses.</p> <p>For more information about the query syntax, check this how-to guide.</p> <pre><code>status_filter = rg.Query(\n    filter = rg.Filter((\"response.status\", \"==\", \"pending\"))\n)\nrecords_to_delete = list(dataset.records(status_filter))\n\ndataset.records.delete(records_to_delete)\n</code></pre>"},{"location":"admin_guide/upgrading/","title":"Upgrading","text":""},{"location":"admin_guide/upgrading/#updating-extralit","title":"Updating Extralit","text":"<p>This guide covers the update process for Extralit across different deployment options: quickstart, Docker, and Kubernetes.</p>"},{"location":"admin_guide/upgrading/#general-update-notes","title":"General Update Notes","text":"<ul> <li>Always backup your data before performing updates.</li> <li>Test updates in a development environment before applying to production.</li> <li>Check the Extralit release notes for any specific update instructions or breaking changes.</li> <li>After updating, verify that all services are functioning correctly and that data is accessible.</li> </ul>"},{"location":"admin_guide/upgrading/#kubernetes-deployment-update","title":"Kubernetes Deployment Update","text":"<ol> <li> <p>Update the service code in the <code>extralit-server</code> repository from the <code>main</code> branch:</p> <pre><code>git pull origin main\n</code></pre> </li> <li> <p>Rebuild the Python package and Docker image:     <pre><code>sh scripts/build_distribution.sh\n</code></pre></p> </li> <li> <p>Update the Docker image tag in the corresponding Kubernetes YAML file or Helm chart values.</p> </li> <li> <p>If using Tilt for development:</p> <ul> <li>Tilt will automatically detect changes and rebuild/redeploy the service.</li> <li>Manually trigger a rebuild using the \u21bb button in the Tilt web interface if needed.</li> </ul> </li> <li> <p>For production updates without Tilt:</p> <ul> <li>Push the updated Docker image to your repository: <pre><code>docker push {DOCKER_REPO}/service-name:tag\n</code></pre></li> <li>Apply the updated Kubernetes configuration: <pre><code>kubectl apply -f path/to/updated/service-config.yaml -n {NAMESPACE}\n</code></pre></li> </ul> </li> <li> <p>Monitor the rollout:     <pre><code>kubectl rollout status deployment/service-name -n {NAMESPACE}\n</code></pre></p> </li> <li> <p>For database schema changes:</p> <ul> <li>Run migrations using the <code>argilla_server</code> CLI: <pre><code>kubectl exec -it deployment/argilla-server -n {NAMESPACE} -- \\\nargilla_server database migrate\n</code></pre></li> </ul> </li> <li> <p>For frontend updates:</p> <ul> <li>Rebuild the frontend: <pre><code>sh scripts/build_frontend.sh\n</code></pre></li> <li>Trigger a rebuild of the <code>argilla-server</code> in the Tilt web interface or reapply the Kubernetes configuration.</li> </ul> </li> </ol>"},{"location":"admin_guide/upgrading/#quickstart-deployment-update","title":"Quickstart Deployment Update","text":"<ol> <li> <p>Pull the latest Extralit image:     <pre><code>docker pull extralit/argilla-quickstart:latest\n</code></pre></p> </li> <li> <p>Stop and remove the existing container:     <pre><code>docker stop extralit-quickstart\ndocker rm extralit-quickstart\n</code></pre></p> </li> <li> <p>Start a new container with the updated image:     <pre><code>docker run -d --name extralit-quickstart -p 6900:6900 \\\n  -e ARGILLA_AUTH_SECRET_KEY=$(openssl rand -hex 32) \\\n  extralit/argilla-quickstart:latest\n</code></pre></p> </li> </ol>"},{"location":"admin_guide/upgrading/#docker-deployment-update","title":"Docker Deployment Update","text":"<ol> <li> <p>Update the <code>docker-compose.yml</code> file with the latest Extralit image version.</p> </li> <li> <p>Pull the updated images:     <pre><code>docker-compose pull\n</code></pre></p> </li> <li> <p>Restart the services with the new images:     <pre><code>docker-compose up -d\n</code></pre></p> </li> <li> <p>For database schema changes, run migrations:     <pre><code>docker-compose exec argilla argilla_server database migrate\n</code></pre></p> </li> </ol>"},{"location":"admin_guide/use_markdown_to_format_rich_content/","title":"Use Markdown to format rich content","text":"<p>This guide provides an overview of how to use Markdown and HTML in <code>TextFields</code> to format chat conversations and allow for basic multi-modal support for images, audio, video and PDFs.</p> <p>The <code>TextField</code> and <code>TextQuestion</code> provide the option to enable Markdown and therefore HTML by setting <code>use_markdown=True</code>. Given the flexibility of HTML, we can get great control over the presentation of data to our annotators. We provide some out-of-the-box methods for multi-modality and chat templates in the examples below.</p> <p>Main Methods</p> image_to_htmlaudio_to_htmlvideo_to_htmlpdf_to_htmlchat_to_html <pre><code>image_to_html(\"local_image_file.png\")\n</code></pre> <pre><code>audio_to_html(\"local_audio_file.mp3\")\n</code></pre> <pre><code>audio_to_html(\"local_video_file.mp4\")\n</code></pre> <pre><code>pdf_to_html(\"local_pdf_file.pdf\")\n</code></pre> <pre><code>chat_to_html([{\"role\": \"user\", \"content\": \"hello\"}])\n</code></pre> <p>Check the Markdown - Python Reference to see the arguments of the <code>rg.markdown</code> methods in detail.</p> <p>Tip</p> <p>You can get pretty creative with HTML. For example, think about visualizing graphs and tables. You can use some interesting Python packages methods like <code>pandas.DataFrame.to_html</code> and <code>plotly.io.to_html</code>.</p>"},{"location":"admin_guide/use_markdown_to_format_rich_content/#multi-modal-support-images-audio-video-pdfs-and-more","title":"Multi-modal support: images, audio, video, PDFs and more","text":"<p>Argilla has basic multi-modal support in different ways, each with pros and cons, but they both offer the same UI experience because they both rely on HTML.</p> <p></p>"},{"location":"admin_guide/use_markdown_to_format_rich_content/#local-content-through-dataurls","title":"Local content through DataURLs","text":"<p>A DataURL is a scheme that allows data to be encoded into a base64-encoded string and then embedded directly into HTML. To facilitate this, we offer some functions: <code>image_to_html</code>, <code>audio_to_html</code>, <code>video_to_thml</code>, and <code>pdf_to_html</code>. These functions accept either the file path or the file's byte data and return the corresponding HTMurl to render the media file within the Argilla user interface. Additionally, you can also set the <code>width</code> and <code>height</code> in pixel or percentage for video and image (defaults to the original dimensions) and the autoplay and loop attributes to True for audio and video (defaults to False).</p> <p>Warning</p> <p>DataURLs increase the memory usage of the original filesize. Additionally, different browsers enforce different size limitations for rendering DataURLs which might block the visualization experience per user.</p> ImageAudioVideoPDF <pre><code>from argilla.markdown import image_to_html\n\nhtml = image_to_html(\n    \"local_image_file.png\",\n    width=\"300px\",\n    height=\"300px\"\n)\n\nrg.Record(\n    fields={\"markdown_enabled_field\": html}\n)\n</code></pre> <pre><code>from argilla.markdown import audio_to_html\n\nhtml = audio_to_html(\n    \"local_audio_file.mp3\",\n    width=\"300px\",\n    height=\"300px\",\n    autoplay=True,\n    loop=True\n)\n\nrg.Record(\n    fields={\"markdown_enabled_field\": html}\n)\n</code></pre> <pre><code>from argilla.markdown import video_to_thml\n\nhtml = video_to_html(\n    \"local_video_file.mp4\",\n    width=\"300px\",\n    height=\"300px\",\n    autoplay=True,\n    loop=True\n)\n\nrg.Record(\n    fields={\"markdown_enabled_field\": html}\n)\n</code></pre> <pre><code>from argilla.markdown import pdf_to_html\n\nhtml = pdf_to_html(\n    \"local_pdf_file.pdf\",\n    width=\"300px\",\n    height=\"300px\"\n)\n\nrg.Record(\n    fields={\"markdown_enabled_field\": html}\n)\n</code></pre>"},{"location":"admin_guide/use_markdown_to_format_rich_content/#hosted-content","title":"Hosted content","text":"<p>Instead of uploading local files through DataURLs, we can also visualize URLs directly linking to media files such as images, audio, video, and PDFs hosted on a public or private server. In this case, you can use basic HTML to visualize content available on platforms like Google Drive or decide to configure a private media server.</p> <p>Warning</p> <p>When trying to access content from a private media server you have to ensure that the Argilla server has network access to the private media server, which might be done through something like IP whitelisting.</p> ImageAudioVideoPDF <pre><code>html = \"&lt;img src='https://example.com/public-image-file.jpg'&gt;\"\n\nrg.Record(\n    fields={\"markdown_enabled_field\": html}\n)\n</code></pre> <pre><code>html = \"\"\"\n&lt;audio controls&gt;\n    &lt;source src=\"https://example.com/public-audio-file.mp3\" type=\"audio/mpeg\"&gt;\n&lt;/audio&gt;\n\"\"\"\"\n\nrg.Record(\n    fields={\"markdown_enabled_field\": html}\n)\n</code></pre> <pre><code>html = \"\"\"\n&lt;video width=\"320\" height=\"240\" controls&gt;\n    &lt;source src=\"https://example.com/public-video-file.mp4\" type=\"video/mp4\"&gt;\n&lt;/video&gt;\n\"\"\"\"\n\nrg.Record(\n    fields={\"markdown_enabled_field\": html}\n)\n</code></pre> <pre><code>html = \"\"\"\n&lt;iframe\n    src=\"https://example.com/public-pdf-file.pdf\"\n    width=\"600\"\n    height=\"500\"&gt;\n&lt;/iframe&gt;\n\"\"\"\"\n\nrg.Record(\n    fields={\"markdown_enabled_field\": html}\n)\n</code></pre>"},{"location":"admin_guide/use_markdown_to_format_rich_content/#chat-and-conversation-support","title":"Chat and conversation support","text":"<p>When working with chat data from multi-turn interaction with a Large Language Model, it might be nice to be able to visualize the conversation in a similar way as a common chat interface. To facilitate this, we offer the <code>chat_to_html</code> function, which converts messages from OpenAI chat format to an HTML-formatted chat interface.</p> OpenAI chat format <p>The OpenAI chat format is a way to structure a list of messages as input from users and returns a model-generated message as output. These messages can only contain the <code>roles</code> \"user\" for human messages and \"assistant\", \"system\" or \"model\" for model-generated messages.</p> <pre><code>from argilla.markdown import chat_to_html\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n    {\"role\": \"assistant\", \"content\": \"I'm good, thank you!\"}\n]\n\nhtml = chat_to_html(messages)\n\nrg.Record(\n    fields={\"markdown_enabled_field\": html}\n)\n</code></pre> <p></p>"},{"location":"admin_guide/user/","title":"User management","text":"<p>This guide provides an overview of user roles and credentials, explaining how to set up and manage users in Argilla.</p> <p>A user in Argilla is an authorized person who, depending on their role, can use the Python SDK and access the UI in a running Argilla instance. We differentiate between three types of users depending on their role, permissions and needs: <code>owner</code>, <code>admin</code> and <code>annotator</code>.</p> OverviewOwnerAdminAnnotator Owner Admin Annotator Number Unlimited Unlimited Unlimited Create and delete workspaces Yes No No Assign users to workspaces Yes No No Create, configure, update, and delete datasets Yes Only within assigned workspaces No Create, update, and delete users Yes No No Provide feedback with Argila UI Yes Yes Yes <p>The <code>owner</code> refers to the root user who created the Argilla instance. Using workspaces within Argilla proves highly beneficial for organizing tasks efficiently. So, the owner has full access to all workspaces and their functionalities:</p> <ul> <li>Workspace management: It can create, read and delete a workspace.</li> <li>User management: It can create a new user, assign it to a workspace, and delete it. It can also list them and search for a specific one.</li> <li>Dataset management: It can create, configure, retrieve, update, and delete datasets.</li> <li>Annotation: It can annotate datasets in the Argilla UI.</li> <li>Feedback: It can provide feedback with the Argilla UI.</li> </ul> <p>An <code>admin</code> user can only access the workspaces it has been assigned to and cannot assign other users to it. An admin user has the following permissions:</p> <ul> <li>Dataset management: It can create, configure, retrieve, update, and delete datasets only on the assigned workspaces.</li> <li>Annotation: It can annotate datasets in the assigned workspaces via the Argilla UI.</li> <li>Feedback: It can provide feedback with the Argilla UI.</li> </ul> <p>An <code>annotator</code> user is limited to accessing only the datasets assigned to it within the workspace. It has two specific permissions:</p> <ul> <li>Annotation: It can annotate the assigned datasets in the Argilla UI.</li> <li>Feedback: It can provide feedback with the Argilla UI.</li> </ul> Question: Who can manage users? <p>Only users with the <code>owner</code> role can manage (create, retrieve, delete) other users.</p>"},{"location":"admin_guide/user/#initial-users-and-credentials","title":"Initial users and credentials","text":"<p>Depending on your Argilla deployment, the initial user with the <code>owner</code> role will vary.</p> <ul> <li>If you deploy on the Hugging Face Hub, the initial user will correspond to the Space owner (your personal account). The API key is automatically generated and can be copied from the \"Settings\" section of the UI.</li> <li>If you deploy with Docker, the default values for the environment variables are: USERNAME: argilla, PASSWORD: 12345678, API_KEY: argilla.apikey.</li> </ul> <p>For the new users, the username and password are set during the creation process. The API key can be copied from the \"Settings\" section of the UI.</p> <p>Main Class</p> <pre><code>rg.User(\n    username=\"username\",\n    first_name=\"first_name\",\n    last_name=\"last_name\",\n    role=\"owner\",\n    password=\"password\",\n    client=client\n)\n</code></pre> <p>Check the User - Python Reference to see the attributes, arguments, and methods of the <code>User</code> class in detail.</p>"},{"location":"admin_guide/user/#get-current-user","title":"Get current user","text":"<p>To ensure you're using the correct credentials for managing users, you can get the current user in Argilla using the <code>me</code> attribute of the <code>Argilla</code> class.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\ncurrent_user = client.me\n</code></pre>"},{"location":"admin_guide/user/#create-a-user","title":"Create a user","text":"<p>To create a new user in Argilla, you can define it in the <code>User</code> class and then call the <code>create</code> method. This method is inherited from the <code>Resource</code> base class and operates without modifications.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nuser_to_create = rg.User(\n    username=\"my_username\",\n    password=\"12345678\",\n)\n\ncreated_user = user_to_create.create()\n</code></pre> <p>Accessing attributes</p> <p>Access the attributes of a user by calling them directly on the <code>User</code> object. For example, <code>user.id</code> or <code>user.username</code>.</p>"},{"location":"admin_guide/user/#list-users","title":"List users","text":"<p>You can list all the existing users in Argilla by accessing the <code>users</code> attribute on the <code>Argilla</code> class and iterating over them. You can also use <code>len(client.users)</code> to get the number of users.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nusers = client.users\n\nfor user in users:\n    print(user)\n</code></pre> <p>Notebooks</p> <p>When using a notebook, executing <code>client.users</code> will display a table with <code>username</code>, <code>id</code>, <code>role</code>, and the last update as <code>updated_at</code>.</p>"},{"location":"admin_guide/user/#retrieve-a-user","title":"Retrieve a user","text":"<p>You can retrieve an existing user from Argilla by accessing the <code>users</code> attribute on the <code>Argilla</code> class and passing the <code>username</code> or <code>id</code> as an argument. If the user does not exist, a warning message will be raised and <code>None</code> will be returned.</p> By usernameBy id <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nretrieved_user = client.users(\"my_username\")\n</code></pre> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nretrieved_user = client.users(id=\"&lt;uuid-or-uuid-string&gt;\")\n</code></pre>"},{"location":"admin_guide/user/#check-user-existence","title":"Check user existence","text":"<p>You can check if a user exists. The <code>client.users</code> method will return <code>None</code> if the user was not found.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nuser = client.users(\"my_username\")\n\nif user is not None:\n    pass\n</code></pre>"},{"location":"admin_guide/user/#list-users-in-a-workspace","title":"List users in a workspace","text":"<p>You can list all the users in a workspace by accessing the <code>users</code> attribute on the <code>Workspace</code> class and iterating over them. You can also use <code>len(workspace.users)</code> to get the number of users by workspace.</p> <p>For further information on how to manage workspaces, check this how-to guide.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nworkspace = client.workspaces('my_workspace')\n\nfor user in workspace.users:\n    print(user)\n</code></pre>"},{"location":"admin_guide/user/#add-a-user-to-a-workspace","title":"Add a user to a workspace","text":"<p>You can add an existing user to a workspace in Argilla by calling the <code>add_to_workspace</code> method on the <code>User</code> class.</p> <p>For further information on how to manage workspaces, check this how-to guide.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nuser = client.users('my_username')\nworkspace = client.workspaces('my_workspace')\n\nadded_user = user.add_to_workspace(workspace)\n</code></pre>"},{"location":"admin_guide/user/#remove-a-user-from-a-workspace","title":"Remove a user from a workspace","text":"<p>You can remove an existing user from a workspace in Argilla by calling the <code>remove_from_workspace</code> method on the <code>User</code> class.</p> <p>For further information on how to manage workspaces, check this how-to guide.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nuser = client.users('my_username')\nworkspace = client.workspaces('my_workspace')\n\nremoved_user = user.remove_from_workspace(workspace)\n</code></pre>"},{"location":"admin_guide/user/#delete-a-user","title":"Delete a user","text":"<p>You can delete an existing user from Argilla by calling the <code>delete</code> method on the <code>User</code> class.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nuser_to_delete = client.users('my_username')\n\ndeleted_user = user_to_delete.delete()\n</code></pre>"},{"location":"admin_guide/workspace/","title":"Workspace management","text":"<p>This guide provides an overview of workspaces, explaining how to set up and manage workspaces in Argilla.</p> <p>A workspace is a space inside your Argilla instance where authorized users can collaborate on datasets. It is accessible through the Python SDK and the UI.</p> Question: Who can manage workspaces? <p>Only users with the <code>owner</code> role can manage (create, read and delete) workspaces.</p> <p>A user with the <code>admin</code> role can only read the workspace to which it belongs.</p>"},{"location":"admin_guide/workspace/#initial-workspaces","title":"Initial workspaces","text":"<p>Depending on your Argilla deployment, the initial workspace will vary.</p> <ul> <li>If you deploy on the Hugging Face Hub, the initial workspace will be the one indicated in the <code>.oauth.yaml</code> file. By default, <code>argilla</code>.</li> <li>If you deploy with Docker, you will need to create a workspace as shown in the next section.</li> </ul> <p>Main Class</p> <pre><code>rg.Workspace(\n    name = \"name\",\n    client=client\n)\n</code></pre> <p>Check the Workspace - Python Reference to see the attributes, arguments, and methods of the <code>Workspace</code> class in detail.</p>"},{"location":"admin_guide/workspace/#create-a-new-workspace","title":"Create a new workspace","text":"<p>To create a new workspace in Argilla, you can define it in the <code>Workspace</code> class and then call the <code>create</code> method. This method is inherited from the <code>Resource</code> base class and operates without modifications.</p> <p>When you create a new workspace, it will be empty. To create and add a new dataset, check these guides.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nworkspace_to_create = rg.Workspace(name=\"my_workspace\")\n\ncreated_workspace = workspace_to_create.create()\n</code></pre> <p>Accessing attributes</p> <p>Access the attributes of a workspace by calling them directly on the <code>Workspace</code> object. For example, <code>workspace.id</code> or <code>workspace.name</code>.</p>"},{"location":"admin_guide/workspace/#list-workspaces","title":"List workspaces","text":"<p>You can list all the existing workspaces in Argilla by calling the <code>workspaces</code> attribute on the <code>Argilla</code> class and iterating over them. You can also use <code>len(client.workspaces)</code> to get the number of workspaces.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nworkspaces = client.workspaces\n\nfor workspace in workspaces:\n    print(workspace)\n</code></pre> <p>Notebooks</p> <p>When using a notebook, executing <code>client.workspaces</code> will display a table with the number of <code>datasets</code> in each workspace, <code>name</code>, <code>id</code>, and the last update as <code>updated_at</code>.</p>"},{"location":"admin_guide/workspace/#retrieve-a-workspace","title":"Retrieve a workspace","text":"<p>You can retrieve a workspace by accessing the <code>workspaces</code> method on the <code>Argilla</code> class and passing the <code>name</code> or <code>id</code> of the workspace as an argument. If the workspace does not exist, a warning message will be raised and <code>None</code> will be returned.</p> By nameBy id <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nretrieved_workspace = client.workspaces(\"my_workspace\")\n</code></pre> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nretrieved_workspace = client.workspaces(id=\"&lt;uuid-or-uuid-string&gt;\")\n</code></pre>"},{"location":"admin_guide/workspace/#check-workspace-existence","title":"Check workspace existence","text":"<p>You can check if a workspace exists. The <code>client.workspaces</code> method will return <code>None</code> if the workspace is not found.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nworkspace = client.workspaces(\"my_workspace\")\n\nif workspace is not None:\n    pass\n</code></pre>"},{"location":"admin_guide/workspace/#list-users-in-a-workspace","title":"List users in a workspace","text":"<p>You can list all the users in a workspace by accessing the <code>users</code> attribute on the <code>Workspace</code> class and iterating over them. You can also use <code>len(workspace.users)</code> to get the number of users by workspace.</p> <p>For further information on how to manage users, check this how-to guide.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nworkspace = client.workspaces('my_workspace')\n\nfor user in workspace.users:\n    print(user)\n</code></pre>"},{"location":"admin_guide/workspace/#add-a-user-to-a-workspace","title":"Add a user to a workspace","text":"<p>You can also add a user to a workspace by calling the <code>add_user</code> method on the <code>Workspace</code> class.</p> <p>For further information on how to manage users, check this how-to guide.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nworkspace = client.workspaces(\"my_workspace\")\n\nadded_user = workspace.add_user(\"my_username\")\n</code></pre>"},{"location":"admin_guide/workspace/#remove-a-user-from-workspace","title":"Remove a user from workspace","text":"<p>You can also remove a user from a workspace by calling the <code>remove_user</code> method on the <code>Workspace</code> class.</p> <p>For further information on how to manage users, check this how-to guide.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nworkspace = client.workspaces(\"my_workspace\")\n\nremoved_user = workspace.remove_user(\"my_username\")\n</code></pre>"},{"location":"admin_guide/workspace/#delete-a-workspace","title":"Delete a workspace","text":"<p>To delete a workspace, no dataset can be associated with it. If the workspace contains any dataset, deletion will fail. You can delete a workspace by calling the <code>delete</code> method on the <code>Workspace</code> class.</p> <p>To clear a workspace and delete all their datasets, refer to this how-to guide.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(api_url=\"&lt;api_url&gt;\", api_key=\"&lt;api_key&gt;\")\n\nworkspace_to_delete = client.workspaces(\"my_workspace\")\n\ndeleted_workspace = workspace_to_delete.delete()\n</code></pre>"},{"location":"community/","title":"Community","text":"<p>We are an open-source community-driven project not only focused on building a great product but also on building a great community, where you can get support, share your experiences, and contribute to the project! We would love to hear from you and help you get started with Argilla.</p> <ul> <li> <p>Discord</p> <p>In our Discord channels (#argilla-distilabel-general and #argilla-distilabel-help), you can get direct support from the community.</p> <p> Discord \u2197</p> </li> <li> <p>Community Meetup</p> <p>We host bi-weekly community meetups where you can listen in or present your work.</p> <p> Community Meetup \u2197</p> </li> <li> <p>Changelog</p> <p>The changelog is where you can find the latest updates and changes to the Argilla project.</p> <p> Changelog \u2197</p> </li> <li> <p>Roadmap</p> <p>We love to discuss our plans with the community. Feel encouraged to participate in our roadmap discussions.</p> <p> Roadmap \u2197</p> </li> </ul>"},{"location":"community/changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"community/changelog/#unreleased","title":"Unreleased","text":""},{"location":"community/changelog/#added","title":"\"Added\"","text":"<ul> <li>Added tests and mocks for extralit FastAPI endpoints</li> <li>Added FileHandler for handling file read/write from disk or S3 to document</li> </ul>"},{"location":"community/changelog/#changed","title":"\"Changed\"","text":"<ul> <li>Changed from \"extralit.app:app\" to \"extralit.server.app:app\".</li> </ul>"},{"location":"community/changelog/#fixed","title":"Fixed","text":"<ul> <li>Fixed a bug in Workspace.get_schemas() method that was returning an empty list</li> </ul>"},{"location":"community/changelog/#v021","title":"v0.2.1","text":""},{"location":"community/changelog/#added_1","title":"\"Added\"","text":"<ul> <li>Added singleton schema support in SchemaStructure</li> <li>Added .devcontainer for \"Docker, Tilt, and K8s\" local development on GH Codespaces</li> <li>Added examples/deployments/k8s/extralit-configs.yaml for configuring the extralit service and secrets in a K8s cluster</li> <li>Added docs site for the extralit project at <code>argilla/docs/</code> </li> <li>Added pytest-xdist for parallel testing</li> <li>Added docker-compose devcontainer</li> </ul>"},{"location":"community/changelog/#changed_1","title":"\"Changed\"","text":"<ul> <li>Updated elasticsearch to 8.15.0</li> <li>Changed K8s elasticsearch deployment from Helm to <code>docker.elastic.co/elasticsearch/elasticsearch</code> to fix PVC restarting issues</li> <li>Refactored extralit dockerfile and Docker Hub images to <code>extralit/argilla-server</code> and <code>extralit/argilla-quickstart</code></li> <li>Changed <code>develop</code> branch changes in argilla/docs to <code>https:/docs.extralit.ai/latest</code> instead of <code>dev</code></li> </ul>"},{"location":"community/changelog/#fixed_1","title":"\"Fixed\"","text":"<ul> <li>Fixed Tiltfile and k8s manifests for mono-repo setup</li> <li>Fixed creating a new Weaviate collection with Weaviate client v4</li> <li>Fixed an error with checking Weaviate collection existence when one doesn't exists</li> <li>Fixed <code>extralit[pdf]</code> installation error by changing deepdoctection requirement</li> <li>Fixed extralit CLI with [server] package option</li> <li>Fixed an issue with llama-index v0.11.0</li> </ul>"},{"location":"community/changelog/#security","title":"\"Security\"","text":"<ul> <li>Allow admin role for workspace creation</li> </ul>"},{"location":"community/changelog/#v020","title":"v0.2.0","text":""},{"location":"community/changelog/#added_2","title":"Added","text":"<ul> <li>Added workspace schema and file management to the Extralit CLI.</li> <li>Introduced the Extralit CLI for improved command-line interactions.</li> <li>Added tooltip in <code>LabelSelection</code>.</li> <li>Added use_table option to <code>QuestionSetting</code>.</li> </ul>"},{"location":"community/changelog/#changed_2","title":"Changed","text":"<ul> <li>Refined workspace schema and file management in the Extralit CLI.</li> <li>Updated <code>rg.Workspace</code> with <code>update_schemas</code> and <code>get_schemas</code> methods.</li> <li>Enabled <code>_ID</code> reference IDs in schemas.</li> <li>Updated status filter options in <code>StatusFilter.vue</code> and <code>RecordRepository.ts</code>.</li> <li>Updated translation for \"Use Table\" option.</li> <li>Updated community links.</li> </ul>"},{"location":"community/changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Fixed import statements in <code>SchemaStructure</code> and <code>Workspace</code>.</li> <li>Ensured <code>.mjs</code> files are properly transpiled with <code>babel-loader</code>.</li> <li>Fixed validation errors in <code>FeedbackRecord</code> suggestions to server payload.</li> <li>Fixed <code>RecordRepository.ts</code> to remove fetching \"All data\".</li> </ul>"},{"location":"community/changelog/#1290","title":"1.29.0","text":""},{"location":"community/changelog/#added_3","title":"Added","text":"<ul> <li>Added support for rating questions to include <code>0</code> as a valid value. (#4860)</li> <li>Added support for Python 3.12. (#4837)</li> <li>Added search by field in the <code>FeedbackDataset</code> UI search. (#4746)</li> <li>Added record metadata info in the <code>FeedbackDataset</code> UI. (#4851)</li> <li>Added highlight on search results in the <code>FeedbackDataset</code> UI. (#4747)</li> </ul>"},{"location":"community/changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Fix wildcard import for the whole argilla module. (#4874)</li> <li>Fix issue when record does not have vectors related. (#4856)</li> <li>Fix issue on character level. (#4836)</li> </ul>"},{"location":"community/changelog/#1280","title":"1.28.0","text":""},{"location":"community/changelog/#added_4","title":"Added","text":"<ul> <li>Added suggestion multi score attribute. (#4730)</li> <li>Added order by suggestion first. (#4731)</li> <li>Added multi selection entity dropdown for span annotation overlap. (#4735)</li> <li>Added pre selection highlight for span annotation. (#4726)</li> <li>Added banner when persistent storage is not enabled. (#4744)</li> <li>Added support on Python SDK for new multi-label questions <code>labels_order</code> attribute. (#4757)</li> </ul>"},{"location":"community/changelog/#changed_3","title":"Changed","text":"<ul> <li>Changed the way how Hugging Face space and user is showed in sign in. (#4748)</li> </ul>"},{"location":"community/changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Fixed Korean character reversed. (#4753)</li> </ul>"},{"location":"community/changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Fixed requirements for version of wrapt library conflicting with Python 3.11 (#4693)</li> </ul>"},{"location":"community/changelog/#1270","title":"1.27.0","text":""},{"location":"community/changelog/#added_5","title":"Added","text":"<ul> <li>Added Allow overlap spans in the <code>FeedbackDataset</code>. (#4668)</li> <li>Added <code>allow_overlapping</code> parameter for span questions. (#4697)</li> <li>Added overall progress bar on <code>Datasets</code> table. (#4696)</li> <li>Added German language translation. (#4688)</li> </ul>"},{"location":"community/changelog/#changed_4","title":"Changed","text":"<ul> <li>New UI design for suggestions. (#4682)</li> </ul>"},{"location":"community/changelog/#fixed_6","title":"Fixed","text":"<ul> <li>Improve performance for more than 250 labels. (#4702)</li> </ul>"},{"location":"community/changelog/#1261","title":"1.26.1","text":""},{"location":"community/changelog/#added_6","title":"Added","text":"<ul> <li>Added support for automatic detection of RTL languages. (#4686)</li> </ul>"},{"location":"community/changelog/#1260","title":"1.26.0","text":""},{"location":"community/changelog/#added_7","title":"Added","text":"<ul> <li>If you expand the labels of a <code>single or multi</code> label Question, the state is maintained during the entire annotation process. (#4630)</li> <li>Added support for span questions in the Python SDK. (#4617)</li> <li>Added support for span values in suggestions and responses. (#4623)</li> <li>Added <code>span</code> questions for <code>FeedbackDataset</code>. (#4622)</li> <li>Added <code>ARGILLA_CACHE_DIR</code> environment variable to configure the client cache directory. (#4509)</li> </ul>"},{"location":"community/changelog/#fixed_7","title":"Fixed","text":"<ul> <li>Fixed contextualized workspaces. (#4665)</li> <li>Fixed prepare for training when passing <code>RankingValueSchema</code> instances to suggestions. (#4628)</li> <li>Fixed parsing ranking values in suggestions from HF datasets. (#4629)</li> <li>Fixed reading description from API response payload. (#4632)</li> <li>Fixed pulling (n*chunk_size)+1 records when using <code>ds.pull</code> or iterating over the dataset. (#4662)</li> <li>Fixed client's resolution of enum values when calling the Search and Metrics api, to support Python &gt;=3.11 enum handling. (#4672)</li> </ul>"},{"location":"community/changelog/#1250","title":"1.25.0","text":"<p>[!NOTE] For changes in the argilla-server module, visit the argilla-server release notes</p>"},{"location":"community/changelog/#added_8","title":"Added","text":"<ul> <li>Reorder labels in <code>dataset settings page</code> for single/multi label questions (#4598)</li> <li>Added pandas v2 support using the python SDK. (#4600)</li> </ul>"},{"location":"community/changelog/#removed","title":"Removed","text":"<ul> <li>Removed <code>missing</code> response for status filter. Use <code>pending</code> instead. (#4533)</li> </ul>"},{"location":"community/changelog/#fixed_8","title":"Fixed","text":"<ul> <li>Fixed FloatMetadataProperty: value is not a valid float (#4570)</li> <li>Fixed redirect to <code>user-settings</code> instead of 404 <code>user_settings</code> (#4609)</li> </ul>"},{"location":"community/changelog/#1240","title":"1.24.0","text":"<p>[!NOTE] This release does not contain any new features, but it includes a major change in the <code>argilla-server</code> dependency. The package is using the <code>argilla-server</code> dependency defined here. (#4537)</p>"},{"location":"community/changelog/#changed_5","title":"Changed","text":"<ul> <li>The package is using the <code>argilla-server</code> dependency defined here. (#4537)</li> </ul>"},{"location":"community/changelog/#1231","title":"1.23.1","text":""},{"location":"community/changelog/#fixed_9","title":"Fixed","text":"<ul> <li>Fixed Responsive view for Feedback Datasets. (#4579)</li> </ul>"},{"location":"community/changelog/#1230","title":"1.23.0","text":""},{"location":"community/changelog/#added_9","title":"Added","text":"<ul> <li>Added bulk annotation by filter criteria. (#4516)</li> <li>Automatically fetch new datasets on focus tab. (#4514)</li> <li>API v1 responses returning <code>Record</code> schema now always include <code>dataset_id</code> as attribute. (#4482)</li> <li>API v1 responses returning <code>Response</code> schema now always include <code>record_id</code> as attribute. (#4482)</li> <li>API v1 responses returning <code>Question</code> schema now always include <code>dataset_id</code> attribute. (#4487)</li> <li>API v1 responses returning <code>Field</code> schema now always include <code>dataset_id</code> attribute. (#4488)</li> <li>API v1 responses returning <code>MetadataProperty</code> schema now always include <code>dataset_id</code> attribute. (#4489)</li> <li>API v1 responses returning <code>VectorSettings</code> schema now always include <code>dataset_id</code> attribute. (#4490)</li> <li>Added <code>pdf_to_html</code> function to <code>.html_utils</code> module that convert PDFs to dataURL to be able to render them in tha Argilla UI. (#4481)</li> <li>Added <code>ARGILLA_AUTH_SECRET_KEY</code> environment variable. (#4539)</li> <li>Added <code>ARGILLA_AUTH_ALGORITHM</code> environment variable. (#4539)</li> <li>Added <code>ARGILLA_AUTH_TOKEN_EXPIRATION</code> environment variable. (#4539)</li> <li>Added <code>ARGILLA_AUTH_OAUTH_CFG</code> environment variable. (#4546)</li> <li>Added OAuth2 support for HuggingFace Hub. (#4546)</li> </ul>"},{"location":"community/changelog/#deprecated","title":"Deprecated","text":"<ul> <li>Deprecated <code>ARGILLA_LOCAL_AUTH_*</code> environment variables. Will be removed in the release v1.25.0. (#4539)</li> </ul>"},{"location":"community/changelog/#changed_6","title":"Changed","text":"<ul> <li>Changed regex pattern for <code>username</code> attribute in <code>UserCreate</code>. Now uppercase letters are allowed. (#4544)</li> </ul>"},{"location":"community/changelog/#removed_1","title":"Removed","text":"<ul> <li>Remove sending <code>Authorization</code> header from python SDK requests. (#4535)</li> </ul>"},{"location":"community/changelog/#fixed_10","title":"Fixed","text":"<ul> <li>Fixed keyboard shortcut for label questions. (#4530)</li> </ul>"},{"location":"community/changelog/#1220","title":"1.22.0","text":""},{"location":"community/changelog/#added_10","title":"Added","text":"<ul> <li>Added Bulk annotation support. (#4333)</li> <li>Restore filters from feedback dataset settings. ([#4461])(https://github.com/argilla-io/argilla/pull/4461)</li> <li>Warning on feedback dataset settings when leaving page with unsaved changes. (#4461)</li> <li>Added pydantic v2 support using the python SDK. (#4459)</li> <li>Added <code>vector_settings</code> to the <code>__repr__</code> method of the <code>FeedbackDataset</code> and <code>RemoteFeedbackDataset</code>. (#4454)</li> <li>Added integration for <code>sentence-transformers</code> using <code>SentenceTransformersExtractor</code> to configure <code>vector_settings</code> in <code>FeedbackDataset</code> and <code>FeedbackRecord</code>. (#4454)</li> </ul>"},{"location":"community/changelog/#changed_7","title":"Changed","text":"<ul> <li>Module <code>argilla.cli.server</code> definitions have been moved to <code>argilla.server.cli</code> module. (#4472)</li> <li>[breaking] Changed <code>vector_settings_by_name</code> for generic <code>property_by_name</code> usage, which will return <code>None</code> instead of raising an error. (#4454)</li> <li>The constant definition <code>ES_INDEX_REGEX_PATTERN</code> in module <code>argilla._constants</code> is now private. (#4472)</li> <li><code>nan</code> values in metadata properties will raise a 422 error when creating/updating records. (#4300)</li> <li><code>None</code> values are now allowed in metadata properties. (#4300)</li> <li>Refactor and add <code>width</code>, <code>height</code>, <code>autoplay</code> and <code>loop</code> attributes as optional args in <code>to_html</code> functions. (#4481)</li> </ul>"},{"location":"community/changelog/#fixed_11","title":"Fixed","text":"<ul> <li>Paginating to a new record, automatically scrolls down to selected form area. (#4333)</li> </ul>"},{"location":"community/changelog/#deprecated_1","title":"Deprecated","text":"<ul> <li>The <code>missing</code> response status for filtering records is deprecated and will be removed in the release v1.24.0. Use <code>pending</code> instead. (#4433)</li> </ul>"},{"location":"community/changelog/#removed_2","title":"Removed","text":"<ul> <li>The deprecated <code>python -m argilla database</code> command has been removed. (#4472)</li> </ul>"},{"location":"community/changelog/#1210","title":"1.21.0","text":""},{"location":"community/changelog/#added_11","title":"Added","text":"<ul> <li>Added new draft queue for annotation view (#4334)</li> <li>Added annotation metrics module for the <code>FeedbackDataset</code> (<code>argilla.client.feedback.metrics</code>). (#4175).</li> <li>Added strategy to handle and translate errors from the server for <code>401</code> HTTP status code` (#4362)</li> <li>Added integration for <code>textdescriptives</code> using <code>TextDescriptivesExtractor</code> to configure <code>metadata_properties</code> in <code>FeedbackDataset</code> and <code>FeedbackRecord</code>. (#4400). Contributed by @m-newhauser</li> <li>Added <code>POST /api/v1/me/responses/bulk</code> endpoint to create responses in bulk for current user. (#4380)</li> <li>Added list support for term metadata properties. (Closes #4359)</li> <li>Added new CLI task to reindex datasets and records into the search engine. (#4404)</li> <li>Added <code>httpx_extra_kwargs</code> argument to <code>rg.init</code> and <code>Argilla</code> to allow passing extra arguments to <code>httpx.Client</code> used by <code>Argilla</code>. (#4440)</li> <li>Added <code>ResponseStatusFilter</code> enum in <code>__init__</code> imports of Argilla (#4118). Contributed by @Piyush-Kumar-Ghosh.</li> </ul>"},{"location":"community/changelog/#changed_8","title":"Changed","text":"<ul> <li>More productive and simpler shortcut system (#4215)</li> <li>Move <code>ArgillaSingleton</code>, <code>init</code> and <code>active_client</code> to a new module <code>singleton</code>. (#4347)</li> <li>Updated <code>argilla.load</code> functions to also work with <code>FeedbackDataset</code>s. (#4347)</li> <li>[breaking] Updated <code>argilla.delete</code> functions to also work with <code>FeedbackDataset</code>s. It now raises an error if the dataset does not exist. (#4347)</li> <li>Updated <code>argilla.list_datasets</code> functions to also work with <code>FeedbackDataset</code>s. (#4347)</li> </ul>"},{"location":"community/changelog/#fixed_12","title":"Fixed","text":"<ul> <li>Fixed error in <code>TextClassificationSettings.from_dict</code> method in which the <code>label_schema</code> created was a list of <code>dict</code> instead of a list of <code>str</code>. (#4347)</li> <li>Fixed total records on pagination component (#4424)</li> </ul>"},{"location":"community/changelog/#removed_3","title":"Removed","text":"<ul> <li>Removed <code>draft</code> auto save for annotation view (#4334)</li> </ul>"},{"location":"community/changelog/#1200","title":"1.20.0","text":""},{"location":"community/changelog/#added_12","title":"Added","text":"<ul> <li>Added <code>GET /api/v1/datasets/:dataset_id/records/search/suggestions/options</code> endpoint to return suggestion available options for searching. (#4260)</li> <li>Added <code>metadata_properties</code> to the <code>__repr__</code> method of the <code>FeedbackDataset</code> and <code>RemoteFeedbackDataset</code>.(#4192).</li> <li>Added <code>get_model_kwargs</code>, <code>get_trainer_kwargs</code>, <code>get_trainer_model</code>, <code>get_trainer_tokenizer</code> and <code>get_trainer</code> -methods to the <code>ArgillaTrainer</code> to improve interoperability across frameworks. (#4214).</li> <li>Added additional formatting checks to the <code>ArgillaTrainer</code> to allow for better interoperability of <code>defaults</code> and <code>formatting_func</code> usage. (#4214).</li> <li>Added a warning to the <code>update_config</code>-method of <code>ArgillaTrainer</code> to emphasize if the <code>kwargs</code> were updated correctly. (#4214).</li> <li>Added <code>argilla.client.feedback.utils</code> module with <code>html_utils</code> (this mainly includes <code>video/audio/image_to_html</code> that convert media to dataURL to be able to render them in tha Argilla UI and <code>create_token_highlights</code> to highlight tokens in a custom way. Both work on TextQuestion and TextField with use_markdown=True) and <code>assignments</code> (this mainly includes <code>assign_records</code> to assign records according to a number of annotators and records, an overlap and the shuffle option; and <code>assign_workspace</code> to assign and create if needed a workspace according to the record assignment). (#4121)</li> </ul>"},{"location":"community/changelog/#fixed_13","title":"Fixed","text":"<ul> <li>Fixed error in <code>ArgillaTrainer</code>, with numerical labels, using <code>RatingQuestion</code> instead of <code>RankingQuestion</code> (#4171)</li> <li>Fixed error in <code>ArgillaTrainer</code>, now we can train for <code>extractive_question_answering</code> using a validation sample (#4204)</li> <li>Fixed error in <code>ArgillaTrainer</code>, when training for <code>sentence-similarity</code> it didn't work with a list of values per record (#4211)</li> <li>Fixed error in the unification strategy for <code>RankingQuestion</code> (#4295)</li> <li>Fixed <code>TextClassificationSettings.labels_schema</code> order was not being preserved. Closes #3828 (#4332)</li> <li>Fixed error when requesting non-existing API endpoints. Closes #4073 (#4325)</li> <li>Fixed error when passing <code>draft</code> responses to create records endpoint. (#4354)</li> </ul>"},{"location":"community/changelog/#changed_9","title":"Changed","text":"<ul> <li>[breaking] Suggestions <code>agent</code> field only accepts now some specific characters and a limited length. (#4265)</li> <li>[breaking] Suggestions <code>score</code> field only accepts now float values in the range <code>0</code> to <code>1</code>. (#4266)</li> <li>Updated <code>POST /api/v1/dataset/:dataset_id/records/search</code> endpoint to support optional <code>query</code> attribute. (#4327)</li> <li>Updated <code>POST /api/v1/dataset/:dataset_id/records/search</code> endpoint to support <code>filter</code> and <code>sort</code> attributes. (#4327)</li> <li>Updated <code>POST /api/v1/me/datasets/:dataset_id/records/search</code> endpoint to support optional <code>query</code> attribute. (#4270)</li> <li>Updated <code>POST /api/v1/me/datasets/:dataset_id/records/search</code> endpoint to support <code>filter</code> and <code>sort</code> attributes. (#4270)</li> <li>Changed the logging style while pulling and pushing <code>FeedbackDataset</code> to Argilla from <code>tqdm</code> style to <code>rich</code>. (#4267). Contributed by @zucchini-nlp.</li> <li>Updated <code>push_to_argilla</code> to print <code>repr</code> of the pushed <code>RemoteFeedbackDataset</code> after push and changed <code>show_progress</code> to True by default. (#4223)</li> <li>Changed <code>models</code> and <code>tokenizer</code> for the <code>ArgillaTrainer</code> to explicitly allow for changing them when needed. (#4214).</li> </ul>"},{"location":"community/changelog/#1190","title":"1.19.0","text":""},{"location":"community/changelog/#added_13","title":"Added","text":"<ul> <li>Added <code>POST /api/v1/datasets/:dataset_id/records/search</code> endpoint to search for records without user context, including responses by all users. (#4143)</li> <li>Added <code>POST /api/v1/datasets/:dataset_id/vectors-settings</code> endpoint for creating vector settings for a dataset. (#3776)</li> <li>Added <code>GET /api/v1/datasets/:dataset_id/vectors-settings</code> endpoint for listing the vectors settings for a dataset. (#3776)</li> <li>Added <code>DELETE /api/v1/vectors-settings/:vector_settings_id</code> endpoint for deleting a vector settings. (#3776)</li> <li>Added <code>PATCH /api/v1/vectors-settings/:vector_settings_id</code> endpoint for updating a vector settings. (#4092)</li> <li>Added <code>GET /api/v1/records/:record_id</code> endpoint to get a specific record. (#4039)</li> <li>Added support to include vectors for <code>GET /api/v1/datasets/:dataset_id/records</code> endpoint response using <code>include</code> query param. (#4063)</li> <li>Added support to include vectors for <code>GET /api/v1/me/datasets/:dataset_id/records</code> endpoint response using <code>include</code> query param. (#4063)</li> <li>Added support to include vectors for <code>POST /api/v1/me/datasets/:dataset_id/records/search</code> endpoint response using <code>include</code> query param. (#4063)</li> <li>Added <code>show_progress</code> argument to <code>from_huggingface()</code> method to make the progress bar for parsing records process optional.(#4132).</li> <li>Added a progress bar for parsing records process to <code>from_huggingface()</code> method with <code>trange</code> in <code>tqdm</code>.(#4132).</li> <li>Added to sort by <code>inserted_at</code> or <code>updated_at</code> for datasets with no metadata. (4147)</li> <li>Added <code>max_records</code> argument to <code>pull()</code> method for <code>RemoteFeedbackDataset</code>.(#4074)</li> <li>Added functionality to push your models to the Hugging Face hub with <code>ArgillaTrainer.push_to_huggingface</code> (#3976). Contributed by @Racso-3141.</li> <li>Added <code>filter_by</code> argument to <code>ArgillaTrainer</code> to filter by <code>response_status</code> (#4120).</li> <li>Added <code>sort_by</code> argument to <code>ArgillaTrainer</code> to sort by <code>metadata</code> (#4120).</li> <li>Added <code>max_records</code> argument to <code>ArgillaTrainer</code> to limit record used for training (#4120).</li> <li>Added <code>add_vector_settings</code> method to local and remote <code>FeedbackDataset</code>. (#4055)</li> <li>Added <code>update_vectors_settings</code> method to local and remote <code>FeedbackDataset</code>. (#4122)</li> <li>Added <code>delete_vectors_settings</code> method to local and remote <code>FeedbackDataset</code>. (#4130)</li> <li>Added <code>vector_settings_by_name</code> method to local and remote <code>FeedbackDataset</code>. (#4055)</li> <li>Added <code>find_similar_records</code> method to local and remote <code>FeedbackDataset</code>. (#4023)</li> <li>Added <code>ARGILLA_SEARCH_ENGINE</code> environment variable to configure the search engine to use. (#4019)</li> </ul>"},{"location":"community/changelog/#changed_10","title":"Changed","text":"<ul> <li>[breaking] Remove support for Elasticsearch &lt; 8.5 and OpenSearch &lt; 2.4. (#4173)</li> <li>[breaking] Users working with OpenSearch engines must use version &gt;=2.4 and set <code>ARGILLA_SEARCH_ENGINE=opensearch</code>. (#4019 and #4111)</li> <li>[breaking] Changed <code>FeedbackDataset.*_by_name()</code> methods to return <code>None</code> when no match is found (#4101).</li> <li>[breaking] <code>limit</code> query parameter for <code>GET /api/v1/datasets/:dataset_id/records</code> endpoint is now only accepting values greater or equal than <code>1</code> and less or equal than <code>1000</code>. (#4143)</li> <li>[breaking] <code>limit</code> query parameter for <code>GET /api/v1/me/datasets/:dataset_id/records</code> endpoint is now only accepting values greater or equal than <code>1</code> and less or equal than <code>1000</code>. (#4143)</li> <li>Update <code>GET /api/v1/datasets/:dataset_id/records</code> endpoint to fetch record using the search engine. (#4142)</li> <li>Update <code>GET /api/v1/me/datasets/:dataset_id/records</code> endpoint to fetch record using the search engine. (#4142)</li> <li>Update <code>POST /api/v1/datasets/:dataset_id/records</code> endpoint to allow to create records with <code>vectors</code> (#4022)</li> <li>Update <code>PATCH /api/v1/datasets/:dataset_id</code> endpoint to allow updating <code>allow_extra_metadata</code> attribute. (#4112)</li> <li>Update <code>PATCH /api/v1/datasets/:dataset_id/records</code> endpoint to allow to update records with <code>vectors</code>. (#4062)</li> <li>Update <code>PATCH /api/v1/records/:record_id</code> endpoint to allow to update record with <code>vectors</code>. (#4062)</li> <li>Update <code>POST /api/v1/me/datasets/:dataset_id/records/search</code> endpoint to allow to search records with vectors. (#4019)</li> <li>Update <code>BaseElasticAndOpenSearchEngine.index_records</code> method to also index record vectors. (#4062)</li> <li>Update <code>FeedbackDataset.__init__</code> to allow passing a list of vector settings. (#4055)</li> <li>Update <code>FeedbackDataset.push_to_argilla</code> to also push vector settings. (#4055)</li> <li>Update <code>FeedbackDatasetRecord</code> to support the creation of records with vectors. (#4043)</li> <li>Using cosine similarity to compute similarity between vectors. (#4124)</li> </ul>"},{"location":"community/changelog/#fixed_14","title":"Fixed","text":"<ul> <li>Fixed svg images out of screen with too large images (#4047)</li> <li>Fixed creating records with responses from multiple users. Closes #3746 and #3808 (#4142)</li> <li>Fixed deleting or updating responses as an owner for annotators. (Commit 403a66d)</li> <li>Fixed passing user_id when getting records by id. (Commit 98c7927)</li> <li>Fixed non-basic tags serialized when pushing a dataset to the Hugging Face Hub. Closes #4089 (#4200)</li> </ul>"},{"location":"community/changelog/#1180","title":"1.18.0","text":""},{"location":"community/changelog/#added_14","title":"Added","text":"<ul> <li>New <code>GET /api/v1/datasets/:dataset_id/metadata-properties</code> endpoint for listing dataset metadata properties. (#3813)</li> <li>New <code>POST /api/v1/datasets/:dataset_id/metadata-properties</code> endpoint for creating dataset metadata properties. (#3813)</li> <li>New <code>PATCH /api/v1/metadata-properties/:metadata_property_id</code> endpoint allowing the update of a specific metadata property. (#3952)</li> <li>New <code>DELETE /api/v1/metadata-properties/:metadata_property_id</code> endpoint for deletion of a specific metadata property. (#3911)</li> <li>New <code>GET /api/v1/metadata-properties/:metadata_property_id/metrics</code> endpoint to compute metrics for a specific metadata property. (#3856)</li> <li>New <code>PATCH /api/v1/records/:record_id</code> endpoint to update a record. (#3920)</li> <li>New <code>PATCH /api/v1/dataset/:dataset_id/records</code> endpoint to bulk update the records of a dataset. (#3934)</li> <li>Missing validations to <code>PATCH /api/v1/questions/:question_id</code>. Now <code>title</code> and <code>description</code> are using the same validations used to create questions. (#3967)</li> <li>Added <code>TermsMetadataProperty</code>, <code>IntegerMetadataProperty</code> and <code>FloatMetadataProperty</code> classes allowing to define metadata properties for a <code>FeedbackDataset</code>. (#3818)</li> <li>Added <code>metadata_filters</code> to <code>filter_by</code> method in <code>RemoteFeedbackDataset</code> to filter based on metadata i.e. <code>TermsMetadataFilter</code>, <code>IntegerMetadataFilter</code>, and <code>FloatMetadataFilter</code>. (#3834)</li> <li>Added a validation layer for both <code>metadata_properties</code> and <code>metadata_filters</code> in their schemas and as part of the <code>add_records</code> and <code>filter_by</code> methods, respectively. (#3860)</li> <li>Added <code>sort_by</code> query parameter to listing records endpoints that allows to sort the records by <code>inserted_at</code>, <code>updated_at</code> or metadata property. (#3843)</li> <li>Added <code>add_metadata_property</code> method to both <code>FeedbackDataset</code> and <code>RemoteFeedbackDataset</code> (i.e. <code>FeedbackDataset</code> in Argilla). (#3900)</li> <li>Added fields <code>inserted_at</code> and <code>updated_at</code> in <code>RemoteResponseSchema</code>. (#3822)</li> <li>Added support for <code>sort_by</code> for <code>RemoteFeedbackDataset</code> i.e. a <code>FeedbackDataset</code> uploaded to Argilla. (#3925)</li> <li>Added <code>metadata_properties</code> support for both <code>push_to_huggingface</code> and <code>from_huggingface</code>. (#3947)</li> <li>Add support for update records (<code>metadata</code>) from Python SDK. (#3946)</li> <li>Added <code>delete_metadata_properties</code> method to delete metadata properties. (#3932)</li> <li>Added <code>update_metadata_properties</code> method to update <code>metadata_properties</code>. (#3961)</li> <li>Added automatic model card generation through <code>ArgillaTrainer.save</code> (#3857)</li> <li>Added <code>FeedbackDataset</code> <code>TaskTemplateMixin</code> for pre-defined task templates. (#3969)</li> <li>A maximum limit of 50 on the number of options a ranking question can accept. (#3975)</li> <li>New <code>last_activity_at</code> field to <code>FeedbackDataset</code> exposing when the last activity for the associated dataset occurs. (#3992)</li> </ul>"},{"location":"community/changelog/#changed_11","title":"Changed","text":"<ul> <li><code>GET /api/v1/datasets/{dataset_id}/records</code>, <code>GET /api/v1/me/datasets/{dataset_id}/records</code> and <code>POST /api/v1/me/datasets/{dataset_id}/records/search</code> endpoints to return the <code>total</code> number of records. (#3848, #3903)</li> <li>Implemented <code>__len__</code> method for filtered datasets to return the number of records matching the provided filters. (#3916)</li> <li>Increase the default max result window for Elasticsearch created for Feedback datasets. (#3929)</li> <li>Force elastic index refresh after records creation. (#3929)</li> <li>Validate metadata fields for filtering and sorting in the Python SDK. (#3993)</li> <li>Using metadata property name instead of id for indexing data in search engine index. (#3994)</li> </ul>"},{"location":"community/changelog/#fixed_15","title":"Fixed","text":"<ul> <li>Fixed response schemas to allow <code>values</code> to be <code>None</code> i.e. when a record is discarded the <code>response.values</code> are set to <code>None</code>. (#3926)</li> </ul>"},{"location":"community/changelog/#1170","title":"1.17.0","text":""},{"location":"community/changelog/#added_15","title":"Added","text":"<ul> <li>Added fields <code>inserted_at</code> and <code>updated_at</code> in <code>RemoteResponseSchema</code> (#3822).</li> <li>Added automatic model card generation through <code>ArgillaTrainer.save</code> (#3857).</li> <li>Added task templates to the <code>FeedbackDataset</code> (#3973).</li> </ul>"},{"location":"community/changelog/#changed_12","title":"Changed","text":"<ul> <li>Updated <code>Dockerfile</code> to use multi stage build (#3221 and #3793).</li> <li>Updated active learning for text classification notebooks to use the most recent small-text version (#3831).</li> <li>Changed argilla dataset name in the active learning for text classification notebooks to be consistent with the default names in the huggingface spaces (#3831).</li> <li>FeedbackDataset API methods have been aligned to be accessible through the several implementations (#3937).</li> <li>The <code>unify_responses</code> support for remote datasets (#3937).</li> </ul>"},{"location":"community/changelog/#fixed_16","title":"Fixed","text":"<ul> <li>Fix field not shown in the order defined in the dataset settings. Closes #3959 (#3984)</li> <li>Updated active learning for text classification notebooks to pass ids of type int to <code>TextClassificationRecord</code> (#3831).</li> <li>Fixed record fields validation that was preventing from logging records with optional fields (i.e. <code>required=True</code>) when the field value was <code>None</code> (#3846).</li> <li>Always set <code>pretrained_model_name_or_path</code> attribute as string in <code>ArgillaTrainer</code> (#3914).</li> <li>The <code>inserted_at</code> and <code>updated_at</code> attributes are create using the <code>utcnow</code> factory to avoid unexpected race conditions on timestamp creation (#3945)</li> <li>Fixed <code>configure_dataset_settings</code> when providing the workspace via the arg <code>workspace</code> (#3887).</li> <li>Fixed saving of models trained with <code>ArgillaTrainer</code> with a <code>peft_config</code> parameter (#3795).</li> <li>Fixed backwards compatibility on <code>from_huggingface</code> when loading a <code>FeedbackDataset</code> from the Hugging Face Hub that was previously dumped using another version of Argilla, starting at 1.8.0, when it was first introduced (#3829).</li> <li>Fixed wrong <code>__repr__</code> problem for <code>TrainingTask</code>. (#3969)</li> <li>Fixed wrong key return error <code>prepare_for_training_with_*</code> for <code>TrainingTask</code>. (#3969)</li> </ul>"},{"location":"community/changelog/#deprecated_2","title":"Deprecated","text":"<ul> <li>Function <code>rg.configure_dataset</code> is deprecated in favour of <code>rg.configure_dataset_settings</code>. The former will be removed in version 1.19.0</li> </ul>"},{"location":"community/changelog/#1160","title":"1.16.0","text":""},{"location":"community/changelog/#added_16","title":"Added","text":"<ul> <li>Added <code>ArgillaTrainer</code> integration with sentence-transformers, allowing fine tuning for sentence similarity (#3739)</li> <li>Added <code>ArgillaTrainer</code> integration with <code>TrainingTask.for_question_answering</code> (#3740)</li> <li>Added <code>Auto save record</code> to save automatically the current record that you are working on (#3541)</li> <li>Added <code>ArgillaTrainer</code> integration with OpenAI, allowing fine tuning for chat completion (#3615)</li> <li>Added <code>workspaces list</code> command to list Argilla workspaces (#3594).</li> <li>Added <code>datasets list</code> command to list Argilla datasets (#3658).</li> <li>Added <code>users create</code> command to create users (#3667).</li> <li>Added <code>whoami</code> command to get current user (#3673).</li> <li>Added <code>users delete</code> command to delete users (#3671).</li> <li>Added <code>users list</code> command to list users (#3688).</li> <li>Added <code>workspaces delete-user</code> command to remove a user from a workspace (#3699).</li> <li>Added <code>datasets list</code> command to list Argilla datasets (#3658).</li> <li>Added <code>users create</code> command to create users (#3667).</li> <li>Added <code>users delete</code> command to delete users (#3671).</li> <li>Added <code>workspaces create</code> command to create an Argilla workspace (#3676).</li> <li>Added <code>datasets push-to-hub</code> command to push a <code>FeedbackDataset</code> from Argilla into the HuggingFace Hub (#3685).</li> <li>Added <code>info</code> command to get info about the used Argilla client and server (#3707).</li> <li>Added <code>datasets delete</code> command to delete a <code>FeedbackDataset</code> from Argilla (#3703).</li> <li>Added <code>created_at</code> and <code>updated_at</code> properties to <code>RemoteFeedbackDataset</code> and <code>FilteredRemoteFeedbackDataset</code> (#3709).</li> <li>Added handling <code>PermissionError</code> when executing a command with a logged in user with not enough permissions (#3717).</li> <li>Added <code>workspaces add-user</code> command to add a user to workspace (#3712).</li> <li>Added <code>workspace_id</code> param to <code>GET /api/v1/me/datasets</code> endpoint (#3727).</li> <li>Added <code>workspace_id</code> arg to <code>list_datasets</code> in the Python SDK (#3727).</li> <li>Added <code>argilla</code> script that allows to execute Argilla CLI using the <code>argilla</code> command (#3730).</li> <li>Added support for passing already initialized <code>model</code> and <code>tokenizer</code> instances to the <code>ArgillaTrainer</code> (#3751)</li> <li>Added <code>server_info</code> function to check the Argilla server information (also accessible via <code>rg.server_info</code>) (#3772).</li> </ul>"},{"location":"community/changelog/#changed_13","title":"Changed","text":"<ul> <li>Move <code>database</code> commands under <code>server</code> group of commands (#3710)</li> <li><code>server</code> commands only included in the CLI app when <code>server</code> extra requirements are installed (#3710).</li> <li>Updated <code>PUT /api/v1/responses/{response_id}</code> to replace <code>values</code> stored with received <code>values</code> in request (#3711).</li> <li>Display a <code>UserWarning</code> when the <code>user_id</code> in <code>Workspace.add_user</code> and <code>Workspace.delete_user</code> is the ID of an user with the owner role as they don't require explicit permissions (#3716).</li> <li>Rename <code>tasks</code> sub-package to <code>cli</code> (#3723).</li> <li>Changed <code>argilla database</code> command in the CLI to now be accessed via <code>argilla server database</code>, to be deprecated in the upcoming release (#3754).</li> <li>Changed <code>visible_options</code> (of label and multi label selection questions) validation in the backend to check that the provided value is greater or equal than/to 3 and less or equal than/to the number of provided options (#3773).</li> </ul>"},{"location":"community/changelog/#fixed_17","title":"Fixed","text":"<ul> <li>Fixed <code>remove user modification in text component on clear answers</code> (#3775)</li> <li>Fixed <code>Highlight raw text field in dataset feedback task</code> (#3731)</li> <li>Fixed <code>Field title too long</code> (#3734)</li> <li>Fixed error messages when deleting a <code>DatasetForTextClassification</code> (#3652)</li> <li>Fixed <code>Pending queue</code> pagination problems when during data annotation (#3677)</li> <li>Fixed <code>visible_labels</code> default value to be 20 just when <code>visible_labels</code> not provided and <code>len(labels) &gt; 20</code>, otherwise it will either be the provided <code>visible_labels</code> value or <code>None</code>, for <code>LabelQuestion</code> and <code>MultiLabelQuestion</code> (#3702).</li> <li>Fixed <code>DatasetCard</code> generation when <code>RemoteFeedbackDataset</code> contains suggestions (#3718).</li> <li>Add missing <code>draft</code> status in <code>ResponseSchema</code> as now there can be responses with <code>draft</code> status when annotating via the UI (#3749).</li> <li>Searches when queried words are distributed along the record fields (#3759).</li> <li>Fixed Python 3.11 compatibility issue with <code>/api/datasets</code> endpoints due to the <code>TaskType</code> enum replacement in the endpoint URL (#3769).</li> <li>Fixed <code>RankingValueSchema</code> and <code>FeedbackRankingValueModel</code> schemas to allow <code>rank=None</code> when <code>status=draft</code> (#3781).</li> </ul>"},{"location":"community/changelog/#1151","title":"1.15.1","text":""},{"location":"community/changelog/#fixed_18","title":"Fixed","text":"<ul> <li>Fixed <code>Text component</code> text content sanitization behavior just for markdown to prevent disappear the text(#3738)</li> <li>Fixed <code>Text component</code> now you need to press Escape to exit the text area (#3733)</li> <li>Fixed <code>SearchEngine</code> was creating the same number of primary shards and replica shards for each <code>FeedbackDataset</code> (#3736).</li> </ul>"},{"location":"community/changelog/#1150","title":"1.15.0","text":""},{"location":"community/changelog/#added_17","title":"Added","text":"<ul> <li>Added <code>Enable to update guidelines and dataset settings for Feedback Datasets directly in the UI</code> (#3489)</li> <li>Added <code>ArgillaTrainer</code> integration with TRL, allowing for easy supervised finetuning, reward modeling, direct preference optimization and proximal policy optimization (#3467)</li> <li>Added <code>formatting_func</code> to <code>ArgillaTrainer</code> for <code>FeedbackDataset</code> datasets add a custom formatting for the data (#3599).</li> <li>Added <code>login</code> function in <code>argilla.client.login</code> to login into an Argilla server and store the credentials locally (#3582).</li> <li>Added <code>login</code> command to login into an Argilla server (#3600).</li> <li>Added <code>logout</code> command to logout from an Argilla server (#3605).</li> <li>Added <code>DELETE /api/v1/suggestions/{suggestion_id}</code> endpoint to delete a suggestion given its ID (#3617).</li> <li>Added <code>DELETE /api/v1/records/{record_id}/suggestions</code> endpoint to delete several suggestions linked to the same record given their IDs (#3617).</li> <li>Added <code>response_status</code> param to <code>GET /api/v1/datasets/{dataset_id}/records</code> to be able to filter by <code>response_status</code> as previously included for <code>GET /api/v1/me/datasets/{dataset_id}/records</code> (#3613).</li> <li>Added <code>list</code> classmethod to <code>ArgillaMixin</code> to be used as <code>FeedbackDataset.list()</code>, also including the <code>workspace</code> to list from as arg (#3619).</li> <li>Added <code>filter_by</code> method in <code>RemoteFeedbackDataset</code> to filter based on <code>response_status</code> (#3610).</li> <li>Added <code>list_workspaces</code> function (to be used as <code>rg.list_workspaces</code>, but <code>Workspace.list</code> is preferred) to list all the workspaces from an user in Argilla (#3641).</li> <li>Added <code>list_datasets</code> function (to be used as <code>rg.list_datasets</code>) to list the <code>TextClassification</code>, <code>TokenClassification</code>, and <code>Text2Text</code> datasets in Argilla (#3638).</li> <li>Added <code>RemoteSuggestionSchema</code> to manage suggestions in Argilla, including the <code>delete</code> method to delete suggestios from Argilla via <code>DELETE /api/v1/suggestions/{suggestion_id}</code> (#3651).</li> <li>Added <code>delete_suggestions</code> to <code>RemoteFeedbackRecord</code> to remove suggestions from Argilla via <code>DELETE /api/v1/records/{record_id}/suggestions</code> (#3651).</li> </ul>"},{"location":"community/changelog/#changed_14","title":"Changed","text":"<ul> <li>Changed <code>Optional label for * mark for required question</code> (#3608)</li> <li>Updated <code>RemoteFeedbackDataset.delete_records</code> to use batch delete records endpoint (#3580).</li> <li>Included <code>allowed_for_roles</code> for some <code>RemoteFeedbackDataset</code>, <code>RemoteFeedbackRecords</code>, and <code>RemoteFeedbackRecord</code> methods that are only allowed for users with roles <code>owner</code> and <code>admin</code> (#3601).</li> <li>Renamed <code>ArgillaToFromMixin</code> to <code>ArgillaMixin</code> (#3619).</li> <li>Move <code>users</code> CLI app under <code>database</code> CLI app (#3593).</li> <li>Move server <code>Enum</code> classes to <code>argilla.server.enums</code> module (#3620).</li> </ul>"},{"location":"community/changelog/#fixed_19","title":"Fixed","text":"<ul> <li>Fixed <code>Filter by workspace in breadcrumbs</code> (#3577)</li> <li>Fixed <code>Filter by workspace in datasets table</code> (#3604)</li> <li>Fixed <code>Query search highlight</code> for Text2Text and TextClassification (#3621)</li> <li>Fixed <code>RatingQuestion.values</code> validation to raise a <code>ValidationError</code> when values are out of range i.e. [1, 10] (#3626).</li> </ul>"},{"location":"community/changelog/#removed_4","title":"Removed","text":"<ul> <li>Removed <code>multi_task_text_token_classification</code> from <code>TaskType</code> as not used (#3640).</li> <li>Removed <code>argilla_id</code> in favor of <code>id</code> from <code>RemoteFeedbackDataset</code> (#3663).</li> <li>Removed <code>fetch_records</code> from <code>RemoteFeedbackDataset</code> as now the records are lazily fetched from Argilla (#3663).</li> <li>Removed <code>push_to_argilla</code> from <code>RemoteFeedbackDataset</code>, as it just works when calling it through a <code>FeedbackDataset</code> locally, as now the updates of the remote datasets are automatically pushed to Argilla (#3663).</li> <li>Removed <code>set_suggestions</code> in favor of <code>update(suggestions=...)</code> for both <code>FeedbackRecord</code> and <code>RemoteFeedbackRecord</code>, as all the updates of any \"updateable\" attribute of a record will go through <code>update</code> instead (#3663).</li> <li>Remove unused <code>owner</code> attribute for client Dataset data model (#3665)</li> </ul>"},{"location":"community/changelog/#1141","title":"1.14.1","text":""},{"location":"community/changelog/#fixed_20","title":"Fixed","text":"<ul> <li>Fixed PostgreSQL database not being updated after <code>begin_nested</code> because of missing <code>commit</code> (#3567).</li> </ul>"},{"location":"community/changelog/#fixed_21","title":"Fixed","text":"<ul> <li>Fixed <code>settings</code> could not be provided when updating a <code>rating</code> or <code>ranking</code> question (#3552).</li> </ul>"},{"location":"community/changelog/#1140","title":"1.14.0","text":""},{"location":"community/changelog/#added_18","title":"Added","text":"<ul> <li>Added <code>PATCH /api/v1/fields/{field_id}</code> endpoint to update the field title and markdown settings (#3421).</li> <li>Added <code>PATCH /api/v1/datasets/{dataset_id}</code> endpoint to update dataset name and guidelines (#3402).</li> <li>Added <code>PATCH /api/v1/questions/{question_id}</code> endpoint to update question title, description and some settings (depending on the type of question) (#3477).</li> <li>Added <code>DELETE /api/v1/records/{record_id}</code> endpoint to remove a record given its ID (#3337).</li> <li>Added <code>pull</code> method in <code>RemoteFeedbackDataset</code> (a <code>FeedbackDataset</code> pushed to Argilla) to pull all the records from it and return it as a local copy as a <code>FeedbackDataset</code> (#3465).</li> <li>Added <code>delete</code> method in <code>RemoteFeedbackDataset</code> (a <code>FeedbackDataset</code> pushed to Argilla) (#3512).</li> <li>Added <code>delete_records</code> method in <code>RemoteFeedbackDataset</code>, and <code>delete</code> method in <code>RemoteFeedbackRecord</code> to delete records from Argilla (#3526).</li> </ul>"},{"location":"community/changelog/#changed_15","title":"Changed","text":"<ul> <li>Improved efficiency of weak labeling when dataset contains vectors (#3444).</li> <li>Added <code>ArgillaDatasetMixin</code> to detach the Argilla-related functionality from the <code>FeedbackDataset</code> (#3427)</li> <li>Moved <code>FeedbackDataset</code>-related <code>pydantic.BaseModel</code> schemas to <code>argilla.client.feedback.schemas</code> instead, to be better structured and more scalable and maintainable (#3427)</li> <li>Update CLI to use database async connection (#3450).</li> <li>Limit rating questions values to the positive range [1, 10] (#3451).</li> <li>Updated <code>POST /api/users</code> endpoint to be able to provide a list of workspace names to which the user should be linked to (#3462).</li> <li>Updated Python client <code>User.create</code> method to be able to provide a list of workspace names to which the user should be linked to (#3462).</li> <li>Updated <code>GET /api/v1/me/datasets/{dataset_id}/records</code> endpoint to allow getting records matching one of the response statuses provided via query param (#3359).</li> <li>Updated <code>POST /api/v1/me/datasets/{dataset_id}/records</code> endpoint to allow searching records matching one of the response statuses provided via query param (#3359).</li> <li>Updated <code>SearchEngine.search</code> method to allow searching records matching one of the response statuses provided (#3359).</li> <li>After calling <code>FeedbackDataset.push_to_argilla</code>, the methods <code>FeedbackDataset.add_records</code> and <code>FeedbackRecord.set_suggestions</code> will automatically call Argilla with no need of calling <code>push_to_argilla</code> explicitly (#3465).</li> <li>Now calling <code>FeedbackDataset.push_to_huggingface</code> dumps the <code>responses</code> as a <code>List[Dict[str, Any]]</code> instead of <code>Sequence</code> to make it more readable via \ud83e\udd17<code>datasets</code> (#3539).</li> </ul>"},{"location":"community/changelog/#fixed_22","title":"Fixed","text":"<ul> <li>Fixed issue with <code>bool</code> values and <code>default</code> from Jinja2 while generating the HuggingFace <code>DatasetCard</code> from <code>argilla_template.md</code> (#3499).</li> <li>Fixed <code>DatasetConfig.from_yaml</code> which was failing when calling <code>FeedbackDataset.from_huggingface</code> as the UUIDs cannot be deserialized automatically by <code>PyYAML</code>, so UUIDs are neither dumped nor loaded anymore (#3502).</li> <li>Fixed an issue that didn't allow the Argilla server to work behind a proxy (#3543).</li> <li><code>TextClassificationSettings</code> and <code>TokenClassificationSettings</code> labels are properly parsed to strings both in the Python client and in the backend endpoint (#3495).</li> <li>Fixed <code>PUT /api/v1/datasets/{dataset_id}/publish</code> to check whether at least one field and question has <code>required=True</code> (#3511).</li> <li>Fixed <code>FeedbackDataset.from_huggingface</code> as <code>suggestions</code> were being lost when there were no <code>responses</code> (#3539).</li> <li>Fixed <code>QuestionSchema</code> and <code>FieldSchema</code> not validating <code>name</code> attribute (#3550).</li> </ul>"},{"location":"community/changelog/#deprecated_3","title":"Deprecated","text":"<ul> <li>After calling <code>FeedbackDataset.push_to_argilla</code>, calling <code>push_to_argilla</code> again won't do anything since the dataset is already pushed to Argilla (#3465).</li> <li>After calling <code>FeedbackDataset.push_to_argilla</code>, calling <code>fetch_records</code> won't do anything since the records are lazily fetched from Argilla (#3465).</li> <li>After calling <code>FeedbackDataset.push_to_argilla</code>, the Argilla ID is no longer stored in the attribute/property <code>argilla_id</code> but in <code>id</code> instead (#3465).</li> </ul>"},{"location":"community/changelog/#1133","title":"1.13.3","text":""},{"location":"community/changelog/#fixed_23","title":"Fixed","text":"<ul> <li>Fixed <code>ModuleNotFoundError</code> caused because the <code>argilla.utils.telemetry</code> module used in the <code>ArgillaTrainer</code> was importing an optional dependency not installed by default (#3471).</li> <li>Fixed <code>ImportError</code> caused because the <code>argilla.client.feedback.config</code> module was importing <code>pyyaml</code> optional dependency not installed by default (#3471).</li> </ul>"},{"location":"community/changelog/#1132","title":"1.13.2","text":""},{"location":"community/changelog/#fixed_24","title":"Fixed","text":"<ul> <li>The <code>suggestion_type_enum</code> ENUM data type created in PostgreSQL didn't have any value (#3445).</li> </ul>"},{"location":"community/changelog/#1131","title":"1.13.1","text":""},{"location":"community/changelog/#fixed_25","title":"Fixed","text":"<ul> <li>Fix database migration for PostgreSQL (See #3438)</li> </ul>"},{"location":"community/changelog/#1130","title":"1.13.0","text":""},{"location":"community/changelog/#added_19","title":"Added","text":"<ul> <li>Added <code>GET /api/v1/users/{user_id}/workspaces</code> endpoint to list the workspaces to which a user belongs (#3308 and #3343).</li> <li>Added <code>HuggingFaceDatasetMixin</code> for internal usage, to detach the <code>FeedbackDataset</code> integrations from the class itself, and use Mixins instead (#3326).</li> <li>Added <code>GET /api/v1/records/{record_id}/suggestions</code> API endpoint to get the list of suggestions for the responses associated to a record (#3304).</li> <li>Added <code>POST /api/v1/records/{record_id}/suggestions</code> API endpoint to create a suggestion for a response associated to a record (#3304).</li> <li>Added support for <code>RankingQuestionStrategy</code>, <code>RankingQuestionUnification</code> and the <code>.for_text_classification</code> method for the <code>TrainingTaskMapping</code> (#3364)</li> <li>Added <code>PUT /api/v1/records/{record_id}/suggestions</code> API endpoint to create or update a suggestion for a response associated to a record (#3304 &amp; 3391).</li> <li>Added <code>suggestions</code> attribute to <code>FeedbackRecord</code>, and allow adding and retrieving suggestions from the Python client (#3370)</li> <li>Added <code>allowed_for_roles</code> Python decorator to check whether the current user has the required role to access the decorated function/method for <code>User</code> and <code>Workspace</code> (#3383)</li> <li>Added API and Python Client support for workspace deletion (Closes #3260)</li> <li>Added <code>GET /api/v1/me/workspaces</code> endpoint to list the workspaces of the current active user (#3390)</li> </ul>"},{"location":"community/changelog/#changed_16","title":"Changed","text":"<ul> <li>Updated output payload for <code>GET /api/v1/datasets/{dataset_id}/records</code>, <code>GET /api/v1/me/datasets/{dataset_id}/records</code>, <code>POST /api/v1/me/datasets/{dataset_id}/records/search</code> endpoints to include the suggestions of the records based on the value of the <code>include</code> query parameter (#3304).</li> <li>Updated <code>POST /api/v1/datasets/{dataset_id}/records</code> input payload to add suggestions (#3304).</li> <li>The <code>POST /api/datasets/:dataset-id/:task/bulk</code> endpoints don't create the dataset if does not exists (Closes #3244)</li> <li>Added Telemetry support for <code>ArgillaTrainer</code> (closes #3325)</li> <li><code>User.workspaces</code> is no longer an attribute but a property, and is calling <code>list_user_workspaces</code> to list all the workspace names for a given user ID (#3334)</li> <li>Renamed <code>FeedbackDatasetConfig</code> to <code>DatasetConfig</code> and export/import from YAML as default instead of JSON (just used internally on <code>push_to_huggingface</code> and <code>from_huggingface</code> methods of <code>FeedbackDataset</code>) (#3326).</li> <li>The protected metadata fields support other than textual info - existing datasets must be reindex. See docs for more detail (Closes #3332).</li> <li>Updated <code>Dockerfile</code> parent image from <code>python:3.9.16-slim</code> to <code>python:3.10.12-slim</code> (#3425).</li> <li>Updated <code>quickstart.Dockerfile</code> parent image from <code>elasticsearch:8.5.3</code> to <code>argilla/argilla-server:${ARGILLA_VERSION}</code> (#3425).</li> </ul>"},{"location":"community/changelog/#removed_5","title":"Removed","text":"<ul> <li>Removed support to non-prefixed environment variables. All valid env vars start with <code>ARGILLA_</code> (See #3392).</li> </ul>"},{"location":"community/changelog/#fixed_26","title":"Fixed","text":"<ul> <li>Fixed <code>GET /api/v1/me/datasets/{dataset_id}/records</code> endpoint returning always the responses for the records even if <code>responses</code> was not provided via the <code>include</code> query parameter (#3304).</li> <li>Values for protected metadata fields are not truncated (Closes #3331).</li> <li>Big number ids are properly rendered in UI (Closes #3265)</li> <li>Fixed <code>ArgillaDatasetCard</code> to include the values/labels for all the existing questions (#3366)</li> </ul>"},{"location":"community/changelog/#deprecated_4","title":"Deprecated","text":"<ul> <li>Integer support for record id in text classification, token classification and text2text datasets.</li> </ul>"},{"location":"community/changelog/#1121","title":"1.12.1","text":""},{"location":"community/changelog/#fixed_27","title":"Fixed","text":"<ul> <li>Using <code>rg.init</code> with default <code>argilla</code> user skips setting the default workspace if not available. (Closes #3340)</li> <li>Resolved wrong import structure for <code>ArgillaTrainer</code> and <code>TrainingTaskMapping</code> (Closes #3345)</li> <li>Pin pydantic dependency to version &lt; 2 (Closes 3348)</li> </ul>"},{"location":"community/changelog/#1120","title":"1.12.0","text":""},{"location":"community/changelog/#added_20","title":"Added","text":"<ul> <li>Added <code>RankingQuestionSettings</code> class allowing to create ranking questions in the API using <code>POST /api/v1/datasets/{dataset_id}/questions</code> endpoint (#3232)</li> <li>Added <code>RankingQuestion</code> in the Python client to create ranking questions (#3275).</li> <li>Added <code>Ranking</code> component in feedback task question form (#3177 &amp; #3246).</li> <li>Added <code>FeedbackDataset.prepare_for_training</code> method for generaring a framework-specific dataset with the responses provided for <code>RatingQuestion</code>, <code>LabelQuestion</code> and <code>MultiLabelQuestion</code> (#3151).</li> <li>Added <code>ArgillaSpaCyTransformersTrainer</code> class for supporting the training with <code>spacy-transformers</code> (#3256).</li> </ul>"},{"location":"community/changelog/#docs","title":"Docs","text":"<ul> <li>Added instructions for how to run the Argilla frontend in the developer docs (#3314).</li> </ul>"},{"location":"community/changelog/#changed_17","title":"Changed","text":"<ul> <li>All docker related files have been moved into the <code>docker</code> folder (#3053).</li> <li><code>release.Dockerfile</code> have been renamed to <code>Dockerfile</code> (#3133).</li> <li>Updated <code>rg.load</code> function to raise a <code>ValueError</code> with a explanatory message for the cases in which the user tries to use the function to load a <code>FeedbackDataset</code> (#3289).</li> <li>Updated <code>ArgillaSpaCyTrainer</code> to allow re-using <code>tok2vec</code> (#3256).</li> </ul>"},{"location":"community/changelog/#fixed_28","title":"Fixed","text":"<ul> <li>Check available workspaces on Argilla on <code>rg.set_workspace</code> (Closes #3262)</li> </ul>"},{"location":"community/changelog/#1110","title":"1.11.0","text":""},{"location":"community/changelog/#fixed_29","title":"Fixed","text":"<ul> <li>Replaced <code>np.float</code> alias by <code>float</code> to avoid <code>AttributeError</code> when using <code>find_label_errors</code> function with <code>numpy&gt;=1.24.0</code> (#3214).</li> <li>Fixed <code>format_as(\"datasets\")</code> when no responses or optional respones in <code>FeedbackRecord</code>, to set their value to what \ud83e\udd17 Datasets expects instead of just <code>None</code> (#3224).</li> <li>Fixed <code>push_to_huggingface()</code> when <code>generate_card=True</code> (default behaviour), as we were passing a sample record to the <code>ArgillaDatasetCard</code> class, and <code>UUID</code>s introduced in 1.10.0 (#3192), are not JSON-serializable (#3231).</li> <li>Fixed <code>from_argilla</code> and <code>push_to_argilla</code> to ensure consistency on both field and question re-construction, and to ensure <code>UUID</code>s are properly serialized as <code>str</code>, respectively (#3234).</li> <li>Refactored usage of <code>import argilla as rg</code> to clarify package navigation (#3279).</li> </ul>"},{"location":"community/changelog/#docs_1","title":"Docs","text":"<ul> <li>Fixed URLs in Weak Supervision with Sentence Tranformers tutorial #3243.</li> <li>Fixed library buttons' formatting on Tutorials page (#3255).</li> <li>Modified styling of error code outputs in notebooks (#3270).</li> <li>Added ElasticSearch and OpenSearch versions (#3280).</li> <li>Removed template notebook from table of contents (#3271).</li> <li>Fixed tutorials with <code>pip install argilla</code> to not use older versions of the package (#3282).</li> </ul>"},{"location":"community/changelog/#added_21","title":"Added","text":"<ul> <li>Added <code>metadata</code> attribute to the <code>Record</code> of the <code>FeedbackDataset</code> (#3194)</li> <li>New <code>users update</code> command to update the role for an existing user (#3188)</li> <li>New <code>Workspace</code> class to allow users manage their Argilla workspaces and the users assigned to those workspaces via the Python client (#3180)</li> <li>Added <code>User</code> class to let users manage their Argilla users via the Python client (#3169).</li> <li>Added an option to display <code>tqdm</code> progress bar to <code>FeedbackDataset.push_to_argilla</code> when looping over the records to upload (#3233).</li> </ul>"},{"location":"community/changelog/#changed_18","title":"Changed","text":"<ul> <li>The role system now support three different roles <code>owner</code>, <code>admin</code> and <code>annotator</code> (#3104)</li> <li><code>admin</code> role is scoped to workspace-level operations (#3115)</li> <li>The <code>owner</code> user is created among the default pool of users in the quickstart, and the default user in the server has now <code>owner</code> role (#3248), reverting (#3188).</li> </ul>"},{"location":"community/changelog/#deprecated_5","title":"Deprecated","text":"<ul> <li>As of Python 3.7 end-of-life (EOL) on 2023-06-27, Argilla will no longer support Python 3.7 (#3188). More information at https://peps.python.org/pep-0537/</li> </ul>"},{"location":"community/changelog/#1100","title":"1.10.0","text":""},{"location":"community/changelog/#added_22","title":"Added","text":"<ul> <li>Added search component for feedback datasets (#3138)</li> <li>Added markdown support for feedback dataset guidelines (#3153)</li> <li>Added Train button for feedback datasets (#3170)</li> </ul>"},{"location":"community/changelog/#changed_19","title":"Changed","text":"<ul> <li>Updated <code>SearchEngine</code> and <code>POST /api/v1/me/datasets/{dataset_id}/records/search</code> to return the <code>total</code> number of records matching the search query (#3166)</li> </ul>"},{"location":"community/changelog/#fixed_30","title":"Fixed","text":"<ul> <li>Replaced Enum for string value in URLs for client API calls (Closes #3149)</li> <li>Resolve breaking issue with <code>ArgillaSpanMarkerTrainer</code> for Named Entity Recognition with <code>span_marker</code> v1.1.x onwards.</li> <li>Move <code>ArgillaDatasetCard</code> import under <code>@requires_version</code> decorator, so that the <code>ImportError</code> on <code>huggingface_hub</code> is handled properly (#3174)</li> <li>Allow flow <code>FeedbackDataset.from_argilla</code> -&gt; <code>FeedbackDataset.push_to_argilla</code> under different dataset names and/or workspaces (#3192)</li> </ul>"},{"location":"community/changelog/#docs_2","title":"Docs","text":"<ul> <li>Resolved typos in the docs (#3240).</li> <li>Fixed mention of master branch (#3254).</li> </ul>"},{"location":"community/changelog/#190","title":"1.9.0","text":""},{"location":"community/changelog/#added_23","title":"Added","text":"<ul> <li>Added boolean <code>use_markdown</code> property to <code>TextFieldSettings</code> model.</li> <li>Added boolean <code>use_markdown</code> property to <code>TextQuestionSettings</code> model.</li> <li>Added new status <code>draft</code> for the <code>Response</code> model.</li> <li>Added <code>LabelSelectionQuestionSettings</code> class allowing to create label selection (single-choice) questions in the API (#3005)</li> <li>Added <code>MultiLabelSelectionQuestionSettings</code> class allowing to create multi-label selection (multi-choice) questions in the API (#3010).</li> <li>Added <code>POST /api/v1/me/datasets/{dataset_id}/records/search</code> endpoint (#3068).</li> <li>Added new components in feedback task Question form: MultiLabel (#3064) and SingleLabel (#3016).</li> <li>Added docstrings to the <code>pydantic.BaseModel</code>s defined at <code>argilla/client/feedback/schemas.py</code> (#3137)</li> <li>Added the information about executing tests in the developer documentation ([#3143]).</li> </ul>"},{"location":"community/changelog/#changed_20","title":"Changed","text":"<ul> <li>Updated <code>GET /api/v1/me/datasets/:dataset_id/metrics</code> output payload to include the count of responses with <code>draft</code> status.</li> <li>Added <code>LabelSelectionQuestionSettings</code> class allowing to create label selection (single-choice) questions in the API.</li> <li>Added <code>MultiLabelSelectionQuestionSettings</code> class allowing to create multi-label selection (multi-choice) questions in the API.</li> <li>Database setup for unit tests. Now the unit tests use a different database than the one used by the local Argilla server (Closes #2987).</li> <li>Updated <code>alembic</code> setup to be able to autogenerate revision/migration scripts using SQLAlchemy metadata from Argilla server models (#3044)</li> <li>Improved <code>DatasetCard</code> generation on <code>FeedbackDataset.push_to_huggingface</code> when <code>generate_card=True</code>, following the official HuggingFace Hub template, but suited to <code>FeedbackDataset</code>s from Argilla (#3110)</li> </ul>"},{"location":"community/changelog/#fixed_31","title":"Fixed","text":"<ul> <li>Disallow <code>fields</code> and <code>questions</code> in <code>FeedbackDataset</code> with the same name (#3126).</li> <li>Fixed broken links in the documentation and updated the development branch name from <code>development</code> to <code>develop</code> ([#3145]).</li> </ul>"},{"location":"community/changelog/#180","title":"1.8.0","text":""},{"location":"community/changelog/#added_24","title":"Added","text":"<ul> <li><code>/api/v1/datasets</code> new endpoint to list and create datasets (#2615).</li> <li><code>/api/v1/datasets/{dataset_id}</code> new endpoint to get and delete datasets (#2615).</li> <li><code>/api/v1/datasets/{dataset_id}/publish</code> new endpoint to publish a dataset (#2615).</li> <li><code>/api/v1/datasets/{dataset_id}/questions</code> new endpoint to list and create dataset questions (#2615)</li> <li><code>/api/v1/datasets/{dataset_id}/fields</code> new endpoint to list and create dataset fields (#2615)</li> <li><code>/api/v1/datasets/{dataset_id}/questions/{question_id}</code> new endpoint to delete a dataset questions (#2615)</li> <li><code>/api/v1/datasets/{dataset_id}/fields/{field_id}</code> new endpoint to delete a dataset field (#2615)</li> <li><code>/api/v1/workspaces/{workspace_id}</code> new endpoint to get workspaces by id (#2615)</li> <li><code>/api/v1/responses/{response_id}</code> new endpoint to update and delete a response (#2615)</li> <li><code>/api/v1/datasets/{dataset_id}/records</code> new endpoint to create and list dataset records (#2615)</li> <li><code>/api/v1/me/datasets</code> new endpoint to list user visible datasets (#2615)</li> <li><code>/api/v1/me/dataset/{dataset_id}/records</code> new endpoint to list dataset records with user responses (#2615)</li> <li><code>/api/v1/me/datasets/{dataset_id}/metrics</code> new endpoint to get the dataset user metrics (#2615)</li> <li><code>/api/v1/me/records/{record_id}/responses</code> new endpoint to create record user responses (#2615)</li> <li>showing new feedback task datasets in datasets list ([#2719])</li> <li>new page for feedback task ([#2680])</li> <li>show feedback task metrics ([#2822])</li> <li>user can delete dataset in dataset settings page ([#2792])</li> <li>Support for <code>FeedbackDataset</code> in Python client (parent PR #2615, and nested PRs: [#2949], [#2827], [#2943], [#2945], [#2962], and [#3003])</li> <li>Integration with the HuggingFace Hub ([#2949])</li> <li>Added <code>ArgillaPeftTrainer</code> for text and token classificaiton #2854</li> <li>Added <code>predict_proba()</code> method to <code>ArgillaSetFitTrainer</code></li> <li>Added <code>ArgillaAutoTrainTrainer</code> for Text Classification #2664</li> <li>New <code>database revisions</code> command showing database revisions info</li> </ul>"},{"location":"community/changelog/#fixes","title":"Fixes","text":"<ul> <li>Avoid rendering html for invalid html strings in Text2text ([#2911]https://github.com/argilla-io/argilla/issues/2911)</li> </ul>"},{"location":"community/changelog/#changed_21","title":"Changed","text":"<ul> <li>The <code>database migrate</code> command accepts a <code>--revision</code> param to provide specific revision id</li> <li><code>tokens_length</code> metrics function returns empty data (#3045)</li> <li><code>token_length</code> metrics function returns empty data (#3045)</li> <li><code>mention_length</code> metrics function returns empty data (#3045)</li> <li><code>entity_density</code> metrics function returns empty data (#3045)</li> </ul>"},{"location":"community/changelog/#deprecated_6","title":"Deprecated","text":"<ul> <li>Using Argilla with Python 3.7 runtime is deprecated and support will be removed from version 1.11.0 (#2902)</li> <li><code>tokens_length</code> metrics function has been deprecated and will be removed in 1.10.0 (#3045)</li> <li><code>token_length</code> metrics function has been deprecated and will be removed in 1.10.0 (#3045)</li> <li><code>mention_length</code> metrics function has been deprecated and will be removed in 1.10.0 (#3045)</li> <li><code>entity_density</code> metrics function has been deprecated and will be removed in 1.10.0 (#3045)</li> </ul>"},{"location":"community/changelog/#removed_6","title":"Removed","text":"<ul> <li>Removed mention <code>density</code>, <code>tokens_length</code> and <code>chars_length</code> metrics from token classification metrics storage (#3045)</li> <li>Removed token <code>char_start</code>, <code>char_end</code>, <code>tag</code>, and <code>score</code> metrics from token classification metrics storage (#3045)</li> <li>Removed tags-related metrics from token classification metrics storage (#3045)</li> </ul>"},{"location":"community/changelog/#170","title":"1.7.0","text":""},{"location":"community/changelog/#added_25","title":"Added","text":"<ul> <li>add <code>max_retries</code> and <code>num_threads</code> parameters to <code>rg.log</code> to run data logging request concurrently with backoff retry policy. See #2458 and #2533</li> <li><code>rg.load</code> accepts <code>include_vectors</code> and <code>include_metrics</code> when loading data. Closes #2398</li> <li>Added <code>settings</code> param to <code>prepare_for_training</code> (#2689)</li> <li>Added <code>prepare_for_training</code> for <code>openai</code> (#2658)</li> <li>Added <code>ArgillaOpenAITrainer</code> (#2659)</li> <li>Added <code>ArgillaSpanMarkerTrainer</code> for Named Entity Recognition (#2693)</li> <li>Added <code>ArgillaTrainer</code> CLI support. Closes (#2809)</li> </ul>"},{"location":"community/changelog/#fixes_1","title":"Fixes","text":"<ul> <li>fix image alignment on token classification</li> </ul>"},{"location":"community/changelog/#changed_22","title":"Changed","text":"<ul> <li>Argilla quickstart image dependencies are externalized into <code>quickstart.requirements.txt</code>. See #2666</li> <li>bulk endpoints will upsert data when record <code>id</code> is present. Closes #2535</li> <li>moved from <code>click</code> to <code>typer</code> CLI support. Closes (#2815)</li> <li>Argilla server docker image is built with PostgreSQL support. Closes #2686</li> <li>The <code>rg.log</code> computes all batches and raise an error for all failed batches.</li> <li>The default batch size for <code>rg.log</code> is now 100.</li> </ul>"},{"location":"community/changelog/#fixed_32","title":"Fixed","text":"<ul> <li><code>argilla.training</code> bugfixes and unification (#2665)</li> <li>Resolved several small bugs in the <code>ArgillaTrainer</code>.</li> </ul>"},{"location":"community/changelog/#deprecated_7","title":"Deprecated","text":"<ul> <li>The <code>rg.log_async</code> function is deprecated and will be removed in next minor release.</li> </ul>"},{"location":"community/changelog/#160","title":"1.6.0","text":""},{"location":"community/changelog/#added_26","title":"Added","text":"<ul> <li><code>ARGILLA_HOME_PATH</code> new environment variable (#2564).</li> <li><code>ARGILLA_DATABASE_URL</code> new environment variable (#2564).</li> <li>Basic support for user roles with <code>admin</code> and <code>annotator</code> (#2564).</li> <li><code>id</code>, <code>first_name</code>, <code>last_name</code>, <code>role</code>, <code>inserted_at</code> and <code>updated_at</code> new user fields (#2564).</li> <li><code>/api/users</code> new endpoint to list and create users (#2564).</li> <li><code>/api/users/{user_id}</code> new endpoint to delete users (#2564).</li> <li><code>/api/workspaces</code> new endpoint to list and create workspaces (#2564).</li> <li><code>/api/workspaces/{workspace_id}/users</code> new endpoint to list workspace users (#2564).</li> <li><code>/api/workspaces/{workspace_id}/users/{user_id}</code> new endpoint to create and delete workspace users (#2564).</li> <li><code>argilla.tasks.users.migrate</code> new task to migrate users from old YAML file to database (#2564).</li> <li><code>argilla.tasks.users.create</code> new task to create a user (#2564).</li> <li><code>argilla.tasks.users.create_default</code> new task to create a user with default credentials (#2564).</li> <li><code>argilla.tasks.database.migrate</code> new task to execute database migrations (#2564).</li> <li><code>release.Dockerfile</code> and <code>quickstart.Dockerfile</code> now creates a default <code>argilladata</code> volume to persist data (#2564).</li> <li>Add user settings page. Closes #2496</li> <li>Added <code>Argilla.training</code> module with support for <code>spacy</code>, <code>setfit</code>, and <code>transformers</code>. Closes #2504</li> </ul>"},{"location":"community/changelog/#fixes_2","title":"Fixes","text":"<ul> <li>Now the <code>prepare_for_training</code> method is working when <code>multi_label=True</code>. Closes #2606</li> </ul>"},{"location":"community/changelog/#changed_23","title":"Changed","text":"<ul> <li><code>ARGILLA_USERS_DB_FILE</code> environment variable now it's only used to migrate users from YAML file to database (#2564).</li> <li><code>full_name</code> user field is now deprecated and <code>first_name</code> and <code>last_name</code> should be used instead (#2564).</li> <li><code>password</code> user field now requires a minimum of <code>8</code> and a maximum of <code>100</code> characters in size (#2564).</li> <li><code>quickstart.Dockerfile</code> image default users from <code>team</code> and <code>argilla</code> to <code>admin</code> and <code>annotator</code> including new passwords and API keys (#2564).</li> <li>Datasets to be managed only by users with <code>admin</code> role (#2564).</li> <li>The list of rules is now accessible while metrics are computed. Closes#2117</li> <li>Style updates for weak labeling and adding feedback toast when delete rules. See #2626 and #2648</li> </ul>"},{"location":"community/changelog/#removed_7","title":"Removed","text":"<ul> <li><code>email</code> user field (#2564).</li> <li><code>disabled</code> user field (#2564).</li> <li>Support for private workspaces (#2564).</li> <li><code>ARGILLA_LOCAL_AUTH_DEFAULT_APIKEY</code> and <code>ARGILLA_LOCAL_AUTH_DEFAULT_PASSWORD</code> environment variables. Use <code>python -m argilla.tasks.users.create_default</code> instead (#2564).</li> <li>The old headers for <code>API Key</code> and <code>workspace</code> from python client</li> <li>The default value for old <code>API Key</code> constant. Closes #2251</li> </ul>"},{"location":"community/changelog/#151-2023-03-30","title":"1.5.1 - 2023-03-30","text":""},{"location":"community/changelog/#fixes_3","title":"Fixes","text":"<ul> <li>Copying datasets between workspaces with proper owner/workspace info. Closes #2562</li> <li>Copy dataset with empty workspace to the default user workspace 905d4de</li> <li>Using elasticsearch config to request backend version. Closes #2311</li> <li>Remove sorting by score in labels. Closes #2622</li> </ul>"},{"location":"community/changelog/#changed_24","title":"Changed","text":"<ul> <li>Update field name in metadata for image url. See #2609</li> <li>Improvements in tutorial doc cards. Closes #2216</li> </ul>"},{"location":"community/changelog/#150-2023-03-21","title":"1.5.0 - 2023-03-21","text":""},{"location":"community/changelog/#added_27","title":"Added","text":"<ul> <li>Add the fields to retrieve when loading the data from argilla. <code>rg.load</code> takes too long because of the vector field, even when users don't need it. Closes #2398</li> <li>Add new page and components for dataset settings. Closes #2442</li> <li>Add ability to show image in records (for TokenClassification and TextClassification) if an URL is passed in metadata with the key _image_url</li> <li>Non-searchable fields support in metadata. #2570</li> <li>Add record ID references to the prepare for training methods. Closes #2483</li> <li>Add tutorial on Image Classification. #2420</li> <li>Add Train button, visible for \"admin\" role, with code snippets from a selection of libraries. Closes [#2591] (https://github.com/argilla-io/argilla/pull/2591)</li> </ul>"},{"location":"community/changelog/#changed_25","title":"Changed","text":"<ul> <li>Labels are now centralized in a specific vuex ORM called GlobalLabel Model, see https://github.com/argilla-io/argilla/issues/2210. This model is the same for TokenClassification and TextClassification (so both task have labels with color_id and shortcuts parameters in the vuex ORM)</li> <li>The shortcuts improvement for labels #2339 have been moved to the vuex ORM in dataset settings feature #2444</li> <li>Update \"Define a labeling schema\" section in docs.</li> <li>The record inputs are sorted alphabetically in UI by default. #2581</li> <li>The record inputs are fully visible when pagination size is one and the height of collapsed area size is bigger for laptop screen. #2587</li> </ul>"},{"location":"community/changelog/#fixes_4","title":"Fixes","text":"<ul> <li>Allow URL to be clickable in Jupyter notebook again. Closes #2527</li> </ul>"},{"location":"community/changelog/#removed_8","title":"Removed","text":"<ul> <li>Removing some data scan deprecated endpoints used by old clients. This change will break compatibility with client <code>&lt;v1.3.0</code></li> <li>Stop using old scan deprecated endpoints in python client. This logic will break client compatibility with server version <code>&lt;1.3.0</code></li> <li>Remove the previous way to add labels through the dataset page. Now labels can be added only through dataset settings page.</li> </ul>"},{"location":"community/contributor/","title":"How to contribute?","text":"<p>Thank you for investing your time in contributing to the project! Any contribution you make will be reflected in the most recent version of Argilla \ud83e\udd29.</p> New to contributing in general? <p>If you're a new contributor, read the README to get an overview of the project. In addition, here are some resources to help you get started with open-source contributions:</p> <ul> <li>Discord: You are welcome to join the Argilla Discord community, where you can keep in touch with other users, contributors and the Argilla team. In the following section, you can find more information on how to get started in Discord.</li> <li>Git: This is a very useful tool to keep track of the changes in your files. Using the command-line interface (CLI), you can make your contributions easily. For that, you need to have it installed and updated on your computer.</li> <li>GitHub: It is a platform and cloud-based service that uses git and allows developers to collaborate on projects. To contribute to Argilla, you'll need to create an account. Check the Contributor Workflow with Git and Github for more info.</li> <li>Developer Documentation: To collaborate, you'll need to set up an efficient environment. Check the developer documentation to know how to do it.</li> <li>Schedule a meeting with our developer advocate: If you have more questions, do not hesitate to contact our developer advocate and schedule a meeting.</li> </ul>"},{"location":"community/contributor/#first-contact-in-discord","title":"First Contact in Discord","text":"<p>Discord is a handy tool for more casual conversations and to answer day-to-day questions. As part of Hugging Face, we have set up some Argilla channels on the server. Click here to join the Hugging Face Discord community effortlessly.</p> <p>When part of the Hugging Face Discord, you can select \"Channels &amp; roles\" and select \"Argilla\" along with any of the other groups that are interesting to you. \"Argilla\" will cover anything about argilla and distilabel. You can join the following channels:</p> <ul> <li>#argilla-distilabel-general: \ud83d\udce3 Stay up-to-date and general discussions.</li> <li>#argilla-distilabel-help: \ud83d\ude4b\u200d\u2640\ufe0f Need assistance? We're always here to help. Select the appropriate label (argilla or distilabel) for your issue and post it.</li> </ul> <p>So now there is only one thing left to do: introduce yourself and talk to the community. You'll always be welcome! \ud83e\udd17\ud83d\udc4b</p>"},{"location":"community/contributor/#contributor-workflow-with-git-and-github","title":"Contributor Workflow with Git and GitHub","text":"<p>If you're working with Argilla and suddenly a new idea comes to your mind or you find an issue that can be improved, it's time to actively participate and contribute to the project!</p>"},{"location":"community/contributor/#report-an-issue","title":"Report an issue","text":"<p>If you spot a problem, search if an issue already exists. You can use the <code>Label</code> filter. If that is the case, participate in the conversation. If it does not exist, create an issue by clicking on <code>New Issue</code>.</p> <p></p> <p>This will show various templates, choose the one that best suits your issue.</p> <p></p> <p>Below, you can see an example of the <code>Feature request</code> template. Once you choose one, you will need to fill in it following the guidelines. Try to be as clear as possible. In addition, you can assign yourself to the issue and add or choose the right labels. Finally, click on <code>Submit new issue</code>.</p> <p></p>"},{"location":"community/contributor/#work-with-a-fork","title":"Work with a fork","text":""},{"location":"community/contributor/#fork-the-argilla-repository","title":"Fork the Argilla repository","text":"<p>After having reported the issue, you can start working on it. For that, you will need to create a fork of the project. To do that, click on the <code>Fork</code> button.</p> <p></p> <p>Now, fill in the information. Remember to uncheck the <code>Copy develop branch only</code> if you are going to work in or from another branch (for instance, to fix documentation the <code>main</code> branch is used). Then, click on <code>Create fork</code>.</p> <p></p> <p>Now, you will be redirected to your fork. You can see that you are in your fork because the name of the repository will be your <code>username/argilla</code>, and it will indicate <code>forked from argilla-io/argilla</code>.</p>"},{"location":"community/contributor/#clone-your-forked-repository","title":"Clone your forked repository","text":"<p>In order to make the required adjustments, clone the forked repository to your local machine. Choose the destination folder and run the following command:</p> <pre><code>git clone https://github.com/[your-github-username]/argilla.git\ncd argilla\n</code></pre> <p>To keep your fork\u2019s main/develop branch up to date with our repo, add it as an upstream remote branch. For more info, check the documentation.</p> <pre><code>git remote add upstream https://github.com/argilla-io/argilla.git\n</code></pre>"},{"location":"community/contributor/#create-a-new-branch","title":"Create a new branch","text":"<p>For each issue you're addressing, it's advisable to create a new branch. GitHub offers a straightforward method to streamline this process.</p> <p>\u26a0\ufe0f Never work directly on the <code>main</code> or <code>develop</code> branch. Always create a new branch for your changes.</p> <p>Navigate to your issue and on the right column, select <code>Create a branch</code>.</p> <p></p> <p>After the new window pops up, the branch will be named after the issue, include a prefix such as feature/, bug/, or docs/ to facilitate quick recognition of the issue type. In the <code>Repository destination</code>, pick your fork ( [your-github-username]/argilla), and then select <code>Change branch source</code> to specify the source branch for creating the new one. Complete the process by clicking <code>Create branch</code>.</p> <p>\ud83e\udd14 Remember that the <code>main</code> branch is only used to work with the documentation. For any other changes, use the <code>develop</code> branch.</p> <p></p> <p>Now, locally change to the new branch you just created.</p> <pre><code>git fetch origin\ngit checkout [branch-name]\n</code></pre>"},{"location":"community/contributor/#use-changelogmd","title":"Use CHANGELOG.md","text":"<p>If you are working on a new feature, it is a good practice to make note of it for others to keep up with the changes. For that, we utilize the <code>CHANGELOG.md</code> file in the root directory. This file is used to list changes made in each version of the project and there are headers that we use to denote each type of change.</p> <ul> <li>Added: for new features.</li> <li>Changed: for changes in existing functionality.</li> <li>Deprecated: for soon-to-be removed features.</li> <li>Removed: for now removed features.</li> <li>Fixed: for any bug fixes.</li> <li>Security: in case of vulnerabilities.</li> </ul> <p>A sample addition would be:</p> <pre><code>- Fixed the key errors for the `init` method ([#NUMBER_OF_PR](LINK_TO_PR)). Contributed by @github_handle.\n</code></pre> <p>You can have a look at the CHANGELOG.md file to see more cases and examples.</p>"},{"location":"community/contributor/#make-changes-and-push-them","title":"Make changes and push them","text":"<p>Make the changes you want in your local repository, and test that everything works and you are following the guidelines. Check the documentation for more information about the development.</p> <p>Once you have finished, you can check the status of your repository and synchronize with the upstreaming repo with the following command:</p> <pre><code># Check the status of your repository\ngit status\n\n# Synchronize with the upstreaming repo\ngit checkout [branch-name]\ngit rebase [default-branch]\n</code></pre> <p>If everything is right, we need to commit and push the changes to your fork. For that, run the following commands:</p> <pre><code># Add the changes to the staging area\ngit add filename\n\n# Commit the changes by writing a proper message\ngit commit -m \"commit-message\"\n\n# Push the changes to your fork\ngit push origin [branch-name]\n</code></pre> <p>When pushing, you will be asked to enter your GitHub login credentials. Once the push is complete, all local commits will be on your GitHub repository.</p>"},{"location":"community/contributor/#create-a-pull-request","title":"Create a pull request","text":"<p>Come back to GitHub, navigate to the original repository where you created your fork, and click on <code>Compare &amp; pull request</code>.</p> <p></p> <p>First, click on <code>compare across forks</code> and select the right repositories and branches.</p> <p>In the base repository, keep in mind to select either <code>main</code> or <code>develop</code> based on the modifications made. In the head repository, indicate your forked repository and the branch corresponding to the issue.</p> <p></p> <p>Then, fill in the pull request template. You should add a prefix to the PR name as we did with the branch above. If you are working on a new feature, you can name your PR as <code>feat: TITLE</code>. If your PR consists of a solution for a bug, you can name your PR as <code>bug: TITLE</code> And, if your work is for improving the documentation, you can name your PR as <code>docs: TITLE</code>.</p> <p>In addition, on the right side, you can select a reviewer (for instance, if you discussed the issue with a member of the Argilla team) and assign the pull request to yourself. It is highly advisable to add labels to PR as well. You can do this again by the labels section right to the screen. For instance, if you are addressing a bug, add the <code>bug</code> label or if the PR is related to the documentation, add the <code>documentation</code> label. This way, PRs can be easily filtered.</p> <p>Finally, fill in the template carefully and follow the guidelines. Remember to link the original issue and enable the checkbox to allow maintainer edits so the branch can be updated for a merge. Then, click on <code>Create pull request</code>.</p>"},{"location":"community/contributor/#review-your-pull-request","title":"Review your pull request","text":"<p>Once you submit your PR, a team member will review your proposal. We may ask questions, request additional information or ask for changes to be made before a PR can be merged, either using suggested changes or pull request comments.</p> <p>You can apply the changes directly through the UI (check the files changed and click on the right-corner three dots, see image below) or from your fork, and then commit them to your branch. The PR will be updated automatically and the suggestions will appear as outdated.</p> <p></p> <p>If you run into any merge issues, check out this git tutorial to help you resolve merge conflicts and other issues.</p>"},{"location":"community/contributor/#your-pr-is-merged","title":"Your PR is merged!","text":"<p>Congratulations \ud83c\udf89\ud83c\udf8a We thank you \ud83e\udd29</p> <p>Once your PR is merged, your contributions will be publicly visible on the Argilla GitHub.</p> <p>Additionally, we will include your changes in the next release based on our development branch.</p>"},{"location":"community/contributor/#additional-resources","title":"Additional resources","text":"<p>Here are some helpful resources for your reference.</p> <ul> <li>Configuring Discord, a guide to learn how to get started with Discord.</li> <li>Pro Git, a book to learn Git.</li> <li>Git in VSCode, a guide to learn how to easily use Git in VSCode.</li> <li>GitHub Skills, an interactive course to learn GitHub.</li> </ul>"},{"location":"getting_started/faq/","title":"FAQs","text":"What is Argilla? <p>Argilla is a collaboration tool for AI engineers and domain experts that require high-quality outputs, full data ownership, and overall efficiency. It is designed to help you achieve and keep high-quality data standards, store your training data, store the results of your models, evaluate their performance, and improve the data through human and AI feedback.</p> Does Argilla cost money? <p>No. Argilla is an open-source project and is free to use. You can deploy Argilla on your own infrastructure or use our cloud offering.</p> What data types does Argilla support? <p>Text data, mostly. Argilla natively supports textual data, however, we do support rich text, which means you can represent different types of data in Argilla as long as you can convert it to text. For example, you can store images, audio, video, and any other type of data as long as you can convert it to their base64 representation or render them as HTML in for example an IFrame.</p> Does Argilla train models? <p>No. Argilla is a collaboration tool to achieve and keep high-quality data standards. You can use Argilla to store your training data, store the results of your models, evaluate their performance and improve the data. For training models, you can use any machine learning framework or library that you prefer even though we recommend starting with Hugging Face Transformers.</p> Does Argilla provide annotation workforces? <p>Yes, kind of. We don't provide annotation workforce in-house but we do have partnerships with workforce providers that ensure ethical practices and secure work environments. Feel free to schedule a meeting here or contact us via email.</p> How does Argilla differ from competitors like Lilac, Snorkel, Prodigy and Scale? <p>Argilla distinguishes itself for its focus on specific use cases and human-in-the-loop approaches. While it does offer programmatic features, Argilla\u2019s core value lies in actively involving human experts in the tool-building process, setting it apart from other competitors.</p> <p>Furthermore, Argilla places particular emphasis on smooth integration with other tools in the community, particularly within the realms of MLOps and NLP. So, its compatibility with popular frameworks like spaCy and Hugging Face makes it exceptionally user-friendly and accessible.</p> <p>Finally, platforms like Snorkel, Prodigy or Scale, while more comprehensive, often require a significant commitment. Argilla, on the other hand, works more as a tool within the MLOps ecosystem, allowing users to begin with specific use cases and then scale up as needed. This flexibility is particularly beneficial for users and customers who prefer to start small and expand their applications over time, as opposed to committing to an all-encompassing tool from the outset.</p> What is the difference between Argilla 2.0 and the legacy datasets in 1.0? <p>Argilla 1.0 relied on 3 main task datasets: <code>DatasetForTextClassification</code>, <code>DatasetForTokenClassification</code>, and <code>DatasetForText2Text</code>. These tasks were designed to be simple, easy to use and high in functionality but they were limited in adaptability. With the introduction of Large Language Models (LLMs) and the increasing complexity of NLP tasks, we realized that we needed to expand the capabilities of Argilla to support more advanced feedback mechanisms which led to the introduction of the <code>FeedbackDataset</code>. Compared to its predecessor it was high in adaptability but still limited in functionality. After having ported all of the functionality of the legacy tasks to the new <code>FeedbackDataset</code>, we decided to deprecate the legacy tasks in favor of a brand new SDK with the <code>FeedbackDataset</code> at its core.</p>"},{"location":"getting_started/how-to-configure-argilla-on-huggingface/","title":"Hugging Face Spaces Settings","text":"<p> This page is currently under construction. Please check back later for updates.</p> <p>This section details how to configure and deploy Extralit on Hugging Face Spaces. It covers:</p> <ul> <li>How to configure persistent storage and database services</li> <li>How to configure Extralit </li> <li>How to deploy Extralit under a Hugging Face Organization</li> <li>How to configure and enable HF OAuth access</li> <li>How to use Private Spaces</li> </ul> <p>Looking to get started easily?</p> <p>If you just discovered Argilla and want to get started quickly, go to the Quickstart guide.</p>"},{"location":"getting_started/how-to-configure-argilla-on-huggingface/#persistent-storage","title":"Persistent storage","text":"<p>In the Space creation UI, persistent storage is set to <code>Small PAID</code>, which is a paid service, charged per hour of usage.</p> <p>Spaces get restarted due to maintainance, inactivity, and every time you change your Spaces settings. Persistent storage enables Argilla to save to disk your datasets and configurations across restarts.</p> <p>Ephimeral FREE persistent storage</p> <p>Not setting persistent storage to <code>Small</code> means that you will loose your data when the Space restarts.</p> <p>If you plan to use the Argilla Space beyond testing, it's highly recommended to set persistent storage to <code>Small</code>.</p> <p>If you just want to quickly test or use Argilla for a few hours with the risk of loosing your datasets, choose <code>Ephemeral FREE</code>. <code>Ephemeral FREE</code> means your datasets and configuration will not be saved to disk, when the Space is restarted your datasets, workspaces, and users will be lost.</p> <p>If you want to disable the persistence storage warning, you can set the environment variable <code>ARGILLA_SHOW_HUGGINGFACE_SPACE_PERSISTENT_STORAGE_WARNING=false</code></p> <p>Read this if you have datasets and want to enable persistent storage</p> <p>If you want to enable persistent storage <code>Small PAID</code> and you have created datasets, users, or workspaces, follow this process:</p> <ul> <li>First, make a local or remote copy of your datasets, following the Import and Export guide. This is the most important step, because changing the settings of your Space leads to a restart and thus a data loss.</li> <li>If you have created users (not signed in with Hugging Face login), consider storing a copy of users following the manage users guide.</li> <li>Once you have stored all your data safely, go to you Space Settings Tab and select <code>Small</code>.</li> <li>Your Space will be restarted and existing data will be lost. From now on, all the new data you create in Argilla will be kept safely</li> <li>Recover your data, by following the above mentioned guides.</li> </ul>"},{"location":"getting_started/how-to-configure-argilla-on-huggingface/#how-to-configure-and-disable-oauth-access","title":"How to configure and disable OAuth access","text":"<p>By default, Argilla Spaces are configured with Hugging Face OAuth, in the following way:</p> <ul> <li>Any Hugging Face user that can see your Space, can use the Sign in button, join as an <code>annotator</code>, and contribute to the datasets available under the <code>argilla</code> workspace. This workspace is created during the deployment process.</li> <li>These users can only explore and annotate datasets in the <code>argilla</code> workspace but can't perform any critical operation like create, delete, update, or configure datasets. By default, any other workspace you create, won't be visible to these users.</li> </ul> <p>To restrict access or change the default behaviour, there's two options:</p> <p>Set your Space to private. This is especially useful if your Space is under an organization. This will only allow members within your organization to see and join your Argilla space. It can also be used for personal, solo projects.</p> <p>Modify the <code>.oauth.yml</code> configuration file. You can find and modify this file under the <code>Files</code> tab of your Space. The default file looks like this:</p> <p><pre><code># Change to `false` to disable HF oauth integration\n#enabled: false\n\nproviders:\n  - name: huggingface\n\n# Allowed workspaces must exists\nallowed_workspaces:\n  - name: argilla\n</code></pre> You can modify two things:</p> <ul> <li>Uncomment <code>enabled: false</code> to completely disable the Sign in with Hugging Face. If you disable it make sure to set the <code>USERNAME</code> and <code>PASSWORD</code> Space secrets to be able to login as an <code>owner</code>.</li> <li>Change the list of <code>allowed</code> workspaces.</li> </ul> <p>For example if you want to let users join a new workspace <code>community-initiative</code>:</p> <pre><code>allowed_workspaces:\n  - name: argilla\n  - name: community-initiative\n</code></pre>"},{"location":"getting_started/how-to-configure-argilla-on-huggingface/#how-to-deploy-argilla-under-a-hugging-face-organization","title":"How to deploy Argilla under a Hugging Face Organization","text":"<p>Creating an Argilla Space within an organization is useful for several scenarios:</p> <ul> <li>You want to only enable members of your organization to join your Space. You can achieve this by setting your Space to private.</li> <li>You want manage the Space together with other users (e.g., Space settings, etc.). Note that if you just want to manage your Argilla datasets, workspaces, you can achieve this by adding other Argilla <code>owner</code> roles to your Argilla Server.</li> <li>More generally, you want to make available your space under an organization/community umbrella.</li> </ul> <p>The steps are very similar the Quickstart guide with two important differences:</p> <p>Setup USERNAME</p> <p>You need to set up the <code>USERNAME</code> Space Secret with your Hugging Face username. This way, the first time you enter with the <code>Hugging Face Sign in</code> button, you'll be granted the <code>owner</code> role.</p> <p>Enable Persistent Storage <code>SMALL</code></p> <p>Not setting persistent storage to <code>Small</code> means that you will loose your data when the Space restarts.</p> <p>For Argilla Spaces with many users, it's strongly recommended to set persistent storage to <code>Small</code>.</p>"},{"location":"getting_started/how-to-configure-argilla-on-huggingface/#how-to-use-private-spaces","title":"How to use Private Spaces","text":"<p>Setting your Space visibility to private can be useful if:</p> <ul> <li>You want to work on your personal, solo project.</li> <li>You want your Argilla to be available only to members of the organization where you deploy the Argilla Space.</li> </ul> <p>You can set the visibility of the Space during the Space creation process or afterwards under the <code>Settings</code> Tab.</p> <p>To use the Python SDK with private Spaces you need to specify your <code>HF_TOKEN</code> which can be found here, when creating the client:</p> <pre><code>import argilla as rg\n\nHF_TOKEN = \"...\"\n\nclient = rg.Argilla(\n    api_url=\"&lt;api_url&gt;\",\n    api_key=\"&lt;api_key&gt;\"\n    headers={\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n)\n</code></pre>"},{"location":"getting_started/how-to-configure-argilla-on-huggingface/#space-secrets-overview","title":"Space Secrets overview","text":"<p>There's two optional secrets to set up the <code>USERNAME</code> and <code>PASSWORD</code> of the <code>owner</code> of the Argilla Space. Remember that, by default Argilla Spaces are configured with a Sign in with Hugging Face button, which is also used to grant an <code>owner</code> to the creator of the Space for personal spaces.</p> <p>The <code>USERNAME</code> and <code>PASSWORD</code> are only useful in a couple of scenarios:</p> <ul> <li>You have disabled Hugging Face OAuth.</li> <li>You want to set up Argilla under an organization and want your Hugging Face username to be granted the <code>owner</code> role.</li> </ul> <p>In summary, when setting up a Space:</p> <p>Creating a Space under your personal account</p> <p>If you are creating the Space under your personal account, don't insert any value for <code>USERNAME</code> and <code>PASSWORD</code>. Once you launch the Space you will be able to Sign in with your Hugging Face username and the <code>owner</code> role.</p> <p>Creating a Space under an organization</p> <p>If you are creating the Space under an organization make sure to insert your Hugging Face username in the secret <code>USERNAME</code>. In this way, you'll be able to Sign in with your Hugging Face user.</p>"},{"location":"getting_started/how-to-deploy-argilla-with-docker/","title":"Deploy with Docker","text":"<p>This guide describes how to deploy the Argilla Server with <code>docker compose</code>. This is useful if you want to deploy Argilla locally, and/or have full control over the configuration the server, database, and search engine (Elasticsearch).</p> <p>First, you need to install <code>docker</code> on your machine and make sure you can run <code>docker compose</code>.</p> <p>Then, create a folder (you can modify the folder name):</p> <pre><code>mkdir argilla &amp;&amp; cd argilla\n</code></pre> <p>Download <code>docker-compose.yaml</code>:</p> <pre><code>wget -O docker-compose.yaml https://raw.githubusercontent.com/extralit/extralit/main/examples/deployments/docker/docker-compose.yaml\n</code></pre> <p>or using curl: <pre><code>curl https://raw.githubusercontent.com/extralit/extralit/main/examples/deployments/docker/docker-compose.yaml -o docker-compose.yaml\n</code></pre></p> <p>Run to deploy the server on <code>http://localhost:6900</code>:</p> <pre><code>docker compose up -d\n</code></pre> <p>Once is completed, go to this URL with your browser: http://localhost:6900 and you should see the Argilla login page.</p> <p>If it's not available, check the logs:</p> <pre><code>docker compose logs -f\n</code></pre> <p>Most of the deployment issues are related to ElasticSearch. Join Hugging Face Discord's server and ask for support on the Argilla channel.</p>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#install-the-extralit-package-with-pip","title":"Install the Extralit package with pip","text":"<pre><code>pip install extralit\n</code></pre>"},{"location":"getting_started/installation/#run-the-extralit-server","title":"Run the Extralit server","text":"<p>If you have already deployed Extralit server, you can skip this step. Otherwise, you can quickly deploy it in two different ways:</p> <ul> <li> <p>Using a HF Space.</p> </li> <li> <p>Locally with Docker.</p> </li> </ul>"},{"location":"getting_started/installation/#connect-to-the-extralit-server","title":"Connect to the Extralit server","text":"<p>Get your <code>&lt;api_url&gt;</code>:</p> <ul> <li>If you are using HF Spaces, it should be constructed as follows: <code>https://[your-owner-name]-[your_space_name].hf.space</code></li> <li>If you are using Docker, it is the URL shown in your browser (by default <code>http://localhost:6900</code>)</li> </ul> <p>Get your <code>&lt;api_key&gt;</code> in <code>My Settings</code> in the Argilla UI (by default owner.apikey).</p> <p>Note</p> <p>Make sure to replace <code>&lt;api_url&gt;</code> and <code>&lt;api_key&gt;</code> with your actual values. If you are using a private HF Space, you need to specify your <code>HF_TOKEN</code> which can be found here.</p> <pre><code>import argilla as rg\n\nclient = rg.init(\n    api_url=\"&lt;api_url&gt;\",\n    api_key=\"&lt;api_key&gt;\",\n    # headers={\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n)\n</code></pre>"},{"location":"getting_started/quickstart/","title":"Quickstart","text":"<p> This page is currently under construction. Please check back later for updates.</p> <p>Extralit is a free, open-source, self-hosted tool. This means you need to deploy its UI to start using it. There is two main ways to deploy Extralit:</p> <p>Deploy on the Hugging Face Hub</p> <p>The recommended choice to get started. You can get up and running in under 5 minutes and don't need to maintain a server or run any commands.</p> <p>If you're just getting started with Extralit, click the deploy button below:</p> <p> </p> <p>You can use the default values following these steps:</p> <ul> <li>Leave the default owner if using your personal account</li> <li> <p>Leave <code>ADMIN_USERNAME</code> and <code>ADMIN_PASSWORD</code> secrets empty since you'll sign in with your HF user as the Argilla Space <code>owner</code>.</p> </li> <li> <p>You must fill out the following Space secrets fields:</p> </li> <li><code>OAUTH2_HUGGINGFACE_CLIENT_ID</code> and <code>OAUTH2_HUGGINGFACE_CLIENT_SECRET</code>: The Oauth.</li> <li><code>ARGILLA_DATABASE_URL</code>: The URL of the PostgreSQL database where the data will be stored. If you leave it blank, the data will be lost when the Space restarts.</li> <li><code>S3_ENDPOINT</code>, <code>S3_ACCESS_KEY</code>, <code>S3_SECRET_KEY</code>: The name of the S3 bucket where papers and data extraction artifacts will be stored. If you leave it blank, the data will be lost when the Space restarts.</li> <li>Click Duplicate Space to build an Extralit instance \ud83d\ude80.</li> <li>Once you see the UI, go to the Sign in into the UI section. If you see the <code>Building</code> message for longer than 2-3 min refresh the page.</li> </ul> <p>Database persistent storage</p> <p>Not setting the <code>ARGILLA_DATABASE_URL</code> Space secret that you will loose your data when the Space restarts. Spaces get restarted due to maintainance, inactivity, and every time you change your Spaces settings. If you want to use the Space just for testing you can leave it blank temporarily.</p> <p>If you want to deploy Extralit within a Hugging Face organization, setup a more stable Space, or understand the settings, check out the HF Spaces settings guide.</p> <ul> <li> <p>Setting up HF Authentication</p> <p>From version <code>1.23.0</code> you can enable Hugging Face authentication for your Extralit Space. This feature allows you to give access to your Extralit Space to users that are logged in to the Hugging Face Hub.</p> <p>This feature is specially useful for public crowdsourcing projects. If you would like to have more control over who can log in to the Space, you can set this up on a private space so that only members of your Organization can sign in. Alternatively, you may want to create users and use their credentials instead.</p> <p>To enable this feature, you will first need to create an OAuth App in Hugging Face. To do that, go to your user settings in Hugging Face and select Connected Apps &gt; Create App. Once inside, choose a name for your app and complete the form with the following information:</p> <ul> <li>Homepage URL: Your Extralit Space Direct URL.</li> <li>Logo URL: <code>[Your Extralit Space Direct URL]/favicon.ico</code></li> <li>Scopes: <code>openid</code> and <code>profile</code>.</li> <li>Redirect URL: <code>[Your Extralit Space Direct URL]/oauth/huggingface/callback</code></li> </ul> <p>This will create a Client ID and an App Secret that you will need to add as variables of your Space. To do this, go to the Space Settings &gt; Variables and Secrets and save the Client ID and App Secret as environment secrets like so:</p> <ol> <li>Name: <code>OAUTH2_HUGGINGFACE_CLIENT_ID</code> - Value: [Your Client ID]</li> <li>Name: <code>OAUTH2_HUGGINGFACE_CLIENT_SECRET</code> - Value: [Your App Secret]</li> </ol> </li> </ul> <p>Deploy with Docker</p> <p>If you want to run Extralit locally on your machine or a server, or tune the server configuration, choose this option. To use this option, check this guide.</p>"},{"location":"getting_started/quickstart/#sign-in-into-the-extralit-ui","title":"Sign in into the Extralit UI","text":"<p>If everything went well, you should see the Extralit sign in page that looks like this:</p> <p></p> <p>Building errors</p> <p>If you get a build error, sometimes restarting the Space from the Settings page works, otherwise check the HF Spaces settings guide.</p> <p>In the sign in page:</p> <ol> <li>Click on Sign in with Hugging Face</li> <li>Authorize the application and you will be logged in into Argilla as an <code>owner</code>.</li> </ol> <p>Unauthorized error</p> <p>Sometimes, after authorizing you'll see an unauthorized error, and get redirected to the sign in page. Typically, clicking the Sign in button solves the issue.</p> <p>Congrats! Your Argilla server is ready to start your first project using the Python SDK. You now have full rights to create datasets. Follow the instructions in the home page, or keep reading this guide if you want a more detailed explanation.</p>"},{"location":"getting_started/quickstart/#install-the-python-sdk","title":"Install the Python SDK","text":"<p>To manage workspaces and datasets in Argilla, you need to use the Argilla Python SDK. You can install it with pip as follows:</p> <pre><code>pip install extralit\n</code></pre>"},{"location":"getting_started/quickstart/#create-your-first-dataset","title":"Create your first dataset","text":"<p>For getting started with Argilla and its SDK, we recommend to use Jupyter Notebook or Google Colab.</p> <p>To start interacting with your Argilla server, you need to create a instantiate a client with an API key and API URL:</p> <ul> <li> <p>The <code>&lt;api_key&gt;</code> is in the <code>My Settings</code> page of your Argilla Space.</p> </li> <li> <p>The <code>&lt;api_url&gt;</code> is the URL shown in your browser if it ends with <code>*.hf.space</code>.</p> </li> </ul> <pre><code>import argilla as rg\n\nclient = rg.client(\n    api_url=\"&lt;api_url&gt;\",\n    api_key=\"&lt;api_key&gt;\"\n)\n</code></pre> <p>You can't find your API URL</p> <p>If you're using Spaces, sometimes the Argilla UI is embedded into the Hub UI so the URL of the browser won't match the API URL. In these scenarios, there are two options:     1. Click on the three points menu at the top of the Space, select \"Embed this Space\", and open the direct URL.     2. Use this pattern: <code>https://[your-owner-name]-[your_space_name].hf.space</code>.</p> <p>To create a dataset with a simple text classification task, first, you need to define the dataset settings.</p> <pre><code>settings = rg.Settings(\n    guidelines=\"Classify the reviews as positive or negative.\",\n    fields=[\n        rg.TextField(\n            name=\"review\",\n            title=\"Text from the review\",\n            use_markdown=False,\n        ),\n    ],\n    questions=[\n        rg.LabelQuestion(\n            name=\"my_label\",\n            title=\"In which category does this article fit?\",\n            labels=[\"positive\", \"negative\"],\n        )\n    ],\n)\n</code></pre> <p>Now you can create the dataset with these settings. Publish the dataset to make it available in the UI and add the records.</p> <p>About workspaces</p> <p>Workspaces in Argilla group datasets and user access rights. The <code>workspace</code> parameter is optional in this case. If you don't specify it, the dataset will be created in the default workspace <code>argilla</code>.</p> <p>By default, this workspace will be visible to users joining with the Sign in with Hugging Face button. You can create other workspaces and decide to grant access to users either with the SDK or the changing the OAuth configuration.</p> <pre><code>dataset = rg.Dataset(\n    name=f\"my_first_dataset\",\n    settings=settings,\n    client=client,\n    #workspace=\"argilla\"\n)\ndataset.create()\n</code></pre> <p>Now you can add records to your dataset. We will use the IMDB dataset from the Hugging Face Datasets library as an example. The <code>mapping</code> parameter indicates which keys/columns in the source dataset correspond to the Argilla dataset fields.</p> <pre><code>from datasets import load_dataset\n\ndata = load_dataset(\"imdb\", split=\"train[:100]\").to_list()\n\ndataset.records.log(records=data, mapping={\"text\": \"review\"})\n</code></pre> <p>\ud83c\udf89 You have successfully created your first dataset with Argilla. You can now access it in the Argilla UI and start annotating the records.</p>"},{"location":"getting_started/quickstart/#next-steps","title":"Next steps","text":"<ul> <li> <p>To learn how to create your datasets, workspace, and manage users, check the how-to guides.</p> </li> <li> <p>To learn Argilla with hands-on examples, check the Tutorials section.</p> </li> <li> <p>To further configure your Argilla Space, check the Hugging Face Spaces settings guide.</p> </li> </ul>"},{"location":"notebooks/quickstart_workflow_feedback/","title":"Quickstart workflow feedback","text":"Note  This tutorial demonstrates a sample usage for `FeedbackDataset`, which offers implementations different from the old `TextClassificationDataset`, `Text2TextDataset` and `TokenClassificationDataset`. To have info about old datasets, you can have a look at them [here]([../getting_started/quickstart_workflow.html](https://docs.argilla.io/en/latest/getting_started/quickstart_workflow.html)). Not sure which dataset to use? Check out our section on [choosing a dataset](https://docs.argilla.io/en/latest/practical_guides/choose_dataset.html).   <pre><code>!pip install extralit pandas plotly tqdm weaviate-client langfuse\n</code></pre> <p>It is possible to connect to our Argilla instance by simply importing the Argilla library and using the environment variables and <code>rg.init()</code>.</p> <ul> <li><code>ARGILLA_API_URL</code>: It is the url of the Argilla Server.</li> <li>If you're using Docker, it is <code>http://localhost:6900</code> by default.</li> <li>If you're using HF Spaces, it is constructed as <code>https://[your-owner-name]-[your_space_name].hf.space</code>.</li> <li><code>ARGILLA_API_KEY</code>: It is the API key of the Argilla Server. It is <code>owner</code> by default.</li> <li><code>HF_TOKEN</code>: It is the Hugging Face API token. It is only needed if you're using a private HF Space. You can configure it in your profile: Setting &gt; Access Tokens.</li> <li><code>workspace</code>: It is a \u201cspace\u201d inside your Argilla instance where authorized users can collaborate. It's <code>argilla</code> by default.</li> </ul> <p>For more info about custom configurations like headers, workspace separation or access credentials, check our config page.</p> <pre><code>import argilla as rg\n</code></pre> <pre><code># Argilla credentials\napi_url = \"http://localhost:6900\"  # \"https://&lt;your-hf-space&gt;.hf.space\"\napi_key = \"admin.apikey\"  # Get your API key from the Argilla web interface at My settings &amp;gt; API key\n# Huggingface credentials\nhf_token = \"hf_...\" # Needed if using a private HF Space\n</code></pre> <pre><code>rg.init(\n    api_url=api_url, \n    api_key=api_key,\n    # If you want to use your private HF Space\n    # extra_headers={\"Authorization\": f\"Bearer {\"HF_TOKEN\"}\"}\n)\n</code></pre> <pre><code>import pandas as pd\nimport pandera as pa\nfrom pandera.typing import Index, DataFrame, Series\nfrom extralit.schema.checks import register_check_methods\nregister_check_methods()\n\nclass Publication(pa.DataFrameModel):\n    \"\"\"\n    General information about the publication, extracted once per paper.\n    \"\"\"\n    reference: Index[str] = pa.Field(check_name=True)\n    title: Series[str] = pa.Field()\n    authors: Series[str] = pa.Field()\n    journal: Series[str] = pa.Field()\n    publication_year: Series[int] = pa.Field(ge=1900, le=2100)\n    doi: Series[str] = pa.Field(nullable=True)\n\n    class Config:\n        singleton = True\n</code></pre> <pre><code>df = pd.DataFrame({\n        \"title\": [\"title1\", \"title2\"],\n        \"authors\": [\"author1\", \"author2\"],\n        \"journal\": [\"journal1\", \"journal2\"],\n        \"publication_year\": [2021, 2022],\n        \"doi\": [\"doi1\", \"doi2\"]\n    }, \n    index=pd.Index([\"ref1\", \"ref1\"], name=\"reference\"))\n\nPublication.to_schema().validate(\n    df, lazy=True\n)\n</code></pre> <pre><code>from extralit.extraction.models import SchemaStructure\n\nss = SchemaStructure(schemas=[Publication], singleton_schema=Publication)\nss\n</code></pre> <pre><code>dataset = rg.FeedbackDataset.for_text_classification(\n    labels=[\"joy\", \"sadness\"],\n    multi_label=False,\n    use_markdown=True,\n    guidelines=None,\n    metadata_properties=None,\n    vectors_settings=None,\n)\ndataset\n</code></pre> <p>Now that we have our dataset, we can push the dataset to the Argilla space.</p>   Note  From Argilla 1.14.0, calling `push_to_argilla` will not just push the `FeedbackDataset` into Argilla, but will also return the remote `FeedbackDataset` instance, which implies that the additions, updates, and deletions of records will be pushed to Argilla as soon as they are made. This is a change from previous versions of Argilla, where you had to call `push_to_argilla` again to push the changes to Argilla.   <pre><code>try:\n    dataset.push_to_argilla(name=\"my-first-dataset\", workspace=\"argilla\")\nexcept:\n    pass\n</code></pre> <pre><code>records = [\n    rg.FeedbackRecord(\n        fields={\n            \"text\": \"I am so happy today\",\n        },\n    ),\n    rg.FeedbackRecord(\n        fields={\n            \"text\": \"I feel sad today\",\n        },\n    )\n]\ndataset.add_records(records)\n</code></pre> <pre><code>dataset.records\n</code></pre> <p>Argilla also offers a way to use suggestions and responses from other models as a starting point for annotators. This way, annotators can save time and effort by correcting the predictions or answers instead of annotating from scratch. </p> <pre><code># Besides Argilla, it can also be imported with load_dataset from datasets\ndataset_hf = rg.FeedbackDataset.from_huggingface(\"argilla/emotion\", split=\"train[1:101]\")\n</code></pre> <pre><code>dataset_hf\n</code></pre> <p>We can then start to create a training pipeline by first defining <code>TrainingTask</code>, which is used to define how the data should be processed and formatted according to the associated task and framework. Each task has its own classmethod and the data formatting can always be customized via <code>formatting_func</code>. You can visit this page for more info. Simpler tasks like text classification can be defined using default definitions, as we do in this example.</p> <pre><code>from argilla.feedback import TrainingTask\n\ntask = TrainingTask.for_text_classification(\n    text=dataset_hf.field_by_name(\"text\"),\n    label=dataset_hf.question_by_name(\"label\")\n)\n</code></pre> <p>We can then define our ArgillaTrainer for any of the supported frameworks and customize the training config using ArgillaTrainer.update_config.</p> <p>Let us define ArgillaTrainer with any of the supported frameworks. </p> <pre><code>from argilla.feedback import ArgillaTrainer\n\ntrainer = ArgillaTrainer(\n    dataset=dataset_hf,\n    task=task,\n    framework=\"setfit\",\n    train_size=0.8\n)\n</code></pre> <p>You can update the model config via <code>update_config</code>.</p> <pre><code>trainer.update_config(num_train_epochs=1, num_iterations=1)\n</code></pre> <p>We can now train the model with <code>train</code></p> <pre><code>trainer.train(output_dir=\"setfit_model\")\n</code></pre> <p>and make inferences with <code>predict</code>.</p> <pre><code>trainer.predict(\"This is just perfect!\")\n</code></pre> <p>We have trained a model with FeedbackDataset in this tutorial. For more info about concepts in Argilla Feedback and LLMs, look here. For a more detailed explanation, refer to the documentation and the end-to-end tutorials for beginners.</p>"},{"location":"notebooks/quickstart_workflow_feedback/#extralit-quickstart-workflow","title":"Extralit Quickstart Workflow","text":"<p>This notebook demonstrates the key steps in using Extralit for scientific literature review and data extraction.</p>"},{"location":"notebooks/quickstart_workflow_feedback/#install-libraries","title":"Install Libraries","text":"<p>Install the latest version of Argilla in Colab, along with other libraries and models used in this notebook.</p>"},{"location":"notebooks/quickstart_workflow_feedback/#set-up-argilla","title":"Set Up Argilla","text":"<p>If you have already deployed Argilla Server, then you can skip this step. Otherwise, you can quickly deploy it in two different ways:</p> <ul> <li> <p>You can deploy Argilla Server on HF Spaces.</p> </li> <li> <p>Alternatively, if you want to run Argilla locally on your own computer, the easiest way to get Argilla UI up and running is to deploy on Docker:</p> <pre><code>docker run -d --name quickstart -p 6900:6900 argilla/argilla-quickstart:latest\n</code></pre> </li> </ul> <p>More info on Installation here.</p>"},{"location":"notebooks/quickstart_workflow_feedback/#connect-to-argilla","title":"Connect to Argilla","text":""},{"location":"notebooks/quickstart_workflow_feedback/#create-schemas","title":"Create Schemas","text":""},{"location":"notebooks/quickstart_workflow_feedback/#create-a-dataset","title":"Create a Dataset","text":"<p>FeedbackDataset is the container for Argilla Feedback structure. Argilla Feedback offers different components for FeedbackDatasets that you can employ for various aspects of your workflow. For a more detailed explanation, refer to the documentation and the end-to-end tutorials for beginners.</p> <p>To start, we need to configure the FeedbackDatasest. To do so, there are two options: use a pre-defined template or create a custom one.</p>"},{"location":"notebooks/quickstart_workflow_feedback/#use-a-task-template","title":"Use a Task Template","text":"<p>Argilla offers a set of pre-defined templates for different tasks. You can use them to configure your dataset straightforward. For instance, if you want to create a dataset for simple text classification, you can use the following code:</p>"},{"location":"notebooks/quickstart_workflow_feedback/#configure-a-custom-dataset","title":"Configure a Custom Dataset","text":"<p>If your dataset does not fit into one of the pre-defined templates, you can create a custom dataset by defining the fields, the different question types, the metadata properties and the vectors settings.</p>"},{"location":"notebooks/quickstart_workflow_feedback/#add-the-records","title":"Add the Records","text":"<p>A record refers to each of the data items that will be annotated by the annotator team. The records will be the pieces of information that will be shown to the user in the UI in order to complete the annotation task. In the current dataset sample, it can only consist of a text to be labeled.</p>"},{"location":"notebooks/quickstart_workflow_feedback/#train-a-model","title":"Train a model","text":"<p>As with other datasets, Feedback datasets also allow to create a training pipeline and make inferences with the resulting model. After you gather responses with Argilla Feedback, you can easily fine-tune an LLM. In this example, we will have to complete a text classification task.</p> <p>For fine-tuning, we will use setfit library and the Argilla Trainer, which is a powerful wrapper around many of our favorite NLP libraries. It provides a very intuitive abstract representation to facilitate simple training workflows using decent default pre-set configurations without having to worry about any data transformations from Argilla.</p> <p>Let us first create our dataset to train. For this example, we will use the emotion dataset from Argilla, which was created using Argilla. Each text item has its responses as 6 different sentiments, which are Sadness, Joy, Love, Anger, Fear and Surprise.</p>"},{"location":"reference/argilla/SUMMARY/","title":"SUMMARY","text":"<ul> <li>rg.Argilla</li> <li>rg.Workspace</li> <li>rg.User</li> <li>rg.Dataset<ul> <li>rg.Dataset.records</li> </ul> </li> <li>rg.Settings<ul> <li>Fields</li> <li>Questions</li> <li>Metadata</li> <li>Vectors</li> <li>Distribution</li> </ul> </li> <li>rg.Record<ul> <li>rg.Response</li> <li>rg.Suggestion</li> <li>rg.Vector</li> <li>rg.Metadata</li> </ul> </li> <li>rg.Query</li> <li>rg.markdown</li> </ul>"},{"location":"reference/argilla/client/","title":"<code>rg.Argilla</code>","text":"<p>To interact with the Argilla server from Python you can use the <code>Argilla</code> class. The <code>Argilla</code> client is used to create, get, update, and delete all Argilla resources, such as workspaces, users, datasets, and records.</p>"},{"location":"reference/argilla/client/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/argilla/client/#connecting-to-an-argilla-server","title":"Connecting to an Argilla server","text":"<p>To connect to an Argilla server, instantiate the <code>Argilla</code> class and pass the <code>api_url</code> of the server and the <code>api_key</code> to authenticate.</p> <pre><code>import argilla as rg\n\nclient = rg.Argilla(\n    api_url=\"https://argilla.example.com\",\n    api_key=\"my_token\",\n)\n</code></pre>"},{"location":"reference/argilla/client/#accessing-dataset-workspace-and-user-objects","title":"Accessing Dataset, Workspace, and User objects","text":"<p>The <code>Argilla</code> clients provides access to the <code>Dataset</code>, <code>Workspace</code>, and <code>User</code> objects of the Argilla server.</p> <pre><code>my_dataset = client.datasets(\"my_dataset\")\n\nmy_workspace = client.workspaces(\"my_workspace\")\n\nmy_user = client.users(\"my_user\")\n</code></pre> <p>These resources can then be interacted with to access their properties and methods. For example, to list all datasets in a workspace:</p> <pre><code>for dataset in my_workspace.datasets:\n    print(dataset.name)\n</code></pre>"},{"location":"reference/argilla/markdown/","title":"<code>rg.markdown</code>","text":"<p>To support the usage of Markdown within Argilla, we've created some helper functions to easy the usage of DataURL conversions and chat message visualizations.</p>"},{"location":"reference/argilla/search/","title":"<code>rg.Query</code>","text":"<p>To collect records based on searching criteria, you can use the <code>Query</code> and <code>Filter</code> classes. The <code>Query</code> class is used to define the search criteria, while the <code>Filter</code> class is used to filter the search results. <code>Filter</code> is passed to a <code>Query</code> object so you can combine multiple filters to create complex search queries. A <code>Query</code> object can also be passed to <code>Dataset.records</code> to fetch records based on the search criteria.</p>"},{"location":"reference/argilla/search/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/argilla/search/#searching-for-records-with-terms","title":"Searching for records with terms","text":"<p>To search for records with terms, you can use the <code>Dataset.records</code> attribute with a query string. The search terms are used to search for records that contain the terms in the text field.</p> <pre><code>for record in dataset.records(query=\"paris\"):\n    print(record)\n</code></pre>"},{"location":"reference/argilla/search/#filtering-records-by-conditions","title":"Filtering records by conditions","text":"<p>Argilla allows you to filter records based on conditions. You can use the <code>Filter</code> class to define the conditions and pass them to the <code>Dataset.records</code> attribute to fetch records based on the conditions. Conditions include \"==\", \"&gt;=\", \"&lt;=\", or \"in\". Conditions can be combined with dot notation to filter records based on metadata, suggestions, or responses.</p> <pre><code># create a range from 10 to 20\nrange_filter = rg.Filter(\n    [\n        (\"metadata.count\", \"&gt;=\", 10),\n        (\"metadata.count\", \"&lt;=\", 20)\n    ]\n)\n\n# query records with metadata count greater than 10 and less than 20\nquery = rg.Query(filters=range_filter, query=\"paris\")\n\n# iterate over the results\nfor record in dataset.records(query=query):\n    print(record)\n</code></pre>"},{"location":"reference/argilla/users/","title":"<code>rg.User</code>","text":"<p>A user in Argilla is a profile that uses the SDK or UI. Their profile can be used to track their feedback activity and to manage their access to the Argilla server.</p>"},{"location":"reference/argilla/users/#usage-examples","title":"Usage Examples","text":"<p>To create a new user, instantiate the <code>User</code> object with the client and the username:</p> <pre><code>user = rg.User(username=\"my_username\", password=\"my_password\")\nuser.create()\n</code></pre> <p>Existing users can be retrieved by their username:</p> <pre><code>user = client.users(\"my_username\")\n</code></pre> <p>The current user of the <code>rg.Argilla</code> client can be accessed using the <code>me</code> attribute:</p> <pre><code>client.me\n</code></pre>"},{"location":"reference/argilla/users/#rguser_1","title":"<code>rg.User</code>","text":""},{"location":"reference/argilla/workspaces/","title":"<code>rg.Workspace</code>","text":"<p>In Argilla, workspaces are used to organize datasets in to groups. For example, you might have a workspace for each project or team.</p>"},{"location":"reference/argilla/workspaces/#usage-examples","title":"Usage Examples","text":"<p>To create a new workspace, instantiate the <code>Workspace</code> object with the client and the name:</p> <pre><code>workspace = rg.Workspace(name=\"my_workspace\")\nworkspace.create()\n</code></pre> <p>To retrieve an existing workspace, use the <code>client.workspaces</code> attribute:</p> <pre><code>workspace = client.workspaces(\"my_workspace\")\n</code></pre>"},{"location":"reference/argilla/workspaces/#rgworkspace_1","title":"<code>rg.Workspace</code>","text":""},{"location":"reference/argilla/datasets/dataset_records/","title":"<code>rg.Dataset.records</code>","text":""},{"location":"reference/argilla/datasets/dataset_records/#usage-examples","title":"Usage Examples","text":"<p>In most cases, you will not need to create a <code>DatasetRecords</code> object directly. Instead, you can access it via the <code>Dataset</code> object:</p> <pre><code>dataset.records\n</code></pre> <p>For user familiar with legacy approaches</p> <ol> <li><code>Dataset.records</code> object is used to interact with the records in a dataset. It interactively fetches records from the server in batches without using a local copy of the records.</li> <li>The <code>log</code> method of <code>Dataset.records</code> is used to both add and update records in a dataset. If the record includes a known <code>id</code> field, the record will be updated. If the record does not include a known <code>id</code> field, the record will be added.</li> </ol>"},{"location":"reference/argilla/datasets/dataset_records/#adding-records-to-a-dataset","title":"Adding records to a dataset","text":"<p>To add records to a dataset, use the <code>log</code> method. Records can be added as dictionaries or as <code>Record</code> objects. Single records can also be added as a dictionary or <code>Record</code>.</p> As a <code>Record</code> objectFrom a data structureFrom a data structure with a mappingFrom a Hugging Face dataset <p>You can also add records to a dataset by initializing a <code>Record</code> object directly.</p> <pre><code>records = [\n    rg.Record(\n        fields={\n            \"question\": \"Do you need oxygen to breathe?\",\n            \"answer\": \"Yes\"\n        },\n    ),\n    rg.Record(\n        fields={\n            \"question\": \"What is the boiling point of water?\",\n            \"answer\": \"100 degrees Celsius\"\n        },\n    ),\n] # (1)\n\ndataset.records.log(records)\n</code></pre> <ol> <li>This is an illustration of a definition. In a real world scenario, you would iterate over a data structure and create <code>Record</code> objects for each iteration.</li> </ol> <pre><code>data = [\n    {\n        \"question\": \"Do you need oxygen to breathe?\",\n        \"answer\": \"Yes\",\n    },\n    {\n        \"question\": \"What is the boiling point of water?\",\n        \"answer\": \"100 degrees Celsius\",\n    },\n] # (1)\n\ndataset.records.log(data)\n</code></pre> <ol> <li>The data structure's keys must match the fields or questions in the Argilla dataset. In this case, there are fields named <code>question</code> and <code>answer</code>.</li> </ol> <pre><code>data = [\n    {\n        \"query\": \"Do you need oxygen to breathe?\",\n        \"response\": \"Yes\",\n    },\n    {\n        \"query\": \"What is the boiling point of water?\",\n        \"response\": \"100 degrees Celsius\",\n    },\n] # (1)\ndataset.records.log(\n    records=data,\n    mapping={\"query\": \"question\", \"response\": \"answer\"} # (2)\n)\n</code></pre> <ol> <li>The data structure's keys must match the fields or questions in the Argilla dataset. In this case, there are fields named <code>question</code> and <code>answer</code>.</li> <li>The data structure has keys <code>query</code> and <code>response</code> and the Argilla dataset has <code>question</code> and <code>answer</code>. You can use the <code>mapping</code> parameter to map the keys in the data structure to the fields in the Argilla dataset.</li> </ol> <p>You can also add records to a dataset using a Hugging Face dataset. This is useful when you want to use a dataset from the Hugging Face Hub and add it to your Argilla dataset.</p> <p>You can add the dataset where the column names correspond to the names of fields, questions, metadata or vectors in the Argilla dataset.</p> <p>If the dataset's schema does not correspond to your Argilla dataset names, you can use a <code>mapping</code> to indicate which columns in the dataset correspond to the Argilla dataset fields.</p> <pre><code>from datasets import load_dataset\n\nhf_dataset = load_dataset(\"imdb\", split=\"train[:100]\") # (1)\n\ndataset.records.log(records=hf_dataset)\n</code></pre> <ol> <li>In this example, the Hugging Face dataset matches the Argilla dataset schema. If that is not the case, you could use the <code>.map</code> of the <code>datasets</code> library to prepare the data before adding it to the Argilla dataset.</li> </ol> <p>Here we use the <code>mapping</code> parameter to specify the relationship between the Hugging Face dataset and the Argilla dataset.</p> <pre><code>dataset.records.log(records=hf_dataset, mapping={\"txt\": \"text\", \"y\": \"label\"}) # (1)\n</code></pre> <ol> <li>In this case, the <code>txt</code> key in the Hugging Face dataset corresponds to the <code>text</code> field in the Argilla dataset, and the <code>y</code> key in the Hugging Face dataset corresponds to the <code>label</code> field in the Argilla dataset.</li> </ol>"},{"location":"reference/argilla/datasets/dataset_records/#updating-records-in-a-dataset","title":"Updating records in a dataset","text":"<p>Records can also be updated using the <code>log</code> method with records that contain an <code>id</code> to identify the records to be updated. As above, records can be added as dictionaries or as <code>Record</code> objects.</p> As a <code>Record</code> objectFrom a data structureFrom a data structure with a mappingFrom a Hugging Face dataset <p>You can update records in a dataset by initializing a <code>Record</code> object directly and providing the <code>id</code> field.</p> <pre><code>records = [\n    rg.Record(\n        metadata={\"department\": \"toys\"},\n        id=\"2\" # (1)\n    ),\n] # (1)\n\ndataset.records.log(records)\n</code></pre> <ol> <li>The <code>id</code> field is required to identify the record to be updated. The <code>id</code> field must be unique for each record in the dataset. If the <code>id</code> field is not provided, the record will be added as a new record.</li> </ol> <p>You can also update records in a dataset by providing the <code>id</code> field in the data structure.</p> <pre><code>data = [\n    {\n        \"metadata\": {\"department\": \"toys\"},\n        \"id\": \"2\" # (1)\n    },\n] # (1)\n\ndataset.records.log(data)\n</code></pre> <ol> <li>The <code>id</code> field is required to identify the record to be updated. The <code>id</code> field must be unique for each record in the dataset. If the <code>id</code> field is not provided, the record will be added as a new record.</li> </ol> <p>You can also update records in a dataset by providing the <code>id</code> field in the data structure and using a mapping to map the keys in the data structure to the fields in the dataset.</p> <p><pre><code>data = [\n    {\n        \"metadata\": {\"department\": \"toys\"},\n        \"my_id\": \"2\" # (1)\n    },\n]\n\ndataset.records.log(\n    records=data,\n    mapping={\"my_id\": \"id\"} # (2)\n)\n</code></pre> 1. The <code>id</code> field is required to identify the record to be updated. The <code>id</code> field must be unique for each record in the dataset. If the <code>id</code> field is not provided, the record will be added as a new record. 2. Let's say that your data structure has keys <code>my_id</code> instead of <code>id</code>. You can use the <code>mapping</code> parameter to map the keys in the data structure to the fields in the dataset.</p> <p>You can also update records to an Argilla dataset using a Hugging Face dataset. To update records, the Hugging Face dataset must contain an <code>id</code> field to identify the records to be updated, or you can use a mapping to map the keys in the Hugging Face dataset to the fields in the Argilla dataset.</p> <pre><code>from datasets import load_dataset\n\nhf_dataset = load_dataset(\"imdb\", split=\"train[:100]\") # (1)\n\ndataset.records.log(records=hf_dataset, mapping={\"uuid\": \"id\"}) # (2)\n</code></pre> <ol> <li>In this example, the Hugging Face dataset matches the Argilla dataset schema.</li> <li>The <code>uuid</code> key in the Hugging Face dataset corresponds to the <code>id</code> field in the Argilla dataset.</li> </ol>"},{"location":"reference/argilla/datasets/dataset_records/#iterating-over-records-in-a-dataset","title":"Iterating over records in a dataset","text":"<p><code>Dataset.records</code> can be used to iterate over records in a dataset from the server. The records will be fetched in batches from the server::</p> <pre><code>for record in dataset.records:\n    print(record)\n\n# Fetch records with suggestions and responses\nfor record in dataset.records(with_suggestions=True, with_responses=True):\n    print(record.suggestions)\n    print(record.responses)\n\n# Filter records by a query and fetch records with vectors\nfor record in dataset.records(query=\"capital\", with_vectors=True):\n    print(record.vectors)\n</code></pre> <p>Check out the <code>rg.Record</code> class reference for more information on the properties and methods available on a record and the <code>rg.Query</code> class reference for more information on the query syntax.</p>"},{"location":"reference/argilla/datasets/dataset_records/#rgdatasetrecords_1","title":"<code>rg.Dataset.records</code>","text":""},{"location":"reference/argilla/datasets/datasets/","title":"<code>rg.Dataset</code>","text":"<p><code>Dataset</code> is a class that represents a collection of records. It is used to store and manage records in Argilla.</p>"},{"location":"reference/argilla/datasets/datasets/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/argilla/datasets/datasets/#creating-a-dataset","title":"Creating a Dataset","text":"<p>To create a new dataset you need to define its name and settings. Optional parameters are <code>workspace</code> and <code>client</code>, if you want to create the dataset in a specific workspace or on a specific Argilla instance.</p> <pre><code>dataset = rg.Dataset(\n    name=\"my_dataset\",\n    settings=rg.Settings(\n        fields=[\n            rg.TextField(name=\"text\"),\n        ],\n        questions=[\n            rg.TextQuestion(name=\"response\"),\n        ],\n    ),\n)\ndataset.create()\n</code></pre> <p>For a detail guide of the dataset creation and publication process, see the Dataset how to guide.</p>"},{"location":"reference/argilla/datasets/datasets/#retrieving-an-existing-dataset","title":"Retrieving an existing Dataset","text":"<p>To retrieve an existing dataset, use <code>client.datasets(\"my_dataset\")</code> instead.</p> <pre><code>dataset = client.datasets(\"my_dataset\")\n</code></pre>"},{"location":"reference/argilla/datasets/datasets/#rgdataset_1","title":"<code>rg.Dataset</code>","text":""},{"location":"reference/argilla/records/metadata/","title":"<code>metadata</code>","text":"<p>Metadata in argilla is a dictionary that can be attached to a record. It is used to store additional information about the record that is not part of the record's fields or responses. For example, the source of the record, the date it was created, or any other information that is relevant to the record. Metadata can be added to a record directly or as valules within a dictionary.</p>"},{"location":"reference/argilla/records/metadata/#usage-examples","title":"Usage Examples","text":"<p>To use metadata within a dataset, you must define a metadata property in the dataset settings. The metadata property is a list of metadata properties that can be attached to a record. The following example demonstrates how to add metadata to a dataset and how to access metadata from a record object:</p> <pre><code>import argilla as rg\n\ndataset = Dataset(\n    name=\"dataset_with_metadata\",\n    settings=Settings(\n        fields=[TextField(name=\"text\")],\n        questions=[LabelQuestion(name=\"label\", labels=[\"positive\", \"negative\"])],\n        metadata=[\n            rg.TermsMetadataProperty(name=\"category\", options=[\"A\", \"B\", \"C\"]),\n        ],\n    ),\n)\ndataset.create()\n</code></pre> <p>Then, you can add records to the dataset with metadata that corresponds to the metadata property defined in the dataset settings:</p> <pre><code>dataset_with_metadata.records.log(\n    [\n        {\"text\": \"text\", \"label\": \"positive\", \"category\": \"A\"},\n        {\"text\": \"text\", \"label\": \"negative\", \"category\": \"B\"},\n    ]\n)\n</code></pre>"},{"location":"reference/argilla/records/metadata/#format-per-metadataproperty-type","title":"Format per <code>MetadataProperty</code> type","text":"<p>Depending on the <code>MetadataProperty</code> type, metadata might need to be formatted in a slightly different way.</p> For <code>FloatMetadataProperty</code>For <code>IntegerMetadataProperty</code>For <code>TermsMetadataProperty</code> <pre><code>rg.Records(\n    fields={\"text\": \"example\"},\n    metadata={\"category\": 2.1}\n)\n</code></pre> <pre><code>rg.Records(\n    fields={\"text\": \"example\"},\n    metadata={\"category\": 42}\n)\n</code></pre> <pre><code>rg.Records(\n    fields={\"text\": \"example\"},\n    metadata={\"category\": \"A\"}\n)\n\n# with multiple terms\n\nrg.Records(\n    fields={\"text\": \"example\"},\n    metadata={\"category\": [\"A\", \"B\"]}\n)\n</code></pre>"},{"location":"reference/argilla/records/records/","title":"<code>rg.Record</code>","text":"<p>The <code>Record</code> object is used to represent a single record in Argilla. It contains fields, suggestions, responses, metadata, and vectors.</p>"},{"location":"reference/argilla/records/records/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/argilla/records/records/#creating-a-record","title":"Creating a Record","text":"<p>To create records, you can use the <code>Record</code> class and pass it to the <code>Dataset.records.log</code> method. The <code>Record</code> class requires a <code>fields</code> parameter, which is a dictionary of field names and values. The field names must match the field names in the dataset's <code>Settings</code> object to be accepted.</p> <pre><code>dataset.records.log(\n    records=[\n        rg.Record(\n            fields={\"text\": \"Hello World, how are you?\"},\n        ),\n    ]\n) # (1)\n</code></pre> <ol> <li>The Argilla dataset contains a field named <code>text</code> matching the key here.</li> </ol>"},{"location":"reference/argilla/records/records/#accessing-record-attributes","title":"Accessing Record Attributes","text":"<p>The <code>Record</code> object has suggestions, responses, metadata, and vectors attributes that can be accessed directly whilst iterating over records in a dataset.</p> <pre><code>for record in dataset.records(\n    with_suggestions=True,\n    with_responses=True,\n    with_metadata=True,\n    with_vectors=True\n    ):\n    print(record.suggestions)\n    print(record.responses)\n    print(record.metadata)\n    print(record.vectors)\n</code></pre> <p>Record properties can also be updated whilst iterating over records in a dataset.</p> <pre><code>for record in dataset.records(with_metadata=True):\n    record.metadata = {\"department\": \"toys\"}\n</code></pre> <p>For changes to take effect, the user must call the <code>update</code> method on the <code>Dataset</code> object, or pass the updated records to <code>Dataset.records.log</code>. All core record atttributes can be updated in this way. Check their respective documentation for more information: Suggestions, Responses, Metadata, Vectors.</p>"},{"location":"reference/argilla/records/records/#rgrecord_1","title":"<code>rg.Record</code>","text":""},{"location":"reference/argilla/records/responses/","title":"<code>rg.Response</code>","text":"<p>Class for interacting with Argilla Responses of records. Responses are answers to questions by a user. Therefore, a recod question can have multiple responses, one for each user that has answered the question. A <code>Response</code> is typically created by a user in the UI or consumed from a data source as a label, unlike a <code>Suggestion</code> which is typically created by a model prediction.</p>"},{"location":"reference/argilla/records/responses/#usage-examples","title":"Usage Examples","text":"<p>Responses can be added to an instantiated <code>Record</code> directly or as a dictionary a dictionary. The following examples demonstrate how to add responses to a record object and how to access responses from a record object:</p> <p>Instantiate the <code>Record</code> and related <code>Response</code> objects:</p> <pre><code>dataset.records.log(\n    [\n        rg.Record(\n            fields={\"text\": \"Hello World, how are you?\"},\n            responses=[rg.Response(\"label\", \"negative\", user_id=user.id)],\n            external_id=str(uuid.uuid4()),\n        )\n    ]\n)\n</code></pre> <p>Or, add a response from a dictionary where key is the question name and value is the response:</p> <pre><code>dataset.records.log(\n    [\n        {\n            \"text\": \"Hello World, how are you?\",\n            \"label.response\": \"negative\",\n        },\n    ]\n)\n</code></pre> <p>Responses can be accessed from a <code>Record</code> via their question name as an attribute of the record. So if a question is named <code>label</code>, the response can be accessed as <code>record.label</code>. The following example demonstrates how to access responses from a record object:</p> <p><pre><code># iterate over the records and responses\n\nfor record in dataset.records:\n    for response in record.responses[\"label\"]: # (1)\n        print(response.value)\n        print(response.user_id)\n\n# validate that the record has a response\n\nfor record in dataset.records:\n    if record.responses[\"label\"]:\n        for response in record.responses[\"label\"]:\n            print(response.value)\n            print(response.user_id)\n    else:\n        record.responses.add(\n            rg.Response(\"label\", \"positive\", user_id=user.id)\n        ) # (2)\n</code></pre>     1. Access the responses for the question named <code>label</code> for each record like a dictionary containing a list of <code>Response</code> objects.     2. Add a response to the record if it does not already have one.</p>"},{"location":"reference/argilla/records/responses/#format-per-question-type","title":"Format per <code>Question</code> type","text":"<p>Depending on the <code>Question</code> type, responses might need to be formatted in a slightly different way.</p> For <code>LabelQuestion</code>For <code>MultiLabelQuestion</code>For <code>RankingQuestion</code>For <code>RatingQuestion</code>For <code>SpanQuestion</code>For <code>TextQuestion</code> <pre><code>rg.Response(\n    question_name=\"label\",\n    value=\"positive\",\n    user_id=user.id,\n    status=\"draft\"\n)\n</code></pre> <pre><code>rg.Response(\n    question_name=\"multi-label\",\n    value=[\"positive\", \"negative\"],\n    user_id=user.id,\n    status=\"draft\"\n)\n</code></pre> <pre><code>rg.Response(\n    question_name=\"rank\",\n    value=[\"1\", \"3\", \"2\"],\n    user_id=user.id,\n    status=\"draft\"\n)\n</code></pre> <pre><code>rg.Response(\n    question_name=\"rating\",\n    value=4,\n    user_id=user.id,\n    status=\"draft\"\n)\n</code></pre> <pre><code>rg.Response(\n    question_name=\"span\",\n    value=[{\"start\": 0, \"end\": 9, \"label\": \"MISC\"}],\n    user_id=user.id,\n    status=\"draft\"\n)\n</code></pre> <pre><code>rg.Response(\n    question_name=\"text\",\n    value=\"value\",\n    user_id=user.id,\n    status=\"draft\"\n)\n</code></pre>"},{"location":"reference/argilla/records/responses/#rgresponse_1","title":"<code>rg.Response</code>","text":""},{"location":"reference/argilla/records/suggestions/","title":"<code>rg.Suggestion</code>","text":"<p>Class for interacting with Argilla Suggestions of records. Suggestions are typically created by a model prediction, unlike a <code>Response</code> which is typically created by a user in the UI or consumed from a data source as a label.</p>"},{"location":"reference/argilla/records/suggestions/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/argilla/records/suggestions/#adding-records-with-suggestions","title":"Adding records with suggestions","text":"<p>Suggestions can be added to a record directly or via a dictionary structure. The following examples demonstrate how to add suggestions to a record object and how to access suggestions from a record object:</p> <p>Add a response from a dictionary where key is the question name and value is the response:</p> <pre><code>dataset.records.log(\n    [\n        {\n            \"text\": \"Hello World, how are you?\",\n            \"label\": \"negative\", # this will be used as a suggestion\n        },\n    ]\n)\n</code></pre> <p>If your data contains scores for suggestions you can add them as well via the <code>mapping</code> parameter. The following example demonstrates how to add a suggestion with a score to a record object:</p> <pre><code>dataset.records.log(\n    [\n        {\n            \"prompt\": \"Hello World, how are you?\",\n            \"label\": \"negative\",  # this will be used as a suggestion\n            \"score\": 0.9,  # this will be used as the suggestion score\n            \"model\": \"model_name\",  # this will be used as the suggestion agent\n        },\n    ],\n    mapping={\n        \"score\": \"label.suggestion.score\",\n        \"model\": \"label.suggestion.agent\",\n    },  # `label` is the question name in the dataset settings\n)\n</code></pre> <p>Or, instantiate the <code>Record</code> and related <code>Suggestions</code> objects directly, like this:</p> <pre><code>dataset.records.log(\n    [\n        rg.Record(\n            fields={\"text\": \"Hello World, how are you?\"},\n            suggestions=[rg.Suggestion(\"negative\", \"label\", score=0.9, agent=\"model_name\")],\n        )\n    ]\n)\n</code></pre>"},{"location":"reference/argilla/records/suggestions/#iterating-over-records-with-suggestions","title":"Iterating over records with suggestions","text":"<p>Just like responses, suggestions can be accessed from a <code>Record</code> via their question name as an attribute of the record. So if a question is named <code>label</code>, the suggestion can be accessed as <code>record.label</code>. The following example demonstrates how to access suggestions from a record object:</p> <pre><code>for record in dataset.records(with_suggestions=True):\n    print(record.suggestions[\"label\"].value)\n</code></pre> <p>We can also add suggestions to records as we iterate over them using the <code>add</code> method:</p> <pre><code>for record in dataset.records(with_suggestions=True):\n    if not record.suggestions[\"label\"]: # (1)\n        record.suggestions.add(\n            rg.Suggestion(\"positive\", \"label\", score=0.9, agent=\"model_name\")\n        ) # (2)\n</code></pre> <ol> <li>Validate that the record has a suggestion</li> <li>Add a suggestion to the record if it does not already have one</li> </ol>"},{"location":"reference/argilla/records/suggestions/#format-per-question-type","title":"Format per <code>Question</code> type","text":"<p>Depending on the <code>Question</code> type, responses might need to be formatted in a slightly different way.</p> For <code>LabelQuestion</code>For <code>MultiLabelQuestion</code>For <code>RankingQuestion</code>For <code>RatingQuestion</code>For <code>SpanQuestion</code>For <code>TextQuestion</code> <pre><code>rg.Suggestion(\n    question_name=\"label\",\n    value=\"positive\",\n    score=0.9,\n    agent=\"model_name\"\n)\n</code></pre> <pre><code>rg.Suggestion(\n    question_name=\"multi-label\",\n    value=[\"positive\", \"negative\"],\n    score=0.9,\n    agent=\"model_name\"\n)\n</code></pre> <pre><code>rg.Suggestion(\n    question_name=\"rank\",\n    value=[\"1\", \"3\", \"2\"],\n    score=0.9,\n    agent=\"model_name\"\n)\n</code></pre> <pre><code>rg.Suggestion(\n    question_name=\"rating\",\n    value=4,\n    score=0.9,\n    agent=\"model_name\"\n)\n</code></pre> <pre><code>rg.Suggestion(\n    question_name=\"span\",\n    value=[{\"start\": 0, \"end\": 9, \"label\": \"MISC\"}],\n    score=0.9,\n    agent=\"model_name\"\n)\n</code></pre> <pre><code>rg.Suggestion(\n    question_name=\"text\",\n    value=\"value\",\n    score=0.9,\n    agent=\"model_name\"\n)\n</code></pre>"},{"location":"reference/argilla/records/suggestions/#rgsuggestion_1","title":"<code>rg.Suggestion</code>","text":""},{"location":"reference/argilla/records/vectors/","title":"<code>rg.Vector</code>","text":"<p>A vector is a numerical representation of a <code>Record</code> field or attribute, usually the record's text. Vectors can be used to search for similar records via the UI or SDK. Vectors can be added to a record directly or as a dictionary with a key that the matches <code>rg.VectorField</code> name.</p>"},{"location":"reference/argilla/records/vectors/#usage-examples","title":"Usage Examples","text":"<p>To use vectors within a dataset, you must define a vector field in the dataset settings. The vector field is a list of vector fields that can be attached to a record. The following example demonstrates how to add vectors to a dataset and how to access vectors from a record object:</p> <pre><code>import argilla as rg\n\ndataset = Dataset(\n    name=\"dataset_with_metadata\",\n    settings=Settings(\n        fields=[TextField(name=\"text\")],\n        questions=[LabelQuestion(name=\"label\", labels=[\"positive\", \"negative\"])],\n        vectors=[\n            VectorField(name=\"vector_name\"),\n        ],\n    ),\n)\ndataset.create()\n</code></pre> <p>Then, you can add records to the dataset with vectors that correspond to the vector field defined in the dataset settings:</p> <pre><code>dataset.records.log(\n    [\n        {\n            \"text\": \"Hello World, how are you?\",\n            \"vector_name\": [0.1, 0.2, 0.3]\n        }\n    ]\n)\n</code></pre> <p>Vectors can be passed using a mapping, where the key is the key in the data source and the value is the name in the dataset's setting's <code>rg.VectorField</code> object. For example, the following code adds a record with a vector using a mapping:</p> <pre><code>dataset.records.log(\n    [\n        {\n            \"text\": \"Hello World, how are you?\",\n            \"x\": [0.1, 0.2, 0.3]\n        }\n    ],\n    mapping={\"x\": \"vector_name\"}\n)\n</code></pre> <p>Or, vectors can be instantiated and added to a record directly, like this:</p> <pre><code>dataset.records.log(\n    [\n        rg.Record(\n            fields={\"text\": \"Hello World, how are you?\"},\n            vectors=[rg.Vector(\"embedding\", [0.1, 0.2, 0.3])],\n        )\n    ]\n)\n</code></pre>"},{"location":"reference/argilla/records/vectors/#rgvector_1","title":"<code>rg.Vector</code>","text":""},{"location":"reference/argilla/settings/fields/","title":"Fields","text":"<p>Fields in Argilla are define the content of a record that will be reviewed by a user.</p>"},{"location":"reference/argilla/settings/fields/#usage-examples","title":"Usage Examples","text":"<p>To define a field, instantiate the <code>TextField</code> class and pass it to the <code>fields</code> parameter of the <code>Settings</code> class.</p> <pre><code>text_field = rg.TextField(name=\"text\")\nmarkdown_field = rg.TextField(name=\"text\", use_markdown=True)\n</code></pre> <p>The <code>fields</code> parameter of the <code>Settings</code> class can accept a list of fields, like this:</p> <pre><code>settings = rg.Settings(\n    fields=[\n        rg.TextField(name=\"text\"),\n    ],\n)\n\ndata = rg.Dataset(\n    name=\"my_dataset\",\n    settings=settings,\n)\n</code></pre> <p>To add records with values for fields, refer to the <code>rg.Dataset.records</code> documentation.</p>"},{"location":"reference/argilla/settings/fields/#rgtextfield","title":"<code>rg.TextField</code>","text":""},{"location":"reference/argilla/settings/metadata_property/","title":"Metadata Properties","text":"<p>Metadata properties are used to define metadata fields in a dataset. Metadata fields are used to store additional information about the records in the dataset. For example, the category of a record, the price of a product, or any other information that is relevant to the record.</p>"},{"location":"reference/argilla/settings/metadata_property/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/argilla/settings/metadata_property/#defining-metadata-property-for-a-dataset","title":"Defining Metadata Property for a dataset","text":"<p>We define metadata properties via type specific classes. The following example demonstrates how to define metadata properties as either a float, integer, or terms metadata property:</p> <p><code>TermsMetadataProperty</code> is used to define a metadata field with a list of options. For example, a color field with options red, blue, and green. <code>FloatMetadataProperty</code> and <code>IntegerMetadataProperty</code> is used to define a metadata field with a float value. For example, a price field with a minimum value of 0.0 and a maximum value of 100.0.</p> <pre><code>import argilla as rg\n\n# Define metadata properties as terms\nmetadata_field = rg.TermsMetadataProperty(\n    name=\"color\",\n    options=[\"red\", \"blue\", \"green\"],\n    title=\"Color\",\n)\n\n# Define metadata properties as float\nfloat_\nmetadata_field = rg.FloatMetadataProperty(\n    name=\"price\",\n    min=0.0,\n    max=100.0,\n    title=\"Price\",\n)\n\n# Define metadata properties as integer\nint_metadata_field = rg.IntegerMetadataProperty(\n    name=\"quantity\",\n    min=0,\n    max=100,\n    title=\"Quantity\",\n)\n</code></pre> <p>Metadata properties can be added to a dataset settings object:</p> <pre><code>dataset = rg.Dataset(\n    name=\"my_dataset\",\n    settings=rg.Settings(\n        fields=[\n            rg.TextField(name=\"text\"),\n        ],\n        metadata=[\n            metadata_field,\n            float_metadata_field,\n            int_metadata_field,\n        ],\n    ),\n)\n</code></pre> <p>To add records with metadata, refer to the <code>rg.Metadata</code> class documentation.</p>"},{"location":"reference/argilla/settings/metadata_property/#rgfloatmetadataproperty","title":"<code>rg.FloatMetadataProperty</code>","text":""},{"location":"reference/argilla/settings/metadata_property/#rgintegermetadataproperty","title":"<code>rg.IntegerMetadataProperty</code>","text":""},{"location":"reference/argilla/settings/metadata_property/#rgtermsmetadataproperty","title":"<code>rg.TermsMetadataProperty</code>","text":""},{"location":"reference/argilla/settings/questions/","title":"Questions","text":"<p>Questions in Argilla are the questions that will be answered as feedback. They are used to define the questions that will be answered by users or models.</p>"},{"location":"reference/argilla/settings/questions/#usage-examples","title":"Usage Examples","text":"<p>To define a label question, for example, instantiate the <code>LabelQuestion</code> class and pass it to the <code>Settings</code> class.</p> <pre><code>label_question = rg.LabelQuestion(name=\"label\", labels=[\"positive\", \"negative\"])\n\nsettings = rg.Settings(\n    fields=[\n        rg.TextField(name=\"text\"),\n    ],\n    questions=[\n        label_question,\n    ],\n)\n</code></pre> <p>Questions can be combined in extensible ways based on the type of feedback you want to collect. For example, you can combine a label question with a text question to collect both a label and a text response.</p> <pre><code>label_question = rg.LabelQuestion(name=\"label\", labels=[\"positive\", \"negative\"])\ntext_question = rg.TextQuestion(name=\"response\")\n\nsettings = rg.Settings(\n    fields=[\n        rg.TextField(name=\"text\"),\n    ],\n    questions=[\n        label_question,\n        text_question,\n    ],\n)\n\ndataset = rg.Dataset(\n    name=\"my_dataset\",\n    settings=settings,\n)\n</code></pre> <p>To add records with responses to questions, refer to the <code>rg.Response</code> class documentation.</p>"},{"location":"reference/argilla/settings/questions/#rglabelquestion","title":"<code>rg.LabelQuestion</code>","text":""},{"location":"reference/argilla/settings/questions/#rgmultilabelquestion","title":"<code>rg.MultiLabelQuestion</code>","text":""},{"location":"reference/argilla/settings/questions/#rgrankingquestion","title":"<code>rg.RankingQuestion</code>","text":""},{"location":"reference/argilla/settings/questions/#rgtextquestion","title":"<code>rg.TextQuestion</code>","text":""},{"location":"reference/argilla/settings/questions/#rgratingquestion","title":"<code>rg.RatingQuestion</code>","text":""},{"location":"reference/argilla/settings/questions/#rgspanquestion","title":"<code>rg.SpanQuestion</code>","text":""},{"location":"reference/argilla/settings/settings/","title":"<code>rg.Settings</code>","text":"<p><code>rg.Settings</code> is used to define the setttings of an Argilla <code>Dataset</code>. The settings can be used to configure the behavior of the dataset, such as the fields, questions, guidelines, metadata, and vectors. The <code>Settings</code> class is passed to the <code>Dataset</code> class and used to create the dataset on the server. Once created, the settings of a dataset cannot be changed.</p>"},{"location":"reference/argilla/settings/settings/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/argilla/settings/settings/#creating-a-new-dataset-with-settings","title":"Creating a new dataset with settings","text":"<p>To create a new dataset with settings, instantiate the <code>Settings</code> class and pass it to the <code>Dataset</code> class.</p> <pre><code>import argilla as rg\n\nsettings = rg.Settings(\n    guidelines=\"Select the sentiment of the prompt.\",\n    fields=[rg.TextField(name=\"prompt\", use_markdown=True)],\n    questions=[rg.LabelQuestion(name=\"sentiment\", labels=[\"positive\", \"negative\"])],\n)\n\ndataset = rg.Dataset(name=\"sentiment_analysis\", settings=settings)\n\n# Create the dataset on the server\ndataset.create()\n</code></pre> <p>To define the settings for fields, questions, metadata, vectors, or distribution, refer to the <code>rg.TextField</code>, <code>rg.LabelQuestion</code>, <code>rg.TermsMetadataProperty</code>, and <code>rg.VectorField</code>, <code>rg.TaskDistribution</code> class documentation.</p>"},{"location":"reference/argilla/settings/settings/#rgsettings_1","title":"<code>rg.Settings</code>","text":""},{"location":"reference/argilla/settings/task_distribution/","title":"Distribution","text":"<p>Distribution settings are used to define the criteria used by the tool to automatically manage records in the dataset depending on the expected number of submitted responses per record.</p>"},{"location":"reference/argilla/settings/task_distribution/#usage-examples","title":"Usage Examples","text":"<p>The default minimum submitted responses per record is 1. If you wish to increase this value, you can define it through the <code>TaskDistribution</code> class and pass it to the <code>Settings</code> class.</p> <pre><code>settings = rg.Settings(\n    guidelines=\"These are some guidelines.\",\n    fields=[\n        rg.TextField(\n            name=\"text\",\n        ),\n    ],\n    questions=[\n        rg.LabelQuestion(\n            name=\"label\",\n            labels=[\"label_1\", \"label_2\", \"label_3\"]\n        ),\n    ],\n    distribution=rg.TaskDistribution(min_submitted=3)\n)\n\ndataset = rg.Dataset(\n    name=\"my_dataset\",\n    settings=settings\n)\n</code></pre>"},{"location":"reference/argilla/settings/task_distribution/#rgtaskdistribution","title":"<code>rg.TaskDistribution</code>","text":""},{"location":"reference/argilla/settings/vectors/","title":"Vectors","text":"<p>Vector fields in Argilla are used to define the vector form of a record that will be reviewed by a user.</p>"},{"location":"reference/argilla/settings/vectors/#usage-examples","title":"Usage Examples","text":"<p>To define a vector field, instantiate the <code>VectorField</code> class with a name and dimenstions, then pass it to the <code>vectors</code> parameter of the <code>Settings</code> class.</p> <pre><code>settings = rg.Settings(\n    fields=[\n        rg.TextField(name=\"text\"),\n    ],\n    vectors=[\n        rg.VectorField(\n            name=\"my_vector\",\n            dimension=768,\n            title=\"Document Embedding\",\n        ),\n    ],\n)\n</code></pre> <p>To add records with vectors, refer to the <code>rg.Vector</code> class documentation.</p>"},{"location":"reference/argilla/settings/vectors/#rgvectorfield","title":"<code>rg.VectorField</code>","text":""},{"location":"reference/argilla-server/configuration/","title":"Server configuration","text":"<p>This section explains advanced operations and settings for running the Argilla Server and Argilla Python Client.</p> <p>By default, the Argilla Server will look for your Elasticsearch (ES) endpoint at <code>http://localhost:9200</code>. You can customize this by setting the <code>ARGILLA_ELASTICSEARCH</code> environment variable. Have a look at the list of available environment variables to further configure the Argilla server.</p> <p>From the Argilla version <code>1.19.0</code>, you must set up the search engine manually to work with datasets. You should set the environment variable <code>ARGILLA_SEARCH_ENGINE=opensearch</code> or <code>ARGILLA_SEARCH_ENGINE=elasticsearch</code> depending on the backend you're using The default value for this variable is set to <code>elasticsearch</code>. The minimal version for Elasticsearch is <code>8.5.0</code>, and for Opensearch is <code>2.4.0</code>. Please, review your backend and upgrade it if necessary.</p> <p>Warning</p> <p>For vector search in OpenSearch, the filtering applied is using a <code>post_filter</code> step, since there is a bug that makes queries fail using filtering + knn from Argilla. See https://github.com/opensearch-project/k-NN/issues/1286</p> <p>This may result in unexpected results when combining filtering with vector search with this engine.</p>"},{"location":"reference/argilla-server/configuration/#launching","title":"Launching","text":""},{"location":"reference/argilla-server/configuration/#using-a-proxy","title":"Using a proxy","text":"<p>If you run Argilla behind a proxy by adding some extra prefix to expose the service, you should set the <code>ARGILLA_BASE_URL</code> environment variable to properly route requests to the server application.</p> <p>For example, if your proxy exposes Argilla in the URL <code>https://my-proxy/custom-path-for-argilla</code>, you should launch the Argilla server with <code>ARGILLA_BASE_URL=/custom-path-for-argilla</code>.</p> <p>NGINX and Traefik have been tested and are known to work with Argilla:</p> <ul> <li>NGINX example</li> <li>Traefik example</li> </ul>"},{"location":"reference/argilla-server/configuration/#environment-variables","title":"Environment variables","text":"<p>You can set the following environment variables to further configure your server and client.</p>"},{"location":"reference/argilla-server/configuration/#server","title":"Server","text":""},{"location":"reference/argilla-server/configuration/#fastapi","title":"FastAPI","text":"<ul> <li> <p><code>ARGILLA_HOME_PATH</code>: The directory where Argilla will store all the files needed to run. If the path doesn't exist it will be automatically created (Default: <code>~/.argilla</code>).</p> </li> <li> <p><code>ARGILLA_BASE_URL</code>: If you want to launch the Argilla server in a specific base path other than /, you should set up this environment variable. This can be useful when running Argilla behind a proxy that adds a prefix path to route the service (Default: \"/\").</p> </li> <li> <p><code>ARGILLA_CORS_ORIGINS</code>: List of host patterns for CORS origin access.</p> </li> <li> <p><code>ARGILLA_DOCS_ENABLED</code>: If False, disables openapi docs endpoint at /api/docs.</p> </li> <li> <p><code>ARGILLA_ENABLE_TELEMETRY</code>: If False, disables telemetry for usage metrics.</p> </li> </ul>"},{"location":"reference/argilla-server/configuration/#authentication","title":"Authentication","text":"<ul> <li><code>ARGILLA_AUTH_SECRET_KEY</code>: The secret key used to sign the API token data. You can use <code>openssl rand -hex 32</code> to generate a 32 character string to use with this environment variable. By default a random value is generated, so if you are using more than one server worker (or more than one Argilla server) you will need to set the same value for all of them.</li> </ul>"},{"location":"reference/argilla-server/configuration/#database","title":"Database","text":"<ul> <li><code>ARGILLA_DATABASE_URL</code>: A URL string that contains the necessary information to connect to a database. Argilla uses SQLite by default, PostgreSQL is also officially supported (Default: <code>sqlite:///$ARGILLA_HOME_PATH/argilla.db?check_same_thread=False</code>).</li> </ul>"},{"location":"reference/argilla-server/configuration/#sqlite","title":"SQLite","text":"<p>The following environment variables are useful only when SQLite is used:</p> <ul> <li><code>ARGILLA_DATABASE_SQLITE_TIMEOUT</code>: How many seconds the connection should wait before raising an <code>OperationalError</code> when a table is locked. If another connection opens a transaction to modify a table, that table will be locked until the transaction is committed. (Defaut: <code>15</code> seconds).</li> </ul>"},{"location":"reference/argilla-server/configuration/#postgresql","title":"PostgreSQL","text":"<p>The following environment variables are useful only when PostgreSQL is used:</p> <ul> <li> <p><code>ARGILLA_DATABASE_POSTGRESQL_POOL_SIZE</code>: The number of connections to keep open inside the database connection pool (Default: <code>15</code>).</p> </li> <li> <p><code>ARGILLA_DATABASE_POSTGRESQL_MAX_OVERFLOW</code>: The number of connections that can be opened above and beyond <code>ARGILLA_DATABASE_POSTGRESQL_POOL_SIZE</code> setting (Default: <code>10</code>).</p> </li> </ul>"},{"location":"reference/argilla-server/configuration/#search-engine","title":"Search engine","text":"<ul> <li> <p><code>ARGILLA_ELASTICSEARCH</code>: URL of the connection endpoint of the Elasticsearch instance (Default: <code>http://localhost:9200</code>).</p> </li> <li> <p><code>ARGILLA_SEARCH_ENGINE</code>: Search engine to use. Valid values are \"elasticsearch\" and \"opensearch\" (Default: \"elasticsearch\").</p> </li> <li> <p><code>ARGILLA_ELASTICSEARCH_SSL_VERIFY</code>: If \"False\", disables SSL certificate verification when connecting to the Elasticsearch backend.</p> </li> <li> <p><code>ARGILLA_ELASTICSEARCH_CA_PATH</code>: Path to CA cert for ES host. For example: <code>/full/path/to/root-ca.pem</code> (Optional)</p> </li> </ul>"},{"location":"reference/argilla-server/configuration/#datasets","title":"Datasets","text":"<ul> <li> <p><code>ARGILLA_LABEL_SELECTION_OPTIONS_MAX_ITEMS</code>: Set the number of maximum items to be allowed by label and multi label questions (Default: <code>500</code>).</p> </li> <li> <p><code>ARGILLA_SPAN_OPTIONS_MAX_ITEMS</code>: Set the number of maximum items to be allowed by span questions (Default: <code>500</code>).</p> </li> </ul>"},{"location":"reference/argilla-server/configuration/#hugging-face","title":"Hugging Face","text":"<ul> <li><code>ARGILLA_SHOW_HUGGINGFACE_SPACE_PERSISTENT_STORAGE_WARNING</code>: When Argilla is running on Hugging Face Spaces you can use this environment variable to disable the warning message showed when persistent storage is disabled for the space (Default: <code>true</code>).</li> </ul>"},{"location":"reference/argilla-server/configuration/#docker-images-only","title":"Docker images only","text":"<ul> <li> <p><code>REINDEX_DATASET</code>: If <code>true</code> or <code>1</code>, the datasets will be reindexed in the search engine. This is needed when some search configuration changed or data must be refreshed (Default: <code>0</code>).</p> </li> <li> <p><code>USERNAME</code>: If provided, the owner username. This can be combined with HF OAuth to define the argilla server owner (Default: <code>\"\"</code>).</p> </li> <li> <p><code>PASSWORD</code>: If provided, the owner password. If <code>USERNAME</code> and <code>PASSWORD</code> are provided, the owner user will be created with these credentials on the server startup (Default: <code>\"\"</code>).</p> </li> <li> <p><code>API_KEY</code>: The default user api key to user. If API_KEY is not provided, a new random api key will be generated (Default: <code>\"\"</code>).</p> </li> </ul>"},{"location":"reference/argilla-server/configuration/#rest-api-docs","title":"REST API docs","text":"<p>FastAPI also provides beautiful REST API docs that you can check at http://localhost:6900/api/v1/docs.</p>"},{"location":"reference/argilla-server/telemetry/","title":"Server Telemetry","text":"<p>Argilla uses telemetry to report anonymous usage and error information. As an open-source software, this type of information is important to improve and understand how the product is used.</p>"},{"location":"reference/argilla-server/telemetry/#how-to-opt-out","title":"How to opt out","text":"<p>You can opt out of telemetry reporting using the <code>ENV</code> variable <code>ARGILLA_ENABLE_TELEMETRY</code> before launching the server. Setting this variable to <code>0</code> will completely disable telemetry reporting.</p> <p>If you are a Linux/MacOs user, you should run:</p> <pre><code>export ARGILLA_ENABLE_TELEMETRY=0\n</code></pre> <p>If you are a Windows user, you should run:</p> <pre><code>set ARGILLA_ENABLE_TELEMETRY=0\n</code></pre> <p>To opt in again, you can set the variable to <code>1</code>.</p>"},{"location":"reference/argilla-server/telemetry/#why-reporting-telemetry","title":"Why reporting telemetry","text":"<p>Anonymous telemetry information enables us to continuously improve the product and detect recurring problems to better serve all users. We collect aggregated information about general usage and errors. We do NOT collect any information on users' data records, datasets, or metadata information.</p>"},{"location":"reference/argilla-server/telemetry/#sensitive-data","title":"Sensitive data","text":"<p>We do not collect any piece of information related to the source data you store in Argilla. We don't identify individual users. Your data does not leave your server at any time:</p> <ul> <li>No dataset record is collected.</li> <li>No dataset names or metadata are collected.</li> </ul>"},{"location":"reference/argilla-server/telemetry/#information-reported","title":"Information reported","text":"<p>The following usage and error information is reported:</p> <ul> <li>The code of the raised error and the entity type related to the error, if any (Dataset, Workspace,...)</li> <li>The <code>user-agent</code> and <code>accept-language</code> http headers</li> <li>Task name and number of records for bulk operations</li> <li>An anonymous generated user uuid</li> <li>The Argilla version running the server</li> <li>The Python version, e.g. <code>3.8.13</code></li> <li>The system/OS name, such as <code>Linux</code>, <code>Darwin</code>, <code>Windows</code></li> <li>The system\u2019s release version, e.g. <code>Darwin Kernel Version 21.5.0: Tue Apr 26 21:08:22 PDT 2022; root:xnu-8020</code></li> <li>The machine type, e.g. <code>AMD64</code></li> <li>The underlying platform spec with as much useful information as possible. (eg. <code>macOS-10.16-x86_64-i386-64bit</code>)</li> <li>The type of deployment: <code>huggingface_space</code> or <code>server</code></li> <li>The dockerized deployment flag: <code>True</code> or <code>False</code></li> </ul> <p>For transparency, you can inspect the source code where this is performed here.</p> <p>If you have any doubts, don't hesitate to join our Discord channel or open a GitHub issue. We'd be very happy to discuss how we can improve this.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>These are the tutorials for the Argilla SDK. They provide step-by-step instructions for common tasks.</p> <ul> <li> <p>Text classification</p> <p>Learn about a standard workflow to improve data quality for a text classification task.  Tutorial</p> </li> <li> <p>Token classification</p> <p>Learn about a standard workflow to improve data quality for a token classification task.  Tutorial</p> </li> </ul>"},{"location":"user_guide/","title":"User guides","text":"<p> This page is currently under construction. Please check back later for updates.</p> <p>These guides provide step-by-step instructions for common scenarios, including detailed explanations and code samples. Please read the Core Concepts section before proceeding with the user guides.</p> <ul> <li> <p>Core concepts</p> <p>Learn about the core concepts of Extralit, including the data pipeline, schema definition, and advanced usage.</p> <p> User guide</p> </li> </ul>"},{"location":"user_guide/#data-pipeline","title":"Data Pipeline","text":"<ul> <li> <p>Schema definition</p> <p>Learn what they are and how to create and edit <code>SchemaStructure</code> in Extralit.</p> <p> User guide</p> </li> </ul>"},{"location":"user_guide/#advanced-usage","title":"Advanced Usage","text":"<ul> <li> <p>Customizing multiple extraction schemas</p> <p>Learn how to define multiple related schemas in Extralit to extract structured data from scientific papers.</p> <p> User guide</p> </li> </ul>"},{"location":"user_guide/core_concepts/","title":"Data Extraction Workflow","text":"<p>The data extraction workflow in Extralit is designed to efficiently transform unstructured scientific papers into structured, analyzable data. This process involves several key steps:</p> <ol> <li> <p>Document Import: Scientific papers are imported into Extralit, typically in PDF format.</p> </li> <li> <p>Text and Table OCR: Extralit applies several advanced OCR models to extract text and table content from the imported documents.</p> </li> <li> <p>Schema Application: Predefined schemas are applied to guide the extraction process, specifying what data should be extracted and how it should be structured.</p> </li> <li> <p>LLM-Assisted Extraction: Large Language Models (LLMs) are employed to assist in identifying and extracting relevant information based on the applied schemas.</p> </li> <li> <p>Manual Review and Correction: Extracted data can be manually reviewed and corrected by researchers to ensure accuracy.</p> </li> <li> <p>Consensus Review: For collaborative projects, multiple reviewers can assess the extracted data, and a consensus can be reached on the final dataset.</p> </li> <li> <p>Data Validation: Extracted data is validated against the defined schemas to ensure consistency and completeness.</p> </li> <li> <p>Export: The final structured dataset can be exported for further analysis or downstream data processes.</p> </li> </ol> <p>This workflow combines automated processes with human oversight, ensuring both efficiency and accuracy in the data extraction process. The use of LLMs and predefined schemas allows for handling complex and diverse scientific literature while maintaining consistency across large-scale extraction projects.</p>"},{"location":"user_guide/core_concepts/#schemas-and-references","title":"Schemas and References","text":""},{"location":"user_guide/core_concepts/#schemas","title":"Schemas","text":"<p>In Extralit, schemas play a crucial role in defining the structure of the organization and format of data to be extracted from scientific papers. They are defined using Pandera dataframe schemas, which provide flexible way to specify and validate the expected structure and content of extracted data.</p> <p>Key aspects of schemas in Extralit:</p> <ol> <li> <p>Column Definitions: Schemas specify the columns that should be present in the extracted data table. Each column is defined with its name, data type, and any constraints (e.g., allowed values, numerical ranges).</p> </li> <li> <p>Data Validation: Pandera schemas allow for complex validation rules, ensuring that extracted data meets specific criteria or relationships between fields.</p> </li> <li> <p>Nested Structures: Schemas can represent complex, nested data structures often found in scientific literature.</p> </li> <li> <p>Relationships: Schemas can reference one another, allowing for the representation of relationships between different tables or datasets extracted from the same paper.</p> </li> </ol> <p>Example of a simple schema definition:</p> <pre><code>import pandera as pa\n\nclass StudySchema(pa.SchemaModel):\n    study_id: pa.typing.Series[int] = pa.Field(gt=0)\n    title: pa.typing.Series[str] = pa.Field(str_length={'min_length': 1})\n    publication_year: pa.typing.Series[int] = pa.Field(ge=1900, le=2100)\n    sample_size: pa.typing.Series[int] = pa.Field(gt=0)\n</code></pre>"},{"location":"user_guide/core_concepts/#references","title":"References","text":"<p>In Extralit, \"references\" refer to unique identifiers for each scientific paper in the system. These references serve several important purposes:</p> <ol> <li> <p>Unique Identification: Each paper has a unique reference, allowing for easy tracking and management of individual documents within the system.</p> </li> <li> <p>Linking Extracted Data: All data extracted from a paper is associated with its reference, maintaining a clear connection between the source document and the extracted information.</p> </li> <li> <p>Version Control: References can include version information, allowing for tracking of different versions or revisions of the same paper.</p> </li> <li> <p>Cross-Referencing: When schemas define relationships between tables, references ensure that data from different tables can be correctly associated with the same source document.</p> </li> </ol> <p>By combining robust schema definitions with a clear referencing system, Extralit ensures that extracted data is well-structured, validated, and traceable back to its source, facilitating high-quality scientific data extraction and analysis.</p>"},{"location":"user_guide/core_concepts/#workspaces-and-datasets","title":"Workspaces and Datasets","text":"<p>Extralit organizes data extraction projects using workspaces and datasets, providing a scalable way to manage sequential steps in the extraction process and collaborate with team members.</p>"},{"location":"user_guide/core_concepts/#workspaces","title":"Workspaces","text":"<p>Workspaces in Extralit serve as the high-level container for organizing an extraction project. They provide an environment to work on verious steps of the extraction process and a reference point to consolidate the data artifacts and extraction outputs.</p> <p>Key features of workspaces:</p> <ol> <li>Project Organization: Group resources and tasks within a single workspace.</li> <li>Access Control: Manage user permissions at the workspace level, controlling who can view, edit, or administer the workspace and its contents.</li> <li>Collaborative Environment: Allow multiple team members to work on the same set of documents and extracted data.</li> <li>Resource Sharing: Share common resources like schemas, documents, model ouputs, LLM configurations, and validation rules across datasets within the workspace.</li> </ol>"},{"location":"user_guide/core_concepts/#datasets","title":"Datasets","text":"<p>Datasets are collections of documents and their associated extracted data within a workspace. They represent a specific extraction task or a subset of documents that are being processed together.</p> <p>Key aspects of datasets:</p> <ol> <li>Document Collection: A dataset contains a set of scientific papers (references) that are being processed for data extraction.</li> <li>Schema Association: Each dataset is associated with one or more schemas that define the structure of the data to be extracted.</li> <li>Extraction Progress Tracking: Datasets allow for tracking the progress of extraction tasks across multiple documents.</li> <li>Version Control: Maintain different versions of extracted data as the extraction process progresses or as papers are updated.</li> <li>Export and Import: Facilitate the export of extracted data for analysis and the import of new documents for processing.</li> </ol>"},{"location":"user_guide/core_concepts/#workflow-integration","title":"Workflow Integration","text":"<p>The workspace and dataset structure in Extralit supports a flexible workflow:</p> <ol> <li>Create a workspace for a project to produce a desired a dataset.</li> <li>Set up datasets within the workspace for different aspects of the project or batches of papers.</li> <li>Import documents into the appropriate datasets.</li> <li>Run OCR models to extract text and table content from the documents.</li> <li>Apply schemas to guide the data extraction process.</li> <li>Make use of LLM-assisted extraction to automate data extraction.</li> <li>Collaborate on extraction tasks within each dataset.</li> <li>Review and validate extracted data at the dataset level.</li> <li>Export finalized data for analysis or integration with other research tools.</li> </ol> <p>This organization allows researchers to flexibly manage large-scale literature reviews and data extraction projects, ensuring that work is well-organized, collaborative, and traceable.</p>"},{"location":"user_guide/multi_schemas/","title":"Customizing Multiple Extraction Schemas","text":"<p>While defining a single schema are useful for many extraction tasks, there are scenarios where using multiple schemas can provide better organization and more accurate representation of the data. This guide will walk you through the process of creating and managing multiple schemas in Extralit.</p>"},{"location":"user_guide/multi_schemas/#why-use-multiple-schemas","title":"Why Use Multiple Schemas?","text":"<p>Multiple schemas are beneficial when:</p> <ol> <li>Different parts of a paper contain distinct types of information.</li> <li>There are one-to-many relationships between data points.</li> <li>You want to establish relational links between different types of data.</li> <li>You need to prevent data duplication and maintain data integrity.</li> </ol>"},{"location":"user_guide/multi_schemas/#example-1-separating-study-design-and-demographic-information","title":"Example 1: Separating Study Design and Demographic Information","text":"<p>Let's consider a scenario where a scientific paper presents information about two studies, each with multiple demographic groups. If we try to capture all this information in a single schema, we might end up with redundant data entry.</p> <p>Here's an example of what the data in the paper might look like:</p> Year Study Type Age Group Gender Count 2020 RCT Child Male 50 2020 RCT Child Female 55 2020 RCT Adult Male 100 2020 RCT Adult Female 95 2021 Observational Adult Male 75 2021 Observational Adult Female 80 2021 Observational Senior Male 40 2021 Observational Senior Female 45 <p>If we were to use a single schema to capture this information, it might look like this:</p> <pre><code>import pandera as pa\nfrom pandera.typing import Series\n\nclass StudyDemographic(pa.DataFrameModel):\n    year: Series[int] = pa.Field(ge=2000, le=2024)\n    study_type: Series[str] = pa.Field(isin=['RCT', 'Observational', 'Meta-analysis'])\n    age_group: Series[str] = pa.Field(isin=['Child', 'Adult', 'Senior'])\n    gender: Series[str] = pa.Field(isin=['Male', 'Female', 'Other'])\n    count: Series[int] = pa.Field(gt=0)\n</code></pre> <p>However, using this schema would require redundant manual data entry. Notice how we would have to repeat the study information (year and study_type) for each demographic entry. This redundancy can lead to data inconsistencies that is exacerbated as the number of demographic groups increase, and makes the data more difficult to update/correct.</p> <p>To solve this, we can separate our schema into two:</p> <pre><code>import pandera as pa\nfrom pandera.typing import Series, Index\n\nclass StudyDesign(pa.DataFrameModel):\n    StudyDesign_ID: Index[str] = pa.Field(unique=True)\n    year: Series[int] = pa.Field(ge=2000, le=2023)\n    sample_size: Series[int] = pa.Field(gt=0)\n    study_type: Series[str] = pa.Field(isin=['RCT', 'Observational', 'Meta-analysis'])\n\nclass Demographic(pa.DataFrameModel):\n    StudyDesign_ID: Series[str]  # This will be our foreign key\n    age_group: Series[str] = pa.Field(isin=['Child', 'Adult', 'Senior'])\n    gender: Series[str] = pa.Field(isin=['Male', 'Female', 'Other'])\n    count: Series[int] = pa.Field(gt=0)\n</code></pre> <p>Note that we've introduced a <code>StudyDesign_ID</code> field in the <code>StudyDesign</code> and <code>Demographic</code> schemas, which serves as a foreign key linking <code>Demographic</code> data to the <code>StudyDesign</code> information.</p> <p>Now, we can represent our data more efficiently:</p> Study Design Table StudyDesign_ID year study_type S01 2020 RCT S02 2021 Observational Demographic Table StudyDesign_ID age_group gender count S01 Child Male 50 S01 Child Female 55 S01 Adult Male 100 S01 Adult Female 95 S02 Adult Male 75 S02 Adult Female 80 S02 Senior Male 40 S02 Senior Female 45 <p>This approach eliminates redundancy in the study information and allows for a more flexible representation of the data. It's particularly useful when:</p> <ul> <li>A single study potentially has large number of demographic groups.</li> <li>You want to update study information without affecting demographic data.</li> <li>You need to analyze demographic data across multiple studies easily.</li> </ul> <p>In the next sections, we'll explore how to establish relationships between these schemas and how to manage them in Extralit.</p>"},{"location":"user_guide/multi_schemas/#example-2-establishing-relational-schemas","title":"Example 2: Establishing Relational Schemas","text":"<p>Let's extend our example to include a third schema for outcome measures:</p> <pre><code>class OutcomeMeasure(pa.DataFrameModel):\n    measure_id: Index[str] = pa.Field(unique=True)\n    study_id: Series[str]  # Foreign key to StudyDesign\n    demographic_id: Series[str]  # Foreign key to Demographic\n    measure_type: Series[str] = pa.Field(isin=['Primary', 'Secondary'])\n    value: Series[float] = pa.Field(ge=0)\n</code></pre> <p>In this schema:</p> <ul> <li><code>study_id</code> links the outcome to a specific study.</li> <li><code>demographic_id</code> optionally links the outcome to a specific demographic group.</li> </ul> <p>This structure allows for complex querying across all three schemas, enabling analysis of outcomes by study and demographic characteristics.</p>"},{"location":"user_guide/multi_schemas/#converting-schemas-to-json","title":"Converting Schemas to JSON","text":"<p>To use these schemas with Extralit's server, we need to convert them to JSON format. Here's how you can do that:</p> <pre><code>from os.path import join\ntarget_dir = 'path/to/schemas/'\n\nStudyDesign.to_schema().to_json(join(target_dir, 'study_design_schema.json'))\nDemographic.to_schema().to_json(join(target_dir, 'demographic_schema.json'))\nOutcomeMeasure.to_schema().to_json(join(target_dir, 'outcome_measure_schema.json'))\n</code></pre> <p>This code will create three JSON files containing the schema definitions.</p>"},{"location":"user_guide/multi_schemas/#uploading-schemas-to-extralit-server","title":"Uploading Schemas to Extralit Server","text":"<p>Once you have your schema JSON files, you can upload them to your Extralit workspace using the command-line interface:</p> <pre><code>extralit schemas upload --workspace {WORKSPACE_NAME} --schemas path/to/schemas/\n</code></pre> <p>Replace <code>{WORKSPACE_NAME}</code> with the name of your Extralit workspace, and ensure the path to your schema JSON files is correct.</p>"},{"location":"user_guide/multi_schemas/#best-practices-for-multiple-schemas","title":"Best Practices for Multiple Schemas","text":"<ol> <li> <p>Keep It Simple: Start with the simplest schema structure that accurately represents your data. You can always add complexity later.</p> </li> <li> <p>Use Meaningful Names: Choose clear, descriptive names for your schemas and fields.</p> </li> <li> <p>Establish Clear Relationships: When using multiple schemas, clearly define how they relate to each other (e.g., through foreign keys).</p> </li> <li> <p>Avoid Redundancy: Don't duplicate information across schemas unnecessarily. Use references (foreign keys) instead.</p> </li> <li> <p>Consider Extraction Efficiency: Design your schemas to align with how information is typically presented in the papers you're analyzing. This can make the extraction process more straightforward.</p> </li> <li> <p>Validate Relationships: Implement cross-schema validation to ensure referential integrity (e.g., every <code>study_id</code> in <code>Demographic</code> exists in <code>StudyDesign</code>).</p> </li> <li> <p>Document Your Schema Structure: Maintain clear documentation of how your schemas relate to each other and what each schema represents.</p> </li> </ol> <p>By thoughtfully designing and implementing multiple schemas, you can create a robust, flexible system for extracting and organizing complex information from scientific papers. This approach allows for more nuanced analysis and can significantly improve the quality and usability of your extracted data.</p>"},{"location":"user_guide/schema_definition/","title":"Schema Definition","text":"<p>In Extralit, schemas are defined using Pandera's DataFrameModel. These schemas specify the structure and validation rules for the data you want to extract from each scientific paper reference. This guide will walk you through the process of defining a schema, explaining each component in detail.</p> <p>Note: Before diving into schema definition, make sure you understand the concept of references in Extralit. References are unique identifiers for each scientific paper in your dataset. Learn more about references and other core concepts in the Core Concepts guide.</p>"},{"location":"user_guide/schema_definition/#basic-usage","title":"Basic Usage","text":"<p>Let's start with two basic examples of schema definitions: one for document-level extraction and another for table extraction.</p>"},{"location":"user_guide/schema_definition/#document-level-extraction","title":"Document-Level Extraction","text":"<p>This type of schema is used for information that appears only once per paper:</p> <pre><code>import pandas as pd\nimport pandera as pa\nfrom pandera.typing import Index, DataFrame, Series\n\nclass Publication(pa.DataFrameModel):\n    \"\"\"\n    General information about the publication, extracted once per paper.\n    \"\"\"\n    reference: Index[str] = pa.Field(unique=True, check_name=True)\n    title: Series[str] = pa.Field()\n    authors: Series[str] = pa.Field()\n    journal: Series[str] = pa.Field()\n    publication_year: Series[int] = pa.Field(ge=1900, le=2100)\n    doi: Series[str] = pa.Field(nullable=True)\n\n    class Config:\n        singleton = {'enabled': True}  # Indicates this is a document-level schema\n</code></pre>"},{"location":"user_guide/schema_definition/#table-extraction","title":"Table Extraction","text":"<p>This type of schema is used for information that may appear multiple times in a paper:</p> <pre><code>class StudyDesign(pa.DataFrameModel):\n    \"\"\"\n    Study design details for one or more experimental setup.\n    \"\"\"\n    year: Series[int] = pa.Field(gt=2000, coerce=True)\n    month: Series[int] = pa.Field(ge=1, le=12, coerce=True)\n    day: Series[int] = pa.Field(ge=0, le=365, coerce=True)\n    sample_size: Series[int] = pa.Field(gt=0)\n</code></pre> <p>Let's break down this schema definition:</p>"},{"location":"user_guide/schema_definition/#1-imports","title":"1. Imports","text":"<pre><code>import pandas as pd\nimport pandera as pa\nfrom pandera.typing import Index, DataFrame, Series\n</code></pre> <p>These imports bring in the necessary modules: - <code>pandas</code> for data manipulation - <code>pandera</code> for schema definition - <code>Index</code>, <code>DataFrame</code>, and <code>Series</code> from <code>pandera.typing</code> for type hinting</p>"},{"location":"user_guide/schema_definition/#2-class-definition","title":"2. Class Definition","text":"<pre><code>class StudyDesign(pa.DataFrameModel):\n    \"\"\"\n    Study design details for one or more experimental setup that are often found in methodology sections.\n    \"\"\"\n</code></pre> <p>This defines a new <code>pa.DataFrameModel</code> class named <code>StudyDesign</code> that represents a table to be extracted from each paper reference. Important points:</p> <ul> <li>The class represents a specific type of information (study design) that may appear multiple times in a single paper.</li> <li>The docstring provides context for the LLM, guiding it to look for specific types of information (i.e. experimental setup) and where to find it (methodology sections).</li> <li>Multiple instances of this schema may be extracted from a single paper if multiple study designs are described.</li> </ul>"},{"location":"user_guide/schema_definition/#3-column-definitions","title":"3. Column Definitions","text":"<p>Each line within the class defines a column in our schema:</p> <pre><code>year: Series[int] = pa.Field(gt=2000, coerce=True)\n</code></pre> <p>Let's break this down:</p> <ul> <li><code>year</code>: This is the name of the column.</li> <li><code>Series[int]</code>: This specifies that the column should be a pandas Series containing integer values.</li> <li><code>pa.Field()</code>: This is where we define validation rules and other properties for the column.</li> </ul>"},{"location":"user_guide/schema_definition/#4-validation-rules","title":"4. Validation Rules","text":"<p>Inside <code>pa.Field()</code>, we specify validation rules:</p> <ul> <li><code>gt=2000</code>: This means the value should be greater than 2000.</li> <li><code>coerce=True</code>: This tells Pandera to try to convert (coerce) the input to the specified type (int in this case) if it's not already of that type.</li> </ul> <p>Similarly, for the <code>month</code> column:</p> <pre><code>month: Series[int] = pa.Field(ge=1, le=12, coerce=True)\n</code></pre> <ul> <li><code>ge=1</code>: Greater than or equal to 1</li> <li><code>le=12</code>: Less than or equal to 12</li> </ul> <p>These rules ensure that the month is always an integer between 1 and 12.</p>"},{"location":"user_guide/schema_definition/#advanced-usage","title":"Advanced Usage","text":"<p>Let's extend our schema with more advanced features, incorporating best practices to ensure data quality and consistency:</p> <pre><code>class StudyDesign(pa.DataFrameModel):\n    \"\"\"\n    Study design details for one or more experimental setup that are often found in methodology sections.\n    This schema includes best practices for extracting structured data from scientific papers.\n    \"\"\"\n    study_id: Index[str] = pa.Field(\n        unique=True, \n        str_length={'min_length': 5, 'max_length': 10},\n        check_name=True\n    )\n    year: Series[int] = pa.Field(ge=1900, le=2100, coerce=True)\n    month: Series[int] = pa.Field(ge=1, le=12, coerce=True)\n    day: Series[int] = pa.Field(ge=1, le=31, coerce=True)\n    sample_size: Series[int] = pa.Field(gt=0)\n    study_type: Series[str] = pa.Field(\n        isin=['RCT', 'Observational', 'Meta-analysis'],\n        description=\"The type of study conducted\",\n        nullable=False\n    )\n\n    @pa.check('sample_size')\n    def check_sample_size(cls, sample_size: Series[int]) -&gt; Series[bool]:\n        return sample_size % 2 == 0  # Ensure sample size is even\n\n    class Config:\n        strict = True\n        coerce = True\n</code></pre> <p>New features in this advanced schema:</p> <ol> <li> <p>String Length Validation:      <pre><code>study_id: Index[str] = pa.Field(unique=True, str_length={'min_length': 5, 'max_length': 10},check_name=True)\n</code></pre>     This ensures the <code>study_id</code> is a string between 5 and 10 characters long.</p> </li> <li> <p>Nullable Fields:      <pre><code>month: Series[int] = pa.Field(ge=1, le=12, nullable=True, coerce=True)\n</code></pre>     Allowing certain fields to be nullable acknowledges that not all information may be available in every paper.</p> </li> <li> <p>Categorical Validation:     <pre><code>study_type: Series[str] = pa.Field(isin=['RCT', 'Observational', 'Meta-analysis'])\n</code></pre>     This restricts <code>study_type</code> to only these three values.</p> </li> <li> <p>Field Descriptions:     <pre><code>study_type: Series[str] = pa.Field(\n    isin=['RCT', 'Observational', 'Meta-analysis'],\n    description=\"The type of study conducted\",\n    nullable=False\n)\n</code></pre>     Adding descriptions to fields helps guide the extraction process and provides context for the LLM and data annotators.</p> </li> <li> <p>Custom Validation Check:     <pre><code>@pa.check('sample_size')\ndef check_sample_size(cls, sample_size: Series[int]) -&gt; Series[bool]:\n     return sample_size % 2 == 0  # Ensure sample size is even\n</code></pre>     This custom check ensures that the sample size is always even.</p> </li> <li> <p>Configuration:     <pre><code>class Config:\n     strict = True\n     coerce = True\n</code></pre></p> <ul> <li><code>strict = True</code>: Ensures no additional columns are allowed beyond what's defined in the schema.</li> <li><code>coerce = True</code>: Applies type coercion to all fields by default.</li> </ul> </li> </ol>"},{"location":"user_guide/schema_definition/#using-the-schema","title":"Using the Schema","text":"<p>Once defined, you can use this schema to validate your data:</p> <pre><code>import pandas as pd\n\ndata = pd.DataFrame({\n    'study_id': ['STUDY001', 'STUDY002'],\n    'year': [2022, 2023],\n    'month': [6, 12],\n    'day': [15, 31],\n    'sample_size': [100, 200],\n    'study_type': ['RCT', 'Observational']\n})\n\nvalidated_data = StudyDesign.validate(data)\n</code></pre> <p>If the data doesn't meet the schema requirements, Pandera will raise an informative error, helping you identify and correct issues in your extracted data.</p>"},{"location":"user_guide/schema_definition/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Start Simple: Begin with basic type and range validations, then add more complex rules as needed.</p> </li> <li> <p>Use Descriptive Names: Choose clear, descriptive names for your schema classes and fields.</p> </li> <li> <p>Use Schema Descriptions: Include a detailed docstring for your schema class. This helps guide the LLM in understanding what type of information to extract and where to find it in the paper.</p> </li> <li> <p>Leverage Field Metadata: Use the description parameter in pa.Field() to provide context for each field. This can help guide the extraction process and provide valuable information for data users.</p> </li> <li> <p>Use Appropriate Data Types: Choose the most specific data type possible for each field. For example, use int for whole numbers and float for decimal values.</p> </li> <li> <p>Implement Logical Constraints: Use multi-field checks to ensure logical consistency between related fields.</p> </li> <li> <p>Allow for Flexibility: Use nullable=True for fields that may not always be present in every paper. This acknowledges the variability in reporting across different studies.</p> </li> <li> <p>Test Your Schema: Create validate your schema to a small dataset to ensure the checks behave as expected with various input data.</p> </li> </ol> <p>By defining clear and comprehensive schemas, you ensure that the data extracted by Extralit is consistent, valid, and ready for analysis. This approach significantly reduces data cleaning efforts and improves the overall quality of your research data.</p>"},{"location":"user_guide/user_guide/","title":"User guide","text":"<ol> <li>PDF Preprocessing: Detection and extraction of table structures in PDF documents.</li> <li>Schema-driven Extraction: Using predefined schemas to accurately extract relevant data fields.</li> <li>Human-in-the-loop: Manual data extraction steps to verify and correct automated extractions through an open-source web interface built from the Argilla project.</li> </ol> <p>Key Features</p> <ul> <li>Schema-driven extraction: Ensures high specificity, contextual relevance, and automated validation of the extracted data.</li> <li>Advanced PDF preprocessing: AI optical character recognition (OCR) algorithms to detect and correct table structures within documents.</li> <li>User-friendly interface: Facilitates easy verification and correction of extracted data.</li> <li>Data flywheel: Continuous data collection of table extractions and LLM outputs to monitor performance and build datasets.</li> </ul> <p> WARNING: This repository contains sensitive information stored in the environment variable file. Do NOT make this repository public.</p>"},{"location":"user_guide/user_guide/#user-guide","title":"User guide","text":"<p>Please refer to the Extralit data pipeline and Extralit documentation (TBD) for more detailed instructions on how to use the Extralit system.</p> <p>Before running the python notebooks, replicate JT's development environment with a conda environment: <pre><code>conda env create -n extralit -f environment.yml\nconda activate extralit\n</code></pre></p>"},{"location":"user_guide/user_guide/#user-install","title":"User install","text":"<p>Install the extralit client and clone the repository. <pre><code>pip install --upgrade \"extralit[extraction]\"\n# or for editable install, in the `argilla0server/` directory\npip install --upgrade -e \".[extraction]\"\n</code></pre></p>"},{"location":"user_guide/user_guide/#login","title":"Login","text":"<p>To login to the extralit server, run the following command with the URL of the argilla-server, and your personal API key.</p> <pre><code>extralit login --api-url $API_BASE_URL\nextralit whoami\n</code></pre> If you don't have an existing account, create a new account by logging in to the `argilla` owner user account  Get the argilla account's API key at [config/users.yaml](config/users.yaml).  <pre><code>extralit login --api-url $API_BASE_URL # login with the argilla owner account's API key\nextralit users create\nextralit logout # logout from the owner account\nextralit login --api-url $API_BASE_URL # login to the new user account's API key\n</code></pre>"},{"location":"user_guide/user_guide/#create-a-new-extraction-project","title":"Create a new extraction project","text":"<p>First create a new extraction project by creating a Workspace. This will also create a new file storage bucket to contain schemas and PDFs. Only users added to the workspace can access the project data files and records.</p> <pre><code>extralit workspaces create {WORKSPACE_NAME}\n</code></pre> <p>Add other team-members to the project workspace as users with the <code>annotator</code> role:</p> <pre><code>extralit workspaces --name {WORKSPACE_NAME} add-user {EXISTING_USERNAME}\n</code></pre>"},{"location":"user_guide/user_guide/#add-new-references-and-pdfs","title":"Add new references and PDFs","text":"<p>Add new references to the workspace by uploading the PDF files listed in the reference table. The reference table should contain the metadata of the papers, such as the title, authors, and publication date. More specifically, the <code>REFERENCES_TABLE</code> needs to be a CSV file with the following columns: - <code>reference</code> (str, required): The reference ID of the paper, e.g <code>{firstauthor_lastname}{year}{title_first_word}</code>.,  - <code>pmid</code> (integer, nullable): The PubMed ID of the paper. - <code>doi</code> (str, nullable): The DOI of the paper. - <code>id</code> (str, optional): The unique UUID of the reference, for tracking with an external reference manager. - <code>file_path</code> (str, required): The file path to the PDF file. - <code>title</code> (str, optional): The title of the paper. - <code>authors</code> (str, optional): The authors. - <code>year</code> (integer, optional): The publication year of the publication. - ...: any other reference metadata fields can be added to the table.</p> <p>With the <code>REFERENCES_TABLE</code> file path in the <code>manifest</code> argument, import the references to the workspace with the <code>extralit references import</code> command, where the <code>metadatas</code> argument would add these attributes as metadatas the record.</p> <pre><code>extralit documents import --workspace {WORKSPACE_NAME} --papers {path/to/REFERENCES_TABLE.csv} --metadatas title,authors,year\n</code></pre> <p>Use 5 - JT - PDF preprocessing.ipynb if the command line tool hasn't been implemented.</p>"},{"location":"user_guide/user_guide/#createupdate-data-extraction-schemas","title":"Create/update data extraction schemas","text":"<p>The data extraction schemas are defined in Python files in the <code>schemas/</code> directory. The schemas define the structure and relationships of the data fields to be extracted from each reference. There is an AI assistant tool to help with create new schemas or update existing schemas.</p> <p>Upload these schemas the extralit server with:</p> <p><pre><code>extralit schemas upload --workspace {WORKSPACE_NAME} --schemas {path/to/schemas/*.json}\n</code></pre> Use 8 - JT - LLM Extraction.ipynb if the command line tool hasn't been implemented.</p>"},{"location":"user_guide/user_guide/#run-the-pdf-preprocessing-step","title":"Run the PDF preprocessing step","text":"<p>The PDF preprocessing step is a computationally intensive step that uses AI OCR algorithms to detect and correct table structures within documents. The PDF preprocessing step is run on the PDF files in the workspace, and the text OCR outputs are stored in the <code>preprocessing/</code> directory and the table outputs are automatically pushed as records to the <code>PDF-Preprocessing</code> Argilla dataset for manual correction.</p> <p><pre><code>pip install --upgrade \"extralit-server[ocr,pdf]\"\nextralit preprocessing run --workspace {WORKSPACE_NAME} --references {REFERENCE_IDS} --text-ocr {TABLE_OCR_MODELS} --table-ocr {TABLE_OCR_MODELS} --output-dataset {DATASET_NAME}\n</code></pre> Use 5 - JT - PDF Preprocessing.ipynb if the command line tool hasn't been implemented.</p>"},{"location":"user_guide/user_guide/#run-the-initial-llm-extraction-step","title":"Run the initial LLM extraction step","text":"<p>After the manual corrections are made to the PDF preprocessing outputs, the LLM extraction step is run to extract the data fields defined in the schemas. The records are automatically pushed to the <code>2-Data-Extraction</code> Argilla dataset for manual correction.</p> <p><pre><code>pip install --upgrade \"extralit-server[llm]\"\nextralit extraction run --workspace {WORKSPACE_NAME} --references {REFERENCE_IDS} --output-dataset {DATASET_NAME}\n</code></pre> Use 8 - JT - LLM Extraction.ipynb if the command line tool hasn't been implemented.</p>"},{"location":"user_guide/user_guide/#getting-the-status-of-the-extractions","title":"Getting the status of the extractions","text":"<p>Check the status of the extraction jobs with the <code>extralit extraction status</code> command. The status of the extraction jobs for each reference at different steps can be <code>pending</code>, <code>submitted</code>, <code>discarded</code>.</p> <p><pre><code>extralit extraction status --workspace {WORKSPACE_NAME} --references {REFERENCE_IDS}\n</code></pre> Use 9 - JT - Concensus extractions.ipynb if the command line tool hasn't been implemented.</p>"},{"location":"user_guide/user_guide/#export-the-extracted-data","title":"Export the extracted data","text":"<p>Export the extracted data from the workspace to a CSV file with the <code>extralit export</code> command. </p> <pre><code>extralit extraction export --workspace {WORKSPACE_NAME} --output {path/to/output.csv}\n</code></pre> <p>Use A - JT - Export data.ipynb if the command line tool hasn't been implemented.</p>"}]}