{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60ed47b6-f13f-4f28-91dd-1d33247b70a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import defaultdict, Counter\n",
    "import requests, re, os, json, datetime, tqdm, unicodedata, ast\n",
    "from typing import Union, List, Dict, Any, Tuple\n",
    "from IPython.display import HTML\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "current_date = datetime.datetime.now().date()\n",
    "date_string = current_date.strftime('%Y-%m-%d')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Bio import Entrez\n",
    "\n",
    "from extralit.extraction.models import Observation, ITNCondition, EntomologicalOutcome, ClinicalOutcome\n",
    "from extralit.metrics import *\n",
    "from extralit.convert import *\n",
    "\n",
    "import mendeley\n",
    "from mendeley import Mendeley\n",
    "from mendeley.auth import MendeleyLoginAuthenticator, MendeleySession, MendeleyClientCredentialsAuthenticator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "a62845e0-a700-4a48-ae2b-c82fc7f19057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Mendeley API credentials\n",
    "client_id = '16777'\n",
    "client_secret = 'RCleugtlK5scmtU7'\n",
    "redirect_uri = 'http://localhost/callback'\n",
    "\n",
    "# Specify the \"NetRecalibrationAutoExtract\" group ID to download publications\n",
    "group_id = 'dd44f670-adee-3872-be32-90f7bb5612c4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b974f34-9744-4bef-83f2-a3fd0c86d9eb",
   "metadata": {},
   "source": [
    "## Authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c67e3a58-bef2-4b7e-9a9d-79d79e8f2d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = mendeley.start_client_credentials_flow().authenticate()\n",
    "# session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "c5cade4d-5dcc-4f81-9a2c-7c67b699be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These values should match the ones supplied when registering your application.\n",
    "mendeley = Mendeley(client_id, client_secret, redirect_uri=redirect_uri)\n",
    "\n",
    "auth = mendeley.start_implicit_grant_flow()\n",
    "\n",
    "# The user needs to visit this URL, and log in to Mendeley.\n",
    "login_url = auth.get_login_url()\n",
    "\n",
    "# # After logging in, the user will be redirected to a URL, auth_response.\n",
    "display(login_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ea8bc5-0c87-4802-bce0-e80cb2ec5940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "access_token = '' # Add the access token here\n",
    "session = auth.authenticate(access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e826ca0c-f401-426e-9402-a88207023966",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e54c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_non_stop_word(words, stop_words = {'the', 'a', 'an', 'is', 'was', 'in', 'to', 'and', 'it', 'of', 'on', 'journal'}):\n",
    "    if not isinstance(words, list): \n",
    "        return None\n",
    "    for word in words:\n",
    "        if word.isanum() and word.lower() not in stop_words:\n",
    "            firsxt_word = re.sub(r'[^\\w]', '', word)\n",
    "            return first_word if first_word.isupper() else first_word.capitalize()\n",
    "            \n",
    "    return None  # return None if there is no non-stop word\n",
    "\n",
    "def create_reference_index(df):\n",
    "    last_name = df.get('Author', df['authors'].map(lambda x: x[0]['last_name']) if 'authors' in df else None)\n",
    "    last_name = convert_non_alphanumeric_to_ascii(last_name.str.split(\"-| \", regex=True, expand=True)[0])\n",
    "    \n",
    "    title_split = df['title'].str.split(' |-', regex=True)\n",
    "    first_title = title_split.apply(first_non_stop_word).str.replace('\\W', '', regex=True)\n",
    "    last_title = title_split.str[-1].str.replace('\\W', '', regex=True)\n",
    "    \n",
    "    first_source = df.get('Journal', df['source'] if 'source' in df else None).str.split(' |-', regex=True).apply(first_non_stop_word).fillna('')\n",
    "    pub_year = df.get('Pub_year', df['year'] if 'year' in df else None).astype(str)\n",
    "\n",
    "    references = last_name.str.lower() + pub_year + first_title.str.lower()\n",
    "    pmi_idx = last_name == 'PMI'\n",
    "    references.loc[pmi_idx] = last_name.loc[pmi_idx].str.lower() + pub_year.loc[pmi_idx] + last_title.loc[pmi_idx].str.lower()\n",
    "\n",
    "    return references\n",
    "\n",
    "def convert_non_alphanumeric_to_ascii(series, custom_mapping = {'ø': 'o','Ø': 'O',}):\n",
    "    # Normalize the Unicode string, then encode to ASCII, decode back to string and remove non-alphanumeric characters\n",
    "    def to_ascii(val):\n",
    "        for char, ascii_char in custom_mapping.items():\n",
    "            val = val.replace(char, ascii_char)\n",
    "        return unicodedata.normalize('NFKD', val).encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    series = series.apply(lambda x: to_ascii(x) if pd.notna(x) else x)\n",
    "    series = series.str.replace(r'[^A-Za-z0-9-]', '', regex=True)\n",
    "    \n",
    "    return series\n",
    "\n",
    "def get_unique_counter(df: pd.DataFrame, index: Union[str, List[str]], columns: List[str], prefix='-', suffix='', n_digits=2):\n",
    "    if not isinstance(columns, list):\n",
    "        columns = list(columns)\n",
    "\n",
    "    counter_series = df.groupby(index)\\\n",
    "        .apply(lambda df_: assign_group_id(df_, columns))\\\n",
    "        .reset_index(level=list(range(len(index))) if isinstance(index, list) else 0, drop=True)\n",
    "\n",
    "    na_ids = df.index[df[columns].replace('NA', None).isna().all(axis=1)]\n",
    "    counter_series.loc[na_ids] = 0\n",
    "    \n",
    "    counter_ids = counter_series.map(lambda x: f'{prefix}{int(x):0{n_digits}}{suffix}').sort_index()\n",
    "    \n",
    "    assert counter_ids.index.size == df.index.size\n",
    "    \n",
    "    return counter_ids\n",
    "\n",
    "\n",
    "def assign_group_id(df, group_columns):\n",
    "    # Filter out any group_columns not in the DataFrame\n",
    "    valid_group_columns = [col for col in group_columns if col in df.columns]\n",
    "\n",
    "    # Now perform the groupby with the valid columns only\n",
    "    return df.groupby(valid_group_columns).ngroup() + 1\n",
    "\n",
    "\n",
    "def check_unique_combinations(df, groupby_column, columns_to_check):\n",
    "    # Group the DataFrame by the groupby column\n",
    "    groupby_column = [*groupby_column]\n",
    "    if columns_to_check not in groupby_column:\n",
    "        groupby_column = groupby_column + ([columns_to_check] if isinstance(columns_to_check, str) else columns_to_check)\n",
    "        \n",
    "    grouped_df = df.dropna(how='all').groupby(groupby_column)\n",
    "\n",
    "    # Iterate over each group\n",
    "    for name, group in grouped_df:\n",
    "        # Create a DataFrame with only the columns to check\n",
    "        check_df = group[columns_to_check]\n",
    "\n",
    "        # Check if all rows in check_df are unique\n",
    "        assert (check_df.nunique() <= 1).all(), \\\n",
    "            f'{name} has duplicates in {check_df[check_df.duplicated(keep=False)].head(4)}'\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91387092-88ab-4b49-8934-fc200099a0a4",
   "metadata": {},
   "source": [
    "## Load Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9609fb-2273-4251-b400-bf894d976973",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Review ITN master list with non-included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46994958-4183-4b9e-9ea6-491d53a14b0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "itn_master_list_w_non_incl = pd.read_csv('data/external/itn_master_list_with_non_inclusions.csv', index_col='reference_key')\n",
    "itn_master_list_w_non_incl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "f4709ea4-7836-488c-ba56-0acf2bd58c4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_itn = {'included': list(), 'not_included': list()}\n",
    "y_itn = {'included': list(), 'not_included': list()}\n",
    "for ref, s in itn_master_list_w_non_incl.query(\"Include == 'N'\")[['Include', 'Notes']].iterrows():\n",
    "    paper = s.to_frame().T\n",
    "    \n",
    "    if (papers.index.str.lower()==ref.lower()).sum():\n",
    "        n_itn['included'].append(paper)\n",
    "    else:\n",
    "        n_itn['not_included'].append(paper)\n",
    "\n",
    "for ref, s in itn_master_list_w_non_incl.query(\"Include == 'Y'\")[['Include', 'Notes']].iterrows():\n",
    "    paper = s.to_frame().T\n",
    "    \n",
    "    if (papers.index.str.lower()==ref.lower()).sum():\n",
    "        y_itn['included'].append(paper)\n",
    "    else:\n",
    "        y_itn['not_included'].append(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "35ba6821-cfcc-44a1-8a29-3d2c05bd478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "b22acf9b-a94f-4be3-899e-f59225cd6233",
   "metadata": {},
   "outputs": [],
   "source": [
    "tally = pd.concat([\n",
    "    pd.concat({k: pd.concat(v) for k,v in n_itn.items()}, names=['master_paper_list', 'reference']),\n",
    "    pd.concat({k: pd.concat(v) for k,v in y_itn.items()}, names=['master_paper_list', 'reference'])\n",
    "]).reset_index(0)\n",
    "tally.to_csv(\"/Users/jonnytr/Desktop/itn_master_list_vs_master_paper_list.csv\")\n",
    "tally.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa5880c-b6d2-4361-886d-d2ca9f9f3163",
   "metadata": {},
   "source": [
    "### Download mendeley collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "63c4f88a-eec2-44b3-a0f9-4a25f8d8b09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "{group.name: group.id for group in session.groups.list().items}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "c3858c79-b144-4ff5-a97d-c176e34094ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group = session.groups.get('dd44f670-adee-3872-be32-90f7bb5612c4')\n",
    "docs = group.documents.list(page_size=500).items\n",
    "docs_df = pd.DataFrame.from_dict([doc.__dict__['json'] for doc in docs])\n",
    "\n",
    "docs_df['source'] = normalize_publisher_names(docs_df['source'])\n",
    "docs_df['reference'] = create_reference_index(docs_df)\n",
    "docs_df = docs_df.join(docs_df['identifiers'].apply(pd.Series)[['issn', 'pmid', 'doi']])\n",
    "docs_df.sort_values(by='pmid')\n",
    "docs_df = docs_df.drop_duplicates(subset=['reference']).set_index('reference')\n",
    "docs_df['collections'] = None\n",
    "docs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc3a0c9-be67-4471-bb26-4fe726fcd7dc",
   "metadata": {},
   "source": [
    "### Load mendeley collection of papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "83ad7fee-f5bf-4152-bbbd-b1872682c3b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convert_obj = lambda x: ast.literal_eval(x) if isinstance(x, str) and x else None\n",
    "converters = {'identifiers': convert_obj, 'authors': convert_obj, 'keywords': convert_obj}\n",
    "\n",
    "ento_docs_df = pd.read_csv('data/processed/ento_papers.csv', \n",
    "                           converters=converters)\n",
    "clinical_docs_df = pd.read_csv('data/processed/clinical_papers.csv', \n",
    "                           converters=converters)\n",
    "net_docs_df = pd.read_csv('data/processed/net_papers.csv', \n",
    "                           converters=converters)\n",
    "pmi_docs_df = pd.read_csv('data/processed/pmi_papers.csv', \n",
    "                           converters=converters).fillna({'source': \"PMI\"})\n",
    "update_docs_df = pd.read_csv('data/processed/updatemarch2023_papers.csv', \n",
    "                           converters=converters)\n",
    "\n",
    "for df, collection in zip([ento_docs_df, clinical_docs_df, net_docs_df, pmi_docs_df, update_docs_df], \n",
    "                          ['Entomological Outcomes', 'Human Health Outcomes', 'Net Outcomes', 'PMI Durability Monitoring Report', 'UpdateMarch2023']):\n",
    "    df['reference'] = create_reference_index(df)\n",
    "    if 'identifiers' in df.columns:\n",
    "        df = df.join(df['identifiers'].apply(pd.Series)[['issn', 'pmid', 'doi']])\n",
    "    df.set_index('reference', inplace=True)\n",
    "    docs_df.loc[df.index, 'collections'] = docs_df.loc[df.index, 'collections'].map(\n",
    "        lambda x: [*x, collection] if isinstance(x, (list, np.ndarray)) else [collection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "de04591b-c5bc-446a-b6d2-199af199ebe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "papers = pd.read_csv('data/external/IDM ITN Recalibration_ Master paper list - Sheet1.csv', dtype={'pmid': str})\\\n",
    "    .rename(columns={\n",
    "        'First Author': 'Author', \n",
    "        'Publication year': 'Pub_year', \n",
    "        'Title': 'title', \n",
    "        'Reference (new)': 'reference',\n",
    "        'Needs additional digitization? Y/N': 'Needs_digitization',\n",
    "        'Measured Outcome': 'Measured_outcome', \n",
    "        'Approximate Number of Rows to Extract': 'Approx_num_rows', \n",
    "        'Table Numbers': 'Tables_count', \n",
    "        'Check out: Initials': 'Check_out_by',\n",
    "        'Check in: Initials': 'Check_in_by',\n",
    "        'Needs additional digitization? Y/N': 'Needs_digitization',\n",
    "        'Notes on esimated extraction count': 'Notes_on_approx_num_rows',\n",
    "        'Notes on extraction': 'Notes_on_extraction',\n",
    "    })\n",
    "papers = papers.assign(\n",
    "    Needs_digitization=papers[\"Needs_digitization\"].apply(normalize_need_digitalization),\n",
    ")\n",
    "papers = papers.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "papers['title'] = papers['title'].str.strip(\"{}.\").tolist()\n",
    "papers['reference'].update(create_reference_index(papers).drop(papers['reference'].dropna().index))\n",
    "papers.set_index('reference', inplace=True)\n",
    "\n",
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "664c1ed4-f8fb-4d17-9a6d-2ed30c5a5465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "papers = papers.drop(columns=['Author', 'Pub_year', 'title', 'Journal'], errors='ignore').join(docs_df, how='left')\n",
    "papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab2f86-5b98-4e3f-ad64-08dfecd73884",
   "metadata": {},
   "source": [
    "### Count subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "2d9f7a12-61a5-43cd-b937-7b1e89fd23ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "[pd.Index(papers['reference']).intersection(ento_docs_df['reference']).size,\n",
    " pd.Index(papers['reference']).intersection(clinical_docs_df['reference']).size,\n",
    " pd.Index(papers['reference']).intersection(net_docs_df['reference']).size,\n",
    " pd.Index(papers['reference']).intersection(pmi_docs_df['reference']).size,\n",
    " pd.Index(papers['reference']).intersection(update_docs_df['reference']).size,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "a93f678c-5e1b-4165-9f0a-54e617c28988",
   "metadata": {},
   "outputs": [],
   "source": [
    "allmendeley_papers = pd.Index(ento_docs_df['reference'])\\\n",
    ".union(clinical_docs_df['reference'])\\\n",
    ".union(net_docs_df['reference'])\\\n",
    ".union(pmi_docs_df['reference'])\\\n",
    ".union(update_docs_df['reference'])\n",
    "display(allmendeley_papers.size)\n",
    "\n",
    "allmendeley_papers.difference(papers['reference']).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "445d4016-bce3-456d-ac9e-e3272158356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Index(papers['reference']).difference(allmendeley_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "79122f94-6b86-41b6-a1d4-6797242e2967",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[pd.Index(ento_docs_df['reference']).difference(pd.Index(papers['reference'])).size,\n",
    " pd.Index(clinical_docs_df['reference']).difference(pd.Index(papers['reference'])).size,\n",
    " pd.Index(net_docs_df['reference']).difference(pd.Index(papers['reference'])).size,\n",
    " pd.Index(pmi_docs_df['reference']).difference(pd.Index(papers['reference'])).size,\n",
    " pd.Index(update_docs_df['reference']).difference(pd.Index(papers['reference'])).size,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38411b81-bb42-46c3-a5d2-b35fbe4aea25",
   "metadata": {},
   "source": [
    "### Create reference index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "8c3a6a15-fbb4-42ba-865d-cd86c1050072",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'reference' not in docs_df.columns and 'reference' not in docs_df.index.names:\n",
    "        \n",
    "    references = create_reference_index(docs_df)\n",
    "    dup_idx = references[references.duplicated(keep=False) & references.notna()].sort_values().index\n",
    "    display(docs_df.loc[dup_idx])\n",
    "    \n",
    "    docs_df['reference'] = references\n",
    "    docs_df = docs_df.drop(docs_df['reference'][docs_df['reference'].duplicated()].index)\n",
    "    \n",
    "    docs_df['reference'].duplicated().sum()\n",
    "    docs_df['reference'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "f97d695f-1e3d-4f91-ad8c-be0374b4cb9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# doi duplicates\n",
    "dois = docs_df['doi']\n",
    "display({'NA': dois.isna().sum(), 'duplicated': dois.dropna().duplicated().sum()})\n",
    "\n",
    "dup_idx = dois[dois.duplicated(keep=False) & dois.notna()].sort_values().index\n",
    "display(docs_df.loc[dup_idx])\n",
    "\n",
    "docs_df['identifiers'] = docs_df['identifiers'].replace('', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "11da840a-87f1-4210-852d-aadf2d4a6fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pmid duplicates\n",
    "pmids = docs_df['pmid']\n",
    "display({'NA': pmids.isna().sum(), 'duplicated': pmids.dropna().duplicated().sum()})\n",
    "dup_idx = pmids[pmids.duplicated(keep=False) & pmids.notna()].sort_values().index\n",
    "display(docs_df.loc[dup_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "33188577-def3-4695-a73e-105872229801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_df.index.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38462c75-8884-4a31-b2fd-0ce99ec781ea",
   "metadata": {},
   "source": [
    "# Generate extraction queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed03b25f-1f1e-481b-83e9-e7a787644e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "addl_fig_dig_papers = [\n",
    "    'Agossa2014', 'Bayili2017', 'Bayili2019', 'Birhanu2019', \n",
    "    'Djenontin2010', 'Duchon2009', 'Etang2013', 'Etang2016', 'Kayedi2015', \n",
    "    'Kolaczinski2000', 'Koudou2011', 'Lindblade2005', 'NGuessan2016', \n",
    "    'Ngufor2017', 'Norris2011', 'Ohashi2012', 'Okumu2012', 'Randriamaherijaona2017', \n",
    "    'Riveron2018', 'Sternberg2014', 'Tchakounte2019', 'Tungu2015', \n",
    "    'Tungu2016', 'Wanjala2015', 'Yewhalaw2012',\n",
    "]\n",
    "\n",
    "pd.Index(docs_df['Mendeley Reference Key'].dropna()).intersection(addl_fig_dig_papers).size, len(addl_fig_dig_papers), set(addl_fig_dig_papers).difference(docs_df['Mendeley Reference Key'].dropna())\n",
    "\n",
    "anchor_index = (docs_df['Needs_digitization']==True).idxmax()\n",
    "print(anchor_index)\n",
    "index_values = [idx for idx, row in docs_df.iterrows() if row['Mendeley Reference Key'] in addl_fig_dig_papers]\n",
    "\n",
    "# Split the DataFrame into parts: before the anchor, the anchor, and after the anchor\n",
    "before_anchor = docs_df.loc[:anchor_index].drop(anchor_index)\n",
    "after_anchor = docs_df.loc[anchor_index:]\n",
    "\n",
    "# Find rows in index_values that are before the anchor index\n",
    "rows_to_move = before_anchor.loc[before_anchor.index.isin(index_values)]\n",
    "\n",
    "# Remove rows from 'before_anchor' if they're in 'rows_to_move'\n",
    "before_anchor = before_anchor.drop(rows_to_move.index)\n",
    "\n",
    "# Concatenate the DataFrame parts, adding 'rows_to_move' after the 'anchor_index'\n",
    "docs_df_rearranged = pd.concat([before_anchor, rows_to_move, after_anchor])\n",
    "docs_df.shape, docs_df_rearranged.shape, rows_to_move.shape, (docs_df_rearranged['Needs_digitization']==True).idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6a74d22-9963-4af0-ad26-d435441aac39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_df_rearranged.loc[(docs_df_rearranged['Needs_digitization'] != True) & docs_df_rearranged['Mendeley Reference Key'].isin(addl_fig_dig_papers), 'Needs_digitization'] = True\n",
    "(docs_df_rearranged['Needs_digitization']==True).idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6579506e-0ec9-4869-92e6-c62f54e6869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractions = pd.read_csv('data/processed/extractions_preJonny_cleaned.csv', index_col='reference',\n",
    "                          dtype={'Start_year': pd.Int64Dtype(), \n",
    "                                 'End_year': pd.Int64Dtype()})\n",
    "extractions['IDM_check1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "67b2b03b-38d2-490d-ac35-5f1a93cb627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractions.index.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "1ad48621-d5e4-4027-8ec2-e88547ddfe42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_round = docs_df.query('`Mendeley Reference Key` in @extractions.index and '\n",
    "                            'Needs_digitization != True and source.notnull()')\n",
    "is_net_outcome = first_round.collections.apply(lambda li: 'Net Outcomes' in li if li else False)\n",
    "first_round_ento_human = first_round.loc[~is_net_outcome]\n",
    "first_round_net = first_round.loc[is_net_outcome]\n",
    "first_round_pmi = docs_df.query('`Mendeley Reference Key` in @extractions.index and Needs_digitization != True '\n",
    "                                'and `Mendeley Reference Key` not in @addl_fig_dig_papers and source.isnull()')\n",
    "second_round = docs_df.query('`Mendeley Reference Key` in @extractions.index and Needs_digitization == True '\n",
    "                             'or `Mendeley Reference Key` in addl_fig_dig_papers')\n",
    "third_round = docs_df.query('`Mendeley Reference Key` not in @extractions.index')\n",
    "\n",
    "assert first_round_ento_human.shape[0] + first_round_net.shape[0] + first_round_pmi.shape[0] + second_round.shape[0] + third_round.shape[0] == docs_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "de7bc8c2-bb65-4222-b157-516d4a6e507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "3227ba1c-906c-473f-b8b1-df42ba9ac5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_round_ento_human.shape[0], first_round_net.shape[0], first_round_pmi.shape[0], second_round.shape[0], third_round.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "6a4a2c6c-4431-400d-afc7-b70086d0dafd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_df = pd.concat([\n",
    "    first_round_ento_human.sample(frac=1), \n",
    "    first_round_net.sample(frac=1), \n",
    "    first_round_pmi.sample(frac=1), \n",
    "    second_round.sample(frac=1), \n",
    "    third_round.sample(frac=1)])\n",
    "docs_df['collections'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e163d92-303c-413b-a311-903872e2ed2d",
   "metadata": {},
   "source": [
    "# Get file manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "dc2c7f4d-7f5d-4fe2-94b6-eb2a5433761b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# docs_df = pd.read_parquet('config/extraction_queue_198papers_2024-04-02.parquet')\n",
    "# docs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "6c561bb6-9c40-4b70-b69b-a2da31c0112a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_file_manifest(doc: pd.Series) -> dict:\n",
    "    files = session.document_files(doc.id).list().items\n",
    "    if not files:\n",
    "        return None\n",
    "    return files[0].__dict__['json']\n",
    "    \n",
    "files_df = papers.apply(get_file_manifest, axis=1)\n",
    "files_df = pd.json_normalize(files_df)\n",
    "files_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "7b3dd497-171e-42f2-974c-d81a7759cc73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "papers = papers.join(files_df.rename(columns={'id': 'file_id', 'document_id': 'id'}).set_index('id').drop(columns=['created']), \n",
    "            on='id')\n",
    "papers[papers['file_name'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a245661-02b3-40f5-8480-8a839aa08f44",
   "metadata": {},
   "source": [
    "### Rename filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "db701486-ea46-4693-8c02-9a426669e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_filename(filename):\n",
    "    filename = re.sub(r'[\\\\/:*?\"<>|]|\\A\\.', '_', filename)\n",
    "    filename = filename[:255]\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "a11d6335-e8d3-43b8-95c4-ab557d72a3f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dup_file_names = papers.index[papers['file_name'].duplicated(keep=False) & papers['file_name'].notnull()]\n",
    "papers.loc[dup_file_names, 'file_name'] = papers.loc[dup_file_names].index.values\n",
    "papers['file_name'] = papers['file_name'].map(normalize_filename)\n",
    "dup_file_names.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "d23decd2-a30e-4930-b36a-0fa429fd1c6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_pdf_ext_files = papers['file_name'][~papers['file_name'].fillna('').str.endswith('.pdf')].dropna().index\n",
    "papers.loc[no_pdf_ext_files, 'file_name'] = papers.loc[no_pdf_ext_files, 'file_name'] + '.pdf'\n",
    "no_pdf_ext_files.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc76a212-2e41-443f-bc21-97b80acdbeb4",
   "metadata": {},
   "source": [
    "### Download files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "85eb810f-2fe9-425b-ab42-71ca2a171b5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def download_file(filepath, document_id, file_id, access_token):\n",
    "    if not (isinstance(filepath, str) and filepath and document_id and isinstance(file_id, str) and access_token): \n",
    "        return None\n",
    "    elif os.path.exists(filepath):\n",
    "        return filepath\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {access_token}',\n",
    "        'Accept': 'application/pdf',\n",
    "    }\n",
    "    url = f'https://api.mendeley.com/files/{file_id}'\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    try:\n",
    "        response.raise_for_status()  # check for request errors\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "    with open(filepath, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "f2a66394-74a7-4ef5-b3b3-4981b5c9d8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = 'data/pdf/'\n",
    "\n",
    "downloaded_filenames = papers.apply(\n",
    "    lambda doc: download_file(os.path.join(download_dir, doc['file_name']) if isinstance(doc['file_name'], str) else None,\n",
    "                              doc.name, \n",
    "                              doc['file_id'], \n",
    "                              session.access_token),\n",
    "    axis=1)\n",
    "downloaded_filenames.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "6a65776d-8f6b-4919-95f7-0283d4d541b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "downloaded_filenames.name = \"file_path\"\n",
    "papers = papers.join(downloaded_filenames)\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07fab37-a2ee-47b8-928c-61135676fd82",
   "metadata": {},
   "source": [
    "# Write files manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "39581a90-f42f-49c9-ba6f-452c6810e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers.to_parquet(f'config/extraction_queue_{papers.shape[0]}papers_{current_date.strftime(\"%Y-%m-%d\")}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd3c4e4-a169-4532-8059-70922878d0cc",
   "metadata": {},
   "source": [
    "### Update file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3cbb6aa-e9ea-4a1d-a102-25613b9e661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_filename(file_path):\n",
    "    # Extract directory path, file name, and extension\n",
    "    directory, file_name = os.path.split(file_path)\n",
    "    file_name, file_extension = os.path.splitext(file_name)\n",
    "    \n",
    "    # Replace spaces with underscores and remove special characters\n",
    "    file_name = re.sub(r'[^\\w\\s-]', '', file_name.replace(' ', '_'))\n",
    "    \n",
    "    # Remove any non-ASCII characters (optional, depending on your requirements)\n",
    "    file_name = file_name.encode('ascii', 'ignore').decode()\n",
    "    \n",
    "    # Ensure the file name length does not exceed 255 characters (excluding the length of the extension)\n",
    "    max_length = 255 - len(file_extension)\n",
    "    file_name = file_name[:max_length]\n",
    "    \n",
    "    # Reconstruct the full file path with the original extension\n",
    "    normalized_path = os.path.join(directory, file_name + file_extension)\n",
    "    return normalized_path\n",
    "\n",
    "def normalize_file_paths_in_df(df, column_name='file_path', rename_files=False):\n",
    "    # Create a new column for the normalized paths\n",
    "    new_paths = df[column_name].apply(normalize_filename)\n",
    "\n",
    "    # Rename files on the filesystem\n",
    "    for old_path, new_path in zip(df[column_name], new_paths):\n",
    "        if old_path != new_path and rename_files:\n",
    "            try:\n",
    "                os.rename(old_path, new_path)\n",
    "                print(f\"Renamed '{old_path}' to '{new_path}'\")\n",
    "            except OSError as e:\n",
    "                print(f\"Error renaming '{old_path}' to '{new_path}': {e}\")\n",
    "\n",
    "    # Update the original file_path column to reflect the new file names\n",
    "    df[column_name] = new_paths\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a64e694-da4f-4fb0-b64b-f48b28995958",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = pd.read_parquet(f'config/extraction_queue_197papers_2024-04-03.parquet')\n",
    "papers = normalize_file_paths_in_df(papers, column_name='file_path', rename_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "110e254c-7540-41ab-9b5f-e0abd828b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers.to_parquet(f'config/extraction_queue_197papers_2024-04-03.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d18de6e0-c328-40e6-9fb6-20ef5c298eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers['file_name'] = papers['file_path'].map(lambda x: os.path.split(x)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb41f816-8850-4ecc-89cb-290aa7d6a748",
   "metadata": {},
   "source": [
    "# Draw annotations on PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d592bad-f50d-4d3b-a97a-d2ab5f328078",
   "metadata": {},
   "source": [
    "## Get all annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c1ce30c9-d7e7-4d64-a0db-cf0567a24581",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anns_df = pd.DataFrame()\n",
    "\n",
    "# Get the first page of annotations\n",
    "pages = session.annotations.list(page_size=200)\n",
    "\n",
    "while pages is not None:\n",
    "    # Get the current page of annotations and append to anns_df\n",
    "    anns = pages.items\n",
    "    current_page_df = pd.DataFrame([ann.__dict__['json'] for ann in anns])\n",
    "    anns_df = pd.concat([anns_df, current_page_df], ignore_index=True)\n",
    "    \n",
    "    # Move on to the next page\n",
    "    pages = pages.next_page\n",
    "anns_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd03224-a911-404a-9d01-2b77df443743",
   "metadata": {},
   "source": [
    "## Reference document_id to the reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "711ab420-2ced-41a7-8e95-dc2e2d29374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_info(doc_id):\n",
    "    # Fetch the document by document_id\n",
    "    document = session.documents.get(doc_id)\n",
    "    doc_json = document.json\n",
    "\n",
    "    return pd.Series(doc_json, name=doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7a31b759-acd0-4550-a36a-28baffc33cea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# anns_docs = anns_df['document_id'].drop_duplicates().map(get_document_info)\n",
    "# anns_docs = pd.DataFrame(anns_docs.tolist())\n",
    "anns_docs.index.name = 'document_id'\n",
    "\n",
    "anns_docs = anns_docs.dropna(subset=['authors', 'year'])\n",
    "anns_docs['year'] = anns_docs['year'].astype(int)\n",
    "anns_docs['reference'] = create_reference_index(anns_docs)\n",
    "anns_docs = anns_docs.drop(anns_docs.index[anns_docs['reference'].duplicated()])\n",
    "anns_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8bcc43-aef4-4a65-b5ec-cbb386cc194a",
   "metadata": {},
   "source": [
    "## Join annotations to reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4d11f75a-c519-49d1-919d-a1ca37b2ab4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anns_df = anns_df.join(anns_docs['reference'], on='document_id')\n",
    "anns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "16d49c91-aaa2-444a-b78d-93d2ce263ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.json_normalize(pd.json_normalize(anns_df['positions'].dropna())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f168d52e-440f-42f2-9c52-f2785b5a3b80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Download annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5d4915bd-2f87-4258-a235-c309140a8c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_annotations(pdf_file_path, annotations):\n",
    "    if not (os.path.exists(pdf_file_path) and len(annotations)): return\n",
    "        \n",
    "    with fitz.open(pdf_file_path) as pdf_document:\n",
    "        pages = {}\n",
    "        \n",
    "        for annotation in annotations:\n",
    "            for position in annotation['positions']:\n",
    "                page_number = position['page'] - 1  # Pages are 0-indexed in PyMuPDF\n",
    "                page = pages.setdefault(page_number, pdf_document.load_page(page_number))\n",
    "                \n",
    "                # Get the dimensions of the page\n",
    "                page_width, page_height = page.rect[2], page.rect[3]\n",
    "                \n",
    "                # invert the y-coordinates of the highlight rectangle\n",
    "                rect = fitz.Rect(\n",
    "                    position['top_left']['x'], page_height - position['bottom_right']['y'],\n",
    "                    position['bottom_right']['x'], page_height - position['top_left']['y'],\n",
    "                )\n",
    "                \n",
    "                highlight = page.add_highlight_annot(rect)\n",
    "                # Update the highlight annotation with color\n",
    "                color = tuple([v/255 for v in annotation['color'].values()])\n",
    "                highlight.set_colors({\"stroke\": color, \"fill\": color})\n",
    "                highlight.set_opacity(0.8)  # Set opacity to 50%\n",
    "                highlight.update()\n",
    "                \n",
    "        output_pdf_path = pdf_file_path #.replace(\"data/pdf\", \"data/pdf-annotated\")\n",
    "        pdf_document.save(output_pdf_path, incremental=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "8a54b221-e1af-4b68-abc5-0dc5be7d78e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = fitz.open('data/pdf-annotated/Corbel_2010_Field_efficacy_of_a_new_mosaic_long.pdf')\n",
    "file.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "68451191-c850-46d7-bb60-a14e545ba82f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in tqdm.tqdm(docs_df.iterrows()):\n",
    "    ref = row['reference']\n",
    "    pdf_file_path = row['file_path']\n",
    "    if not isinstance(pdf_file_path, str):\n",
    "        continue\n",
    "    \n",
    "    # Filter annotations by document_id\n",
    "    doc_annotations = anns_df[(anns_df['reference'] == ref) & anns_df['positions'].notnull()].to_dict(orient='records')\n",
    "    if not len(doc_annotations): \n",
    "        continue\n",
    "    draw_annotations(pdf_file_path, doc_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d66f6-c25b-4861-8ac7-051655f5ea84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
